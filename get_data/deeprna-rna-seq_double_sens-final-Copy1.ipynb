{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(seq):\n",
    "    seq = seq.split('\\n')\n",
    "    seq2 = ''\n",
    "    for j in seq:\n",
    "        seq2 = seq2 + j\n",
    "    seq2 = seq2[2:len(seq2)-1]\n",
    "    seq2 = seq2.split(' ')\n",
    "    #print(seq2)\n",
    "    seq3=[]\n",
    "    for j in seq2:\n",
    "        #print(j)\n",
    "        if j=='':\n",
    "            a=0\n",
    "        else:\n",
    "            seq3.append(float(j))\n",
    "    return seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(nb_train = 165000,nb_val = 6000,nb_test = 5000):\n",
    "    # load your data using this function\n",
    "    verif = []\n",
    "\n",
    "    verif = pd.read_csv(\"genomes/negatifs_m-m-str.csv\", sep = \"\\t\",header=None)\n",
    "    verif = np.array(verif)\n",
    "    print(len(verif))\n",
    "    for i in range(len(verif)):\n",
    "        for j in range(2):\n",
    "            if len(verif[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "    neg1 = verif\n",
    "\n",
    "    verif=[]\n",
    "\n",
    "    verif = pd.read_csv(\"genomes/negatifs_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    verif = np.array(verif)\n",
    "    print(len(verif))\n",
    "    for i in range(len(verif)):\n",
    "        for j in range(2):\n",
    "            if len(verif[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    neg2 = verif\n",
    "    verif=[]\n",
    "    verif = pd.read_csv(\"genomes/negatifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    verif = np.array(verif)\n",
    "    print(len(verif))\n",
    "    for i in range(len(verif)):\n",
    "        for j in range(2):\n",
    "            if len(verif[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    neg3 = verif\n",
    "    verif=[]\n",
    "\n",
    "    verif = pd.read_csv(\"genomes/positifs_m-m-str.csv\", sep = \"\\t\",header=None)  \n",
    "    verif = np.array(verif)\n",
    "    print(len(verif))\n",
    "    for i in range(len(verif)):\n",
    "        for j in range(2):\n",
    "            if len(verif[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "            \n",
    "    pos1 = verif\n",
    "    l = len(pos1)\n",
    "    verif=[]\n",
    "\n",
    "    verif = pd.read_csv(\"genomes/positifs_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    verif = np.array(verif)\n",
    "    print(len(verif))\n",
    "    for i in range(len(verif)):\n",
    "        for j in range(2):\n",
    "            if len(verif[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "            \n",
    "    pos2 = verif\n",
    "    l=l+len(pos2)\n",
    "    verif=[]\n",
    "\n",
    "    verif = pd.read_csv(\"genomes/positifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    verif = np.array(verif)\n",
    "    print(len(verif))\n",
    "    for i in range(len(verif)):\n",
    "        for j in range(2):\n",
    "            if len(verif[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "            \n",
    "    pos3 = verif\n",
    "    l=l+len(pos3)\n",
    "    verif=[]\n",
    "\n",
    "    bdd = np.concatenate((pos1,pos2,pos3,neg1,neg2,neg3))\n",
    "\n",
    "    pos1 = []\n",
    "    neg1 = []\n",
    "    pos2 = []\n",
    "    neg2 = []\n",
    "\n",
    "    labels = np.zeros((len(bdd),1))\n",
    "\n",
    "    bdd = np.concatenate((bdd,labels),axis=1)\n",
    "    for i in range(l):\n",
    "        bdd[i,4]=1\n",
    "    labels=[]\n",
    "    \n",
    "    # shuffle total\n",
    "\n",
    "    indices = np.arange(len(bdd))\n",
    "    shuffle(indices)\n",
    "    bdd = bdd[indices]\n",
    "    indices=[]\n",
    "\n",
    "    # \n",
    "    l = len(bdd)\n",
    "    matrice = np.zeros((l,36,36,1)) #4\n",
    "    matrice1 = np.zeros((l,36,36,1))\n",
    "    matrice2 = np.zeros((l,36,4))\n",
    "    matrice3 = np.zeros((l,36,4))\n",
    "    matrice4 = np.zeros((l,36,4))\n",
    "    matrice5 = np.zeros((l,36,4))\n",
    "\n",
    "    for i in range(l):\n",
    "        seq1 = bdd[i,0]\n",
    "        seq2 = bdd[i,1]\n",
    "        prob1 = np.array(clean(bdd[i,2]))\n",
    "        prob2 = np.array(clean(bdd[i,3]))\n",
    "        for j in range(len(seq1)):\n",
    "            for k in range(len(seq2)):\n",
    "                if (seq1[j]=='a' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='a'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                    #matrice[i,j,k,1] = prob1[j]+prob2[k]\n",
    "                elif (seq1[j]=='g' and seq2[k]=='c') or (seq1[j]=='c' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                    #matrice[i,j,k,1] = prob1[j]+prob2[k]\n",
    "                elif (seq1[j]=='g' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                    #matrice[i,j,k,1] = prob1[j]+prob2[k]\n",
    "                matrice1[i,j,k,0] = prob1[j]+prob2[k]\n",
    "        for j in range(len(seq1)):\n",
    "            if seq1[j]=='a':\n",
    "                matrice2[i,j,0] = 1\n",
    "            elif seq1[j]=='u':\n",
    "                matrice2[i,j,1] = 1\n",
    "            elif seq1[j]=='g':\n",
    "                matrice2[i,j,2] = 1\n",
    "            elif seq1[j]=='c':\n",
    "                matrice2[i,j,3] = 1\n",
    "        for j in range(len(seq2)):\n",
    "            if seq2[j]=='a':\n",
    "                matrice3[i,j,0] = 1\n",
    "            elif seq2[j]=='u':\n",
    "                matrice3[i,j,1] = 1\n",
    "            elif seq2[j]=='g':\n",
    "                matrice3[i,j,2] = 1\n",
    "            elif seq2[j]=='c':\n",
    "                matrice3[i,j,3] = 1\n",
    "\n",
    "    for i in range(36):\n",
    "        matrice4[:,36-i-1,:] = matrice2[:,i,:]\n",
    "    for i in range(36):\n",
    "        matrice5[:,36-i-1,:] = matrice3[:,i,:]\n",
    "    \n",
    "    training = []\n",
    "    training.append(matrice[:nb_train])\n",
    "    training.append(matrice1[:nb_train])\n",
    "    training.append(matrice2[:nb_train])\n",
    "    training.append(matrice4[:nb_train])\n",
    "    training.append(matrice3[:nb_train])\n",
    "    training.append(matrice5[:nb_train])    \n",
    "    \n",
    "    validation = []\n",
    "    validation.append(matrice[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice1[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice2[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice4[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice3[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice5[nb_train:nb_train+nb_val])\n",
    "\n",
    "    testing = []\n",
    "    testing.append(matrice[-nb_test:])\n",
    "    testing.append(matrice1[-nb_test:])\n",
    "    testing.append(matrice2[-nb_test:])\n",
    "    testing.append(matrice4[-nb_test:])\n",
    "    testing.append(matrice3[-nb_test:])\n",
    "    testing.append(matrice5[-nb_test:])\n",
    "\n",
    "    labels = bdd[:,4]\n",
    "    bdd = []\n",
    "    y = labels[:nb_train]\n",
    "    y = keras.utils.np_utils.to_categorical(y,2)\n",
    "    val_y = labels[nb_train:nb_train+nb_val]\n",
    "    val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "    true_y = labels[-nb_test:]\n",
    "    argtest=[]\n",
    "    np.sum(y[:,1])\n",
    "    return training, y, validation, val_y, testing, true_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_performance(test_num, pred_y,  labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+fp)\n",
    "    sensitivity = float(tp)/(tp+fn)\n",
    "    specificity = float(tn)/(tn+fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26023\n",
      "24229\n",
      "37991\n",
      "26023\n",
      "24229\n",
      "37993\n"
     ]
    }
   ],
   "source": [
    "Data = load_data(100,250,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perfs_test(model,test,true):\n",
    "    print('predicting')\n",
    "            \n",
    "    predictions = model.predict(test)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true)\n",
    "    \n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paramètres \n",
    "true_y = Data[5]\n",
    "training = Data[0]\n",
    "y = Data[1]\n",
    "batch_size=64\n",
    "epochs=25\n",
    "verbose1 = 1\n",
    "verbose2 = 1\n",
    "validation = Data[2]\n",
    "val_y = Data[3]\n",
    "testing = Data[4]\n",
    "matrixsize11 = 6\n",
    "nbfilter11 = 24\n",
    "matrixsize12 = 7\n",
    "nbfilter12 = 4\n",
    "matrixsize21 = 6\n",
    "nbfilter21 = 24\n",
    "matrixsize22 = 7\n",
    "nbfilter22 = 4\n",
    "nbfilter1 = 96\n",
    "kernelsize = 7\n",
    "nbfilters2 = 96\n",
    "kernel_size2 = 7\n",
    "Dense1 = 128\n",
    "Dense2 = 512\n",
    "Dense3 = 512\n",
    "Dense4 = 128\n",
    "Dense5 = 64\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras import Model\n",
    "k = matrixsize11\n",
    "# init_weights\n",
    "I = np.eye(k)\n",
    "M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "I2 = np.zeros((k,k))\n",
    "M2 = np.zeros((k,k))\n",
    "for j in range(k):\n",
    "    I2[:,j] = I[:,k-j-1]\n",
    "    M2[:,j] = M[:,k-j-1]        \n",
    "W = np.zeros((k,k,1,nbfilter11))\n",
    "W[:,:,0,0] = I\n",
    "W[:,:,0,1] = I2\n",
    "W[:,:,0,2] = M\n",
    "W[:,:,0,3] = M2\n",
    "for j in range(4,nbfilter11):\n",
    "    W[:,:,0,j] = np.random.randn(k,k)*0.2\n",
    "\n",
    "\n",
    "k2 = matrixsize12\n",
    "I = np.eye(k2)\n",
    "M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "I2=np.zeros((k2,k2))\n",
    "M2=np.zeros((k2,k2))\n",
    "for j in range(k2):\n",
    "    I2[:,j] = I[:,k2-j-1]\n",
    "    M2[:,j] = M[:,k2-j-1]   \n",
    "\n",
    "Z = np.zeros((k2,k2,nbfilter11,nbfilter12))\n",
    "\n",
    "for u in range(nbfilter12):\n",
    "    Z[:,:,u,0] = I\n",
    "    Z[:,:,u,1] = I2\n",
    "    Z[:,:,u,2] = M\n",
    "    Z[:,:,u,3] = M2\n",
    "    for p in range(4,nbfilter12):\n",
    "        Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Train on 100 samples, validate on 250 samples\n",
      "Epoch 1/25\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.8544 - val_loss: 0.7519\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.6518 - val_loss: 1.2051\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.5285 - val_loss: 1.0959\n",
      "Epoch 00003: early stopping\n",
      "predicting\n",
      "[0.2447393  0.34259072 0.08586895 ... 0.28511932 0.26527423 0.28291538]\n",
      "[0. 0. 0. ... 0. 0. 0.] [0.0 1 0.0 ... 0.0 0.0 0.0]\n",
      "batch_size :  64\n",
      "epochs :  25\n",
      "acc :  0.5024\n",
      "precision :  0.5\n",
      "sensitivity :  0.0008038585209003215\n",
      "specificity :  0.9992038216560509\n",
      "MCC :  0.00013582040505390896\n"
     ]
    }
   ],
   "source": [
    "# Création d'un modèle similaire à celui enregistré\n",
    "c2d1_input = keras.Input(shape=(36,36,1))\n",
    "cnn2d1 = Conv2D(filters = nbfilter11, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter11)])(c2d1_input)\n",
    "cnn2d1 = AveragePooling2D(pool_size=(3,3))(cnn2d1)\n",
    "cnn2d1 = Conv2D(filters = nbfilter12, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter12)])(cnn2d1)\n",
    "cnn2d1 = Dropout(0.1)(cnn2d1)\n",
    "cnn2d1 = Flatten()(cnn2d1)\n",
    "#cnn2d1 = get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12)(c2d1_input)\n",
    "#training_net1.append(cnn2d1)\n",
    "c2d2_input = keras.Input(shape=(36,36,1))\n",
    "cnn2d2 = Conv2D(filters = nbfilter11, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter11)])(c2d1_input)\n",
    "cnn2d2 = AveragePooling2D(pool_size=(3,3))(cnn2d2)\n",
    "cnn2d2 = Conv2D(filters = nbfilter12, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter12)])(cnn2d2)\n",
    "cnn2d2 = Dropout(0.1)(cnn2d2)\n",
    "cnn2d2 = Flatten()(cnn2d2)\n",
    "#cnn2d2 = get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22)(c2d2_input)\n",
    "#training_net1.append(cnn2d2)\n",
    "c1d1_input = keras.Input(shape=(36,4))\n",
    "cnn1d1 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d1_input)\n",
    "cnn1d1 = MaxPooling1D(pool_size=8)(cnn1d1)\n",
    "cnn1d1 = Dropout(0.2)(cnn1d1)\n",
    "cnn1d1 = Flatten()(cnn1d1)\n",
    "#cnn1d1 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d1_input)\n",
    "c1d2_input = keras.Input(shape=(36,4))\n",
    "cnn1d2 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d2_input)\n",
    "cnn1d2 = MaxPooling1D(pool_size=8)(cnn1d2)\n",
    "cnn1d2 = Dropout(0.2)(cnn1d2)\n",
    "cnn1d2 = Flatten()(cnn1d2)\n",
    "#cnn1d2 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d2_input)\n",
    "c1d3_input = keras.Input(shape=(36,4))\n",
    "cnn1d3 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d3_input)\n",
    "cnn1d3 = MaxPooling1D(pool_size=8)(cnn1d3)\n",
    "cnn1d3 = Dropout(0.2)(cnn1d3)\n",
    "cnn1d3 = Flatten()(cnn1d3)\n",
    "#cnn1d3 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d3_input)\n",
    "c1d4_input = keras.Input(shape=(36,4))\n",
    "cnn1d4 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d4_input)\n",
    "cnn1d4 = MaxPooling1D(pool_size=8)(cnn1d4)\n",
    "cnn1d4 = Dropout(0.2)(cnn1d4)\n",
    "cnn1d4 = Flatten()(cnn1d4)\n",
    "#cnn1d4 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d4_input)\n",
    "#training_net21.append(cnn1d1)\n",
    "#training_net21.append(cnn1d2)\n",
    "#training_net22.append(cnn1d3)\n",
    "#training_net22.append(cnn1d4)\n",
    "\n",
    "#model21 = Sequential()\n",
    "model21 = keras.layers.concatenate([cnn1d1,cnn1d2])\n",
    "model21 = Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model21)\n",
    "model21 = BatchNormalization()(model21)\n",
    "model21 = Activation('relu')(model21)\n",
    "model21 = Dropout(0.2)(model21)\n",
    "#model22 = Sequential()\n",
    "model22 = keras.layers.concatenate([cnn1d3,cnn1d4])\n",
    "model22 = Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model22)\n",
    "model22 = BatchNormalization()(model22)\n",
    "model22 = Activation('relu')(model22)\n",
    "model22 = Dropout(0.2)(model22)\n",
    "#training_net2.append(model21)\n",
    "#training_net2.append(model22)\n",
    "#model2 = Sequential()\n",
    "model2 = keras.layers.concatenate([model21,model22])\n",
    "model2 = Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Activation('relu')(model2)\n",
    "model2 = Dropout(0.2)(model2)\n",
    "#model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "#model2.add(BatchNormalization())\n",
    "#model2.add(Activation('relu'))\n",
    "#model2.add(Dropout(0.2))\n",
    "\n",
    "#model1 = Sequential()\n",
    "model1 = keras.layers.concatenate([cnn2d1,cnn2d2])\n",
    "model1 = Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model1)\n",
    "model1 = Dropout(0.1)(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "model1 = Activation('relu')(model1)\n",
    "\n",
    "#training_net.append(model1)\n",
    "#training_net.append(model2)\n",
    "#model = Sequential()\n",
    "model = keras.layers.concatenate([model1,model2])\n",
    "model = Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "#model.add(Dropout(0.1))\n",
    "if Dense5>0:\n",
    "    model = Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "model = Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "interaction_output = Activation('softmax')(model)\n",
    "#model.add(Activation('softmax'))\n",
    "interaction = Model(inputs=[c2d1_input,c2d2_input,c1d1_input,c1d2_input,c1d3_input,c1d4_input],outputs=[interaction_output])\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "interaction.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose = verbose1)\n",
    "print('model training')\n",
    "#model.fit(training, y, batch_size, epochs, verbose = verbose2, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "interaction.fit(training, y, batch_size, epochs, verbose = verbose2, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "# test   \n",
    "\n",
    "print('predicting')\n",
    "\n",
    "\n",
    "#predictions = model.predict_proba(testing)[:,1]\n",
    "predictions = interaction.predict(testing)[:,1]\n",
    "print(predictions)\n",
    "for i,nulll in enumerate(predictions):\n",
    "    predictions[i] = round(predictions[i])\n",
    "print(predictions,true_y)\n",
    "perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "print(\"batch_size : \", batch_size)\n",
    "print(\"epochs : \",epochs)\n",
    "print(\"acc : \", perfs[0])\n",
    "print(\"precision : \", perfs[1])\n",
    "print(\"sensitivity : \", perfs[2])\n",
    "print(\"specificity : \", perfs[3])\n",
    "print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Récupération des poids\n",
    "interaction.load_weights('model_double_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "[0.25861937 0.6298028  0.61370957 ... 0.36930397 0.05166985 0.23346059]\n",
      "[0. 1. 1. ... 0. 0. 0.] [0.0 0.0 1 ... 0.0 1 1]\n",
      "acc :  0.758\n",
      "precision :  0.8095238095238095\n",
      "sensitivity :  0.6932038834951456\n",
      "specificity :  0.8268041237113402\n",
      "MCC :  0.5234308508036729\n"
     ]
    }
   ],
   "source": [
    "# Test des performances (performances supérieures à la réalité car sur\n",
    "#  des données déjà apprises)\n",
    "\n",
    "perfs_test(interaction,Data[4],Data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
