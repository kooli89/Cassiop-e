{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction de la base d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_sequence(sequence,long):\n",
    "    nucleotides = np.zeros((4,int(max(long,25))))\n",
    "    cnt = 0\n",
    "    for lettre in sequence:\n",
    "        if lettre=='a':\n",
    "            nucleotides[0,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        elif lettre=='c':\n",
    "            nucleotides[1,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        elif lettre=='g':\n",
    "            nucleotides[2,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        else:\n",
    "            nucleotides[3,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_structure(structure):\n",
    "    long = len(structure)\n",
    "    a = np.zeros(25)\n",
    "    a[:long] = structure\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduire(seq,strand):\n",
    "    nucleotides = ''\n",
    "    if strand:\n",
    "        for lettre in seq:\n",
    "            if lettre=='A':\n",
    "                nucleotides=nucleotides+'a'\n",
    "            elif lettre=='C':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            elif lettre=='G':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'u'    \n",
    "    else:\n",
    "        string = ''\n",
    "        for i in seq:\n",
    "            string = i + string\n",
    "        seq = string\n",
    "        for lettre in seq:\n",
    "            if lettre=='A':\n",
    "                nucleotides=nucleotides+'u'\n",
    "            elif lettre=='C':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            elif lettre=='G':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'a'\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau de neurones basé sur iDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "#from keras.optimizers import kl_divergence\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "\n",
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(test_num, pred_y,  labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+ fp)\n",
    "    sensitivity = float(tp)/ (tp+fn)\n",
    "    specificity = float(tn)/(tn + fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "bdd = np.zeros((50000,35,35,1))\n",
    "B = np.zeros((6,6))\n",
    "B[3,3]=1\n",
    "for i in range(25000):\n",
    "    a = np.random.randint(2)\n",
    "    if a==0:\n",
    "        z=random.randint(2)\n",
    "        k=random.randint(2)\n",
    "        bdd[i,k:6+k,:6,0] = np.eye(6)-z*B\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,6+k:12+k,6:12,0] = np.eye(6)\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,12+k:18+k,12:18,0] = np.eye(6)-z*B\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,18+k:24+k,18:24,0] = np.eye(6)\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,24+k:30+k,24:30,0] = np.eye(6)-z*B\n",
    "    elif a==1: \n",
    "        z=random.randint(2)\n",
    "        k=random.randint(2)\n",
    "        bdd[i,29-k:35-k,29:35,0] = np.eye(6)-z*B\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,23-k:29-k,23:29,0] = np.eye(6)\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,17-k:23-k,17:23,0] = np.eye(6)-z*B\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,11-k:17-k,11:17,0] = np.eye(6)\n",
    "        k=k+random.randint(2)\n",
    "        bdd[i,5-k:11-k,5:11,0] = np.eye(6)-z*B\n",
    "    b = random.randint(6)\n",
    "    bruit = np.zeros((35,35))\n",
    "    #bruit[:12+b,23-b:] = random.randint(0,2,(12+b,12+b))\n",
    "    #bruit[23-b:,:12+b] = random.randint(0,2,(12+b,12+b))\n",
    "    #bruit[:12+b,23-b:] = (random.rand(12+b,12+b)*3/4+0.53125).astype(int)\n",
    "    #bruit[23-b:,:12+b] = (random.rand(12+b,12+b)*3/4+0.53125).astype(int)\n",
    "    for c in range(2,35):\n",
    "        bruit = np.diag((random.rand(35-c)*3/4+0.53125).astype(int),c)\n",
    "        bdd[i,:,:,0] = bdd[i,:,:,0] + bruit\n",
    "        bruit = np.diag((random.rand(35-c)*3/4+0.53125).astype(int),-c)\n",
    "        bdd[i,:,:,0] = bdd[i,:,:,0] + bruit\n",
    "    \n",
    "for i in range(12500,25000):\n",
    "    p = np.zeros((35,35))\n",
    "    for j in range(35):        \n",
    "        p[:,j] = bdd[i,:,35-j-1,0]\n",
    "    bdd[i,:,:,0] = p\n",
    "        \n",
    "for i in range(25000,50000):\n",
    "    bdd[i,:,:,0] = np.random.rand(35,35)*3/4\n",
    "    for j in range(35):\n",
    "        for k in range(35):\n",
    "            bdd[i,j,k,0] = round(bdd[i,j,k,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25000):\n",
    "    for k in range(35):\n",
    "        for q in range(35):\n",
    "            bdd[i,k,q,0] = bdd[i,k,q,0]%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining\n",
    "\n",
    "nb_train = 40000\n",
    "nb_val = 2000\n",
    "nb_test = 2000\n",
    "\n",
    "indices = np.arange(len(bdd))\n",
    "shuffle(indices)\n",
    "bdd = bdd[indices]\n",
    "labels = np.zeros(50000)\n",
    "labels[:25000] = np.ones(25000)\n",
    "labels = labels[indices]\n",
    "\n",
    "training = []\n",
    "training.append(bdd[:nb_train])\n",
    "validation=[]\n",
    "validation.append(bdd[nb_train:nb_train+nb_val])\n",
    "testing=[]\n",
    "testing.append(bdd[-nb_test:])\n",
    "bdd=[]\n",
    "y = labels[:nb_train]\n",
    "y = keras.utils.np_utils.to_categorical(y,2)\n",
    "val_y = labels[nb_train:nb_train+nb_val]\n",
    "val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "true_y = labels[-nb_test:]\n",
    "indices=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_pretraining():\n",
    "    k = 6\n",
    "    nbfilter = 24\n",
    "    # init_weights\n",
    "    I = np.eye(k)\n",
    "    M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "    I2 = np.zeros((k,k))\n",
    "    M2 = np.zeros((k,k))\n",
    "    for j in range(k):\n",
    "        I2[:,j] = I[:,k-j-1]\n",
    "        M2[:,j] = M[:,k-j-1]        \n",
    "    W = np.zeros((k,k,1,nbfilter))\n",
    "    W[:,:,0,0] = I\n",
    "    W[:,:,0,1] = I2\n",
    "    W[:,:,0,2] = M\n",
    "    W[:,:,0,3] = M2\n",
    "    for j in range(4,nbfilter):\n",
    "        W[:,:,0,j] = np.random.randn(k,k)*0.2        \n",
    "    # cnn\n",
    "    print('configure cnn network')\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = nbfilter, kernel_size=(k,k), padding='valid', input_shape=(35,35,1),strides=(1,1),weights=[W,np.zeros(nbfilter)]))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(AveragePooling2D(pool_size=(3,3)))\n",
    "    print(model.output_shape)\n",
    "    nbfilter2 = 4\n",
    "    k2 = 7\n",
    "    #\n",
    "    I = np.eye(k2)\n",
    "    M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "    I2=np.zeros((k2,k2))\n",
    "    M2=np.zeros((k2,k2))\n",
    "    for j in range(k2):\n",
    "        I2[:,j] = I[:,k2-j-1]\n",
    "        M2[:,j] = M[:,k2-j-1]   \n",
    "    Z = np.zeros((k2,k2,nbfilter,nbfilter2))\n",
    "    for u in range(nbfilter):\n",
    "        Z[:,:,u,0] = I\n",
    "        Z[:,:,u,1] = I2\n",
    "        Z[:,:,u,2] = M\n",
    "        Z[:,:,u,3] = M2\n",
    "        for p in range(4,nbfilter2):\n",
    "            Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            \n",
    "    #\n",
    "    model.add(Conv2D(filters = nbfilter2, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter2)]))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_pretraining():\n",
    "    \n",
    "    print('training', nb_train)\n",
    "    \n",
    "    training_net=get_cnn_pretraining()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(training_net)\n",
    "    model.add(Dense(64,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "        \n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "    print('model training')\n",
    "    model.fit(training[0], y, batch_size=16, epochs=2, verbose=1, validation_data=(validation[0], val_y), callbacks=[earlystopper])\n",
    "    \n",
    "    # test       \n",
    "    print('predicting')            \n",
    "    predictions = model.predict_proba(testing[0])[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training', 40000)\n",
      "configure cnn network\n",
      "(None, 10, 10, 24)\n",
      "(None, 4, 4, 4)\n",
      "model training\n",
      "Train on 40000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 23s 565us/step - loss: 0.2195 - val_loss: 0.6823\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 21s 537us/step - loss: 0.1394 - val_loss: 0.0665\n",
      "predicting\n",
      "[9.9376291e-01 9.9633539e-01 9.9835771e-01 ... 1.5251886e-03 6.0232681e-01\n",
      " 5.6996569e-04]\n",
      "(array([1., 1., 1., ..., 0., 1., 0.], dtype=float32), array([1., 1., 1., ..., 0., 1., 0.]))\n",
      "('acc : ', 0.978)\n",
      "('precision : ', 0.9906054279749478)\n",
      "('sensitivity : ', 0.9644308943089431)\n",
      "('specificity : ', 0.9911417322834646)\n",
      "('MCC : ', 0.9562941291795969)\n"
     ]
    }
   ],
   "source": [
    "pretrain = network_pretraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "poidscnn1 = pretrain.get_weights()[0]\n",
    "bias1 = pretrain.get_weights()[1]\n",
    "poidscnn2 = pretrain.get_weights()[2]\n",
    "bias2 = pretrain.get_weights()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "[4.42874409e-07 6.36836630e-04 2.39901328e-06 ... 4.62607801e-08\n",
      " 1.06100806e-07 9.82342954e-07]\n",
      "(array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([1, 1, 0.0, ..., 0.0, 1, 1], dtype=object))\n",
      "('acc : ', 0.50258)\n",
      "('precision : ', 0.6356877323420075)\n",
      "('sensitivity : ', 0.006855355997434253)\n",
      "('specificity : ', 0.996088761174968)\n",
      "('MCC : ', 0.020123535001122184)\n"
     ]
    }
   ],
   "source": [
    "# Test  predict rna with prenetwork (avec les données RNA)\n",
    "print('predicting')            \n",
    "predictions = pretrain.predict_proba(testing[0])[:,1]\n",
    "print(predictions)\n",
    "for i,nulll in enumerate(predictions):\n",
    "    predictions[i] = round(predictions[i])\n",
    "print(predictions,true_y)\n",
    "perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "print(\"acc : \", perfs[0])\n",
    "print(\"precision : \", perfs[1])\n",
    "print(\"sensitivity : \", perfs[2])\n",
    "print(\"specificity : \", perfs[3])\n",
    "print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26017\n",
      "24222\n",
      "37991\n",
      "26023\n",
      "24229\n",
      "37993\n"
     ]
    }
   ],
   "source": [
    "verif = []\n",
    "def clean(seq):\n",
    "    seq = seq.split('\\n')\n",
    "    seq2 = ''\n",
    "    for j in seq:\n",
    "        seq2 = seq2 + j\n",
    "    seq2 = seq2[2:len(seq2)-1]\n",
    "    seq2 = seq2.split(' ')\n",
    "    #print(seq2)\n",
    "    seq3=[]\n",
    "    for j in seq2:\n",
    "        #print(j)\n",
    "        if j=='':\n",
    "            a=0\n",
    "        else:\n",
    "            seq3.append(float(j))\n",
    "    return seq3\n",
    "\n",
    "verif = pd.read_csv(\"genomes/negatifs_m-m-str.csv\", sep = \"\\t\",header=None)\n",
    "verif = np.array(verif)\n",
    "print(len(verif))\n",
    "for i in range(len(verif)):\n",
    "    for j in range(2):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "neg1 = verif\n",
    "\n",
    "verif=[]\n",
    "\n",
    "verif = pd.read_csv(\"genomes/negatifs_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "verif = np.array(verif)\n",
    "print(len(verif))\n",
    "for i in range(len(verif)):\n",
    "    for j in range(2):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "neg2 = verif\n",
    "verif=[]\n",
    "\n",
    "verif = pd.read_csv(\"genomes/negatifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "verif = np.array(verif)\n",
    "print(len(verif))\n",
    "for i in range(len(verif)):\n",
    "    for j in range(2):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "neg3 = verif\n",
    "verif=[]\n",
    "\n",
    "\n",
    "verif = pd.read_csv(\"genomes/positifs_m-m-str.csv\", sep = \"\\t\",header=None)  \n",
    "verif = np.array(verif)\n",
    "print(len(verif))\n",
    "for i in range(len(verif)):\n",
    "    for j in range(2):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "pos1 = verif\n",
    "l = len(pos1)\n",
    "verif=[]\n",
    "\n",
    "verif = pd.read_csv(\"genomes/positifs_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "verif = np.array(verif)\n",
    "print(len(verif))\n",
    "for i in range(len(verif)):\n",
    "    for j in range(2):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "pos2 = verif\n",
    "l=l+len(pos2)\n",
    "verif=[]\n",
    "\n",
    "verif = pd.read_csv(\"genomes/positifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "verif = np.array(verif)\n",
    "print(len(verif))\n",
    "for i in range(len(verif)):\n",
    "    for j in range(2):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "pos3 = verif\n",
    "l=l+len(pos3)\n",
    "verif=[]\n",
    "\n",
    "bdd = np.concatenate((pos1,pos2,pos3,neg1,neg2,neg3))\n",
    "\n",
    "\n",
    "pos1 = []\n",
    "neg1 = []\n",
    "pos2 = []\n",
    "neg2 = []\n",
    "\n",
    "labels = np.zeros((len(bdd),1))\n",
    "\n",
    "bdd = np.concatenate((bdd,labels),axis=1)\n",
    "for i in range(l):\n",
    "    bdd[i,4]=1\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176475"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train = 165000\n",
    "nb_val = 6000\n",
    "nb_test = 5000 #initial avant retour à 50%-50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle total\n",
    "\n",
    "indices = np.arange(len(bdd))\n",
    "shuffle(indices)\n",
    "bdd = bdd[indices]\n",
    "indices=[]\n",
    "\n",
    "# \n",
    "l = len(bdd)\n",
    "matrice = np.zeros((l,36,36,1)) #4\n",
    "matrice1 = np.zeros((l,36,36,1))\n",
    "matrice2 = np.zeros((l,36,4))\n",
    "matrice3 = np.zeros((l,36,4))\n",
    "\n",
    "for i in range(l):\n",
    "    seq1 = bdd[i,0]\n",
    "    seq2 = bdd[i,1]\n",
    "    prob1 = np.array(clean(bdd[i,2]))\n",
    "    prob2 = np.array(clean(bdd[i,3]))\n",
    "    for j in range(len(seq1)):\n",
    "        for k in range(len(seq2)):\n",
    "            if (seq1[j]=='a' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='a'):\n",
    "                matrice[i,j,k,0] = 1\n",
    "                #matrice[i,j,k,1] = prob1[j]+prob2[k]\n",
    "            elif (seq1[j]=='g' and seq2[k]=='c') or (seq1[j]=='c' and seq2[k]=='g'):\n",
    "                matrice[i,j,k,0] = 1\n",
    "                #matrice[i,j,k,1] = prob1[j]+prob2[k]\n",
    "            elif (seq1[j]=='g' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='g'):\n",
    "                matrice[i,j,k,0] = 1\n",
    "                #matrice[i,j,k,1] = prob1[j]+prob2[k]\n",
    "            matrice1[i,j,k,0] = prob1[j]+prob2[k]\n",
    "    for j in range(len(seq1)):\n",
    "        if seq1[j]=='a':\n",
    "            matrice2[i,j,0] = 1\n",
    "        elif seq1[j]=='u':\n",
    "            matrice2[i,j,1] = 1\n",
    "        elif seq1[j]=='g':\n",
    "            matrice2[i,j,2] = 1\n",
    "        elif seq1[j]=='c':\n",
    "            matrice2[i,j,3] = 1\n",
    "    for j in range(len(seq2)):\n",
    "        if seq2[j]=='a':\n",
    "            matrice3[i,j,0] = 1\n",
    "        elif seq2[j]=='u':\n",
    "            matrice3[i,j,1] = 1\n",
    "        elif seq2[j]=='g':\n",
    "            matrice3[i,j,2] = 1\n",
    "        elif seq2[j]=='c':\n",
    "            matrice3[i,j,3] = 1                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argtest0 = np.argwhere(bdd[-nb_test:,2]==1)[:,0]\n",
    "#argtest1 = np.argwhere(bdd[-nb_test:,2]==0)[:,0][:len(argtest0)]\n",
    "#argtest=np.concatenate((argtest0,argtest1))+len(bdd)-nb_test\n",
    "#argtest0=[]\n",
    "#argtest1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(bdd[argtest,2])/len(argtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "training.append(matrice[:nb_train])\n",
    "training.append(matrice1[:nb_train])\n",
    "training.append(matrice2[:nb_train])\n",
    "training.append(matrice3[:nb_train])\n",
    "\n",
    "validation = []\n",
    "validation.append(matrice[nb_train:nb_train+nb_val])\n",
    "validation.append(matrice1[nb_train:nb_train+nb_val])\n",
    "validation.append(matrice2[nb_train:nb_train+nb_val])\n",
    "validation.append(matrice3[nb_train:nb_train+nb_val])\n",
    "\n",
    "testing = []\n",
    "testing.append(matrice[-nb_test:])\n",
    "testing.append(matrice1[-nb_test:])\n",
    "testing.append(matrice2[-nb_test:])\n",
    "testing.append(matrice3[-nb_test:])\n",
    "\n",
    "labels = bdd[:,4]\n",
    "bdd = []\n",
    "y = labels[:nb_train]\n",
    "y = keras.utils.np_utils.to_categorical(y,2)\n",
    "val_y = labels[nb_train:nb_train+nb_val]\n",
    "val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "true_y = labels[-nb_test:]\n",
    "argtest=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82487.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training', 165000)\n",
      "configure cnn network\n",
      "(None, 10, 10, 24)\n",
      "(None, 4, 4, 6)\n",
      "configure cnn network\n",
      "(None, 10, 10, 24)\n",
      "(None, 4, 4, 6)\n",
      "(None, 5, 64)\n",
      "(None, 5, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:20: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Train on 165000 samples, validate on 6000 samples\n",
      "Epoch 1/6\n",
      "165000/165000 [==============================] - 160s 972us/step - loss: 0.5952 - val_loss: 0.5607\n",
      "Epoch 2/6\n",
      "165000/165000 [==============================] - 159s 962us/step - loss: 0.5473 - val_loss: 0.5262\n",
      "Epoch 3/6\n",
      "165000/165000 [==============================] - 159s 963us/step - loss: 0.5250 - val_loss: 0.5283\n",
      "Epoch 4/6\n",
      "165000/165000 [==============================] - 159s 962us/step - loss: 0.5096 - val_loss: 0.5230\n",
      "Epoch 5/6\n",
      "165000/165000 [==============================] - 162s 982us/step - loss: 0.4941 - val_loss: 0.5046\n",
      "Epoch 6/6\n",
      "165000/165000 [==============================] - 162s 980us/step - loss: 0.4798 - val_loss: 0.5000\n",
      "predicting\n",
      "[0.92890245 0.9656374  0.30589166 ... 0.03495842 0.27011627 0.8712664 ]\n",
      "(array([1., 1., 0., ..., 0., 0., 1.], dtype=float32), array([1, 1, 1, ..., 0.0, 0.0, 0.0], dtype=object))\n",
      "('acc : ', 0.7554)\n",
      "('precision : ', 0.7743929359823399)\n",
      "('sensitivity : ', 0.7112733171127331)\n",
      "('specificity : ', 0.7983425414364641)\n",
      "('MCC : ', 0.5118350316396348)\n"
     ]
    }
   ],
   "source": [
    "m = merge_networks_train_predict_doublemerge2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model training\n",
    "Train on 165000 samples, validate on 6000 samples\n",
    "Epoch 1/6\n",
    "165000/165000 [==============================] - 160s 972us/step - loss: 0.5952 - val_loss: 0.5607\n",
    "Epoch 2/6\n",
    "165000/165000 [==============================] - 159s 962us/step - loss: 0.5473 - val_loss: 0.5262\n",
    "Epoch 3/6\n",
    "165000/165000 [==============================] - 159s 963us/step - loss: 0.5250 - val_loss: 0.5283\n",
    "Epoch 4/6\n",
    "165000/165000 [==============================] - 159s 962us/step - loss: 0.5096 - val_loss: 0.5230\n",
    "Epoch 5/6\n",
    "165000/165000 [==============================] - 162s 982us/step - loss: 0.4941 - val_loss: 0.5046\n",
    "Epoch 6/6\n",
    "165000/165000 [==============================] - 162s 980us/step - loss: 0.4798 - val_loss: 0.5000\n",
    "predicting\n",
    "[0.92890245 0.9656374  0.30589166 ... 0.03495842 0.27011627 0.8712664 ]\n",
    "(array([1., 1., 0., ..., 0., 0., 1.], dtype=float32), array([1, 1, 1, ..., 0.0, 0.0, 0.0], dtype=object))\n",
    "##### ('acc : ', 0.7554)\n",
    "('precision : ', 0.7743929359823399)\n",
    "('sensitivity : ', 0.7112733171127331)\n",
    "('specificity : ', 0.7983425414364641)\n",
    "('MCC : ', 0.5118350316396348)\n",
    "dense-merge2 512 dense-merge1 128 dense 512-128-64-2 cnn24/6 avg3 cnn64,7 max6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_networks_train_predict_doublemerge2():\n",
    "    \n",
    "    print('training', nb_train)\n",
    "    training_net=[]\n",
    "    training_net1 = []\n",
    "    training_net2 = []\n",
    "    training_net1.append(get_cnn_network3())\n",
    "    training_net1.append(get_cnn_network3())\n",
    "    training_net2.append(get_cnn_network_seq2())\n",
    "    training_net2.append(get_cnn_network_seq2())\n",
    "    \n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(Merge(training_net2, mode ='concat'))\n",
    "    model2.add(Dense(512,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Activation('relu'))\n",
    "    #model2.add(Dropout(0.1))\n",
    "    model1 = Sequential()\n",
    "    model1.add(Merge(training_net1, mode ='concat'))\n",
    "    model1.add(Dense(128,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model1.add(Dropout(0.1))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Activation('relu'))\n",
    "    training_net.append(model1)\n",
    "    training_net.append(model2)\n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    "    \n",
    "    model.add(Dense(512,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(128,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(64,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "        \n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "    \n",
    "    \n",
    "    print('model training')\n",
    "    model.fit(training, y, batch_size=16, epochs=6, verbose=1, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "    \n",
    "    # test   \n",
    "    \n",
    "    print('predicting')\n",
    "    \n",
    "        \n",
    "    predictions = model.predict_proba(testing)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network3():    \n",
    "    k = 6\n",
    "    nbfilter = 24\n",
    "    # init_weights\n",
    "    I = np.eye(k)\n",
    "    M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "    I2 = np.zeros((k,k))\n",
    "    M2 = np.zeros((k,k))\n",
    "    for j in range(k):\n",
    "        I2[:,j] = I[:,k-j-1]\n",
    "        M2[:,j] = M[:,k-j-1]        \n",
    "    W = np.zeros((k,k,1,nbfilter))\n",
    "    W[:,:,0,0] = I\n",
    "    #W[:,:,1,0] = I\n",
    "    W[:,:,0,1] = I2\n",
    "   # W[:,:,1,1] = I2\n",
    "    W[:,:,0,2] = M\n",
    "   # W[:,:,1,2] = M\n",
    "    W[:,:,0,3] = M2\n",
    "   # W[:,:,1,3] = M2\n",
    "    for j in range(4,nbfilter):\n",
    "        W[:,:,0,j] = np.random.randn(k,k)*0.2\n",
    "       # W[:,:,1,j] = np.random.randn(k,k)*0.2\n",
    "    # cnn\n",
    "    print('configure cnn network')\n",
    " \n",
    "    model = Sequential()\n",
    "    #model.add(Conv2D(filters = nbfilter, kernel_size=(k,k), padding='valid', input_shape=(36,36,2),strides=(1,1),weights=[poidscnn1,bias1]))\n",
    "    model.add(Conv2D(filters = nbfilter, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter)]))    \n",
    "    #model.add(BatchNormalization())    \n",
    "    #model.add(Activation('sigmoid'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(AveragePooling2D(pool_size=(3,3)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    nbfilter2 = 6\n",
    "    k2 = 7\n",
    "    #\n",
    "    I = np.eye(k2)\n",
    "    M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "    I2=np.zeros((k2,k2))\n",
    "    M2=np.zeros((k2,k2))\n",
    "    for j in range(k2):\n",
    "        I2[:,j] = I[:,k2-j-1]\n",
    "        M2[:,j] = M[:,k2-j-1]   \n",
    "    Z = np.zeros((k2,k2,nbfilter,nbfilter2))\n",
    "    for u in range(nbfilter):\n",
    "        Z[:,:,u,0] = I\n",
    "        Z[:,:,u,1] = I2\n",
    "        Z[:,:,u,2] = M\n",
    "        Z[:,:,u,3] = M2\n",
    "        for p in range(4,nbfilter2):\n",
    "            Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            \n",
    "    #\n",
    "    #model.add(Conv2D(filters = nbfilter2, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[poidscnn2,bias2]))\n",
    "    model.add(Conv2D(filters = nbfilter2, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter2)]))\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network_seq2():\n",
    "    f = 7\n",
    "    nb = 64\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = nb, kernel_size = f, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    model.add(MaxPooling1D(pool_size=6))\n",
    "    print(model.output_shape)\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('acc : ', 0.6827445652173914)\n",
    "('precision : ', 0.6881118881118881)\n",
    "('sensitivity : ', 0.6684782608695652)\n",
    "('specificity : ', 0.6970108695652174)\n",
    "('MCC : ', 0.3656379954692535)\n",
    "\n",
    "Deux couches pour cnn1 24/4 dense512 pour merge1 puis dense128-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network_seq():\n",
    "    f = 7\n",
    "    nb = 128\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = nb, kernel_size = f, strides = 1, padding = 'valid', input_shape=(35,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_networks_train_predict():\n",
    "    \n",
    "    print('training', nb_train)\n",
    "    \n",
    "    #hid = 64*8*8\n",
    "    training_net = []\n",
    "    training_net.append(get_cnn_network2())\n",
    "    training_net.append(get_cnn_network_seq())\n",
    "    training_net.append(get_cnn_network_seq())\n",
    "\n",
    "    #y, encoder = preprocess_labels(training_label)\n",
    "    #val_y, encoder = preprocess_labels(validation_label, encoder = encoder)\n",
    "       \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode ='concat'))\n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    #print(hid)\n",
    "    model.add(Dense(256,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "    #checkpointer = ModelCheckpoint(filepath=\"models/bestmodel.hdf5\", verbose=0, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    #validation_data=(np.transpose(validmat['validxdata'],axes=(0,2,1)), validmat['validdata']), callbacks=[checkpointer,earlystopper]\n",
    "    print('model training')\n",
    "    model.fit(training, y, batch_size=64, epochs=15, verbose=0, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "    \n",
    "    # test   \n",
    "    \n",
    "    print('predicting')\n",
    "    \n",
    "        \n",
    "    predictions = model.predict_proba(testing)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network2():    \n",
    "    k = 5\n",
    "    nbfilter = 24\n",
    "    # init_weights\n",
    "    I = np.eye(k)\n",
    "    M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "    I2 = np.zeros((k,k))\n",
    "    M2 = np.zeros((k,k))\n",
    "    for j in range(k):\n",
    "        I2[:,j] = I[:,k-j-1]\n",
    "        M2[:,j] = M[:,k-j-1]        \n",
    "    W = np.zeros((k,k,1,nbfilter))\n",
    "    W[:,:,0,0] = I\n",
    "    W[:,:,0,1] = I2\n",
    "    W[:,:,0,2] = M\n",
    "    W[:,:,0,3] = M2\n",
    "    for j in range(4,nbfilter):\n",
    "        W[:,:,0,j] = np.random.randn(k,k)*0.2        \n",
    "    # cnn\n",
    "    print('configure cnn network')\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = nbfilter, kernel_size=(k,k), padding='valid', input_shape=(35,35,1),strides=(1,1),weights=[W,np.zeros(nbfilter)]))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('sigmoid'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(AveragePooling2D(pool_size=(3,3)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(training[0][0,:24,:24,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(training[0][3,12:,:24,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=35*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a=2\n",
    "print((matrice_train[a,:,:,0]))[:24,:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération rapide des séquences à partir de chr_nb start end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download genome at ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_24/GRCh38.primary_assembly.genome.fa.gz\n",
    "\n",
    "parser = SeqIO.parse(open(\"genomes/GRCh38.genome.fa\"),\"fasta\")\n",
    "\n",
    "dict_fasta = dict([(seq.id, seq) for seq in parser])\n",
    "\n",
    "ide, begin, end = ['chr17',8222876,8222895]\n",
    "\n",
    "dict_fasta[ide][begin:end].format(\"fasta\").split('\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir de là c'est l'ancienne méthode (à conserver au cas où, archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On  sauvegarde la base de données dans un fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "import csv\n",
    "\n",
    "writer = csv.writer(open(\"genomes/database.csv\", 'w',newline=''),delimiter=\"\\t\")\n",
    "for i,el in enumerate(varrbdd):\n",
    "    row = transformer_database2(varrbdd[i])    \n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche les trous dans les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_5000_trouee = pd.read_csv(\"genomes/database.csv\", sep = \"\\t\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4983"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdd_5000_trouee = np.array(bdd_5000_trouee)\n",
    "l = []\n",
    "for i,el in enumerate(bdd_5000_trouee):\n",
    "    if type(bdd_5000_trouee[i,0])==float or type(bdd_5000_trouee[i,1])==float:\n",
    "        a=0\n",
    "    elif len(bdd_5000_trouee[i,0])==0 or len(bdd_5000_trouee[i,0])==0:\n",
    "        a=0\n",
    "    else:\n",
    "        l.append(i)\n",
    "l = np.array(l)\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_miRNA_mRNA = bdd_5000_trouee[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_miRNA_mRNA = nettoyer(bdd_miRNA_mRNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_miRNA_mRNA = traduire_séquences(bdd_miRNA_mRNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_apprentissage = construire_seq_et_structure_bdd(bdd_miRNA_mRNA[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir de mainenant, on essaye d'obtenir les probabilités d'accesibilité de chaque nucléotide sur une séquence quelconque ARN par exemple 'ACGUUGUCACACGAU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traduire_test(seq,strand):\n",
    "    nucleotides = ''\n",
    "    if strand:\n",
    "        string = ''\n",
    "        for i in seq:\n",
    "            string = i + string\n",
    "        seq = string\n",
    "        for lettre in seq:\n",
    "            if lettre=='a':\n",
    "                nucleotides=nucleotides+'u'\n",
    "            elif lettre=='c':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            elif lettre=='g':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'a'\n",
    "    else:\n",
    "        for lettre in seq:\n",
    "            if lettre=='a':\n",
    "                nucleotides=nucleotides+'a'\n",
    "            elif lettre=='c':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            elif lettre=='g':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'u'    \n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cacuacagcaagccuggcacgaccucuaaggcgguucgcggcaacguccgcacgucggcucguuggucuagggguaugauucucgcuucgggugcgagaggucccggguucaaaucccggacgagcccuccuuuaccuuuuacugagacaagagugucuuc'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traduire_test('gaagacactcttgtctcagtaaaaggtaaaggagggctcgtccgggatttgaacccgggacctctcgcacccgaagcgagaatcatacccctagaccaacgagccgacgtgcggacgttgccgcgaaccgccttagaggtcgtgccaggcttgctgtagtg',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'gaagacactcttgtctcagtaaaaggtaaaggagggctcgtccgggatttgaacccgggacctctcgcacccgaagcgagaatcatacccctagaccaacgagccgacgtgcggacgttgccgcgaaccgccttagaggtcgtgccaggcttgctgtagtg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/DATA/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (1,2,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "bdd = pd.read_csv(\"rise_human_transcriptome (copie).csv\", sep = \"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bdd convertie panda -> numpy array et seuls les 70 000 premiers RRIs retenus\n",
    "arrbdd = np.array(bdd)[:70000]\n",
    "\n",
    "#enlever 'chr' et formater les numéros de chromosomes\n",
    "for i,el in enumerate(arrbdd[:,0]):\n",
    "    arrbdd[i,0] = arrbdd[i,0][3:]\n",
    "\n",
    "for i,el in enumerate(arrbdd[:,0]):\n",
    "    if arrbdd[i,0] == 'X':\n",
    "        arrbdd[i,0] = '23'\n",
    "    if arrbdd[i,0] == 'Y':\n",
    "        arrbdd[i,0] = '24'\n",
    "    if arrbdd[i,0] == '1':\n",
    "        arrbdd[i,0] = '01'\n",
    "    if arrbdd[i,0] == '2':\n",
    "        arrbdd[i,0] = '02'\n",
    "    if arrbdd[i,0] == '3':\n",
    "        arrbdd[i,0] = '03'\n",
    "    if arrbdd[i,0] == '4':\n",
    "        arrbdd[i,0] = '04'\n",
    "    if arrbdd[i,0] == '5':\n",
    "        arrbdd[i,0] = '05'\n",
    "    if arrbdd[i,0] == '6':\n",
    "        arrbdd[i,0] = '06'\n",
    "    if arrbdd[i,0] == '7':\n",
    "        arrbdd[i,0] = '07'\n",
    "    if arrbdd[i,0] == '8':\n",
    "        arrbdd[i,0] = '08'\n",
    "    if arrbdd[i,0] == '9':\n",
    "        arrbdd[i,0] = '09'\n",
    "\n",
    "for i,el in enumerate(arrbdd[:,3]):\n",
    "    arrbdd[i,3] = arrbdd[i,3][3:]\n",
    "\n",
    "for i,el in enumerate(arrbdd[:,3]):\n",
    "    if arrbdd[i,3] == 'X':\n",
    "        arrbdd[i,3] = '23'\n",
    "    elif arrbdd[i,3] == 'Y':\n",
    "        arrbdd[i,3] = '24'  \n",
    "    elif arrbdd[i,3] == '1':\n",
    "        arrbdd[i,3] = '01'\n",
    "    elif arrbdd[i,3] == '2':\n",
    "        arrbdd[i,3] = '02'\n",
    "    elif arrbdd[i,3] == '3':\n",
    "        arrbdd[i,3] = '03'\n",
    "    elif arrbdd[i,3] == '4':\n",
    "        arrbdd[i,3] = '04'\n",
    "    elif arrbdd[i,3] == '5':\n",
    "        arrbdd[i,3] = '05'\n",
    "    elif arrbdd[i,3] == '6':\n",
    "        arrbdd[i,3] = '06'\n",
    "    elif arrbdd[i,3] == '7':\n",
    "        arrbdd[i,3] = '07'\n",
    "    elif arrbdd[i,3] == '8':\n",
    "        arrbdd[i,3] = '08'\n",
    "    elif arrbdd[i,3] == '9':\n",
    "        arrbdd[i,3] = '09'\n",
    "\n",
    "\n",
    "#enlever les chromosomes M\n",
    "arg_sansM = np.argwhere((arrbdd[:,0] != 'M') & (arrbdd[:,3] != 'M'))\n",
    "varrbdd = arrbdd[arg_sansM[:,0]]\n",
    "\n",
    "#changement strand\n",
    "for i,el in enumerate(varrbdd[:,8]):\n",
    "    if varrbdd[i,8] == '+':\n",
    "        varrbdd[i,8] = 'true'\n",
    "    else:\n",
    "        varrbdd[i,8] = 'false'\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,9]):\n",
    "    if varrbdd[i,9] == '+':\n",
    "        varrbdd[i,9] = 'true'\n",
    "    else:\n",
    "        varrbdd[i,9] = 'false'\n",
    "\n",
    "\n",
    "#int str\n",
    "for i,el in enumerate(varrbdd[:,1]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,1] = str(el)\n",
    "for i,el in enumerate(varrbdd[:,2]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,2] = str(el)\n",
    "for i,el in enumerate(varrbdd[:,4]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,4] = str(el)\n",
    "for i,el in enumerate(varrbdd[:,5]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,5] = str(el)\n",
    "\n",
    "#génération position début et fin de séquence (nucléotides)\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,1]):\n",
    "    milieu = int((int(varrbdd[i,1])+int(varrbdd[i,2]))/2)\n",
    "    varrbdd[i,1] = str(milieu-50)\n",
    "    varrbdd[i,2] = str(milieu+50)\n",
    "    milieu = int((int(varrbdd[i,4])+int(varrbdd[i,5]))/2)\n",
    "    varrbdd[i,4] = str(milieu-50)\n",
    "    varrbdd[i,5] = str(milieu+50)\n",
    "\n",
    "# Génération URLs\n",
    "\n",
    "#for i,el in enumerate(varrbdd[:,6]):\n",
    " #   varrbdd[i,6] = \"https://www.ncbi.nlm.nih.gov/nuccore/NC_0000\"+varrbdd[i,0]+\"?report=genbank&from=\"+varrbdd[i,1]+\"&to=\"+varrbdd[i,2]+\"&strand=\"+varrbdd[i,8]+\".html\"\n",
    "#for i,el in enumerate(varrbdd[:,6]):\n",
    " #   varrbdd[i,6] = 'https://www.ncbi.nlm.nih.gov/nuccore/NC_0000'+varrbdd[i,0]+'?report=fasta&log$=seqview&format=text&from='+varrbdd[i,1]+'&to='+varrbdd[i,2]\n",
    "\n",
    "#for i,el in enumerate(varrbdd[:,7]):\n",
    " #   varrbdd[i,7] = \"https://www.ncbi.nlm.nih.gov/nuccore/NC_0000\"+varrbdd[i,3]+\"?report=genbank&from=\"+varrbdd[i,4]+\"&to=\"+varrbdd[i,5]+\"&strand=\"+varrbdd[i,9]+\".html\"\n",
    "#for i,el in enumerate(varrbdd[:,7]):\n",
    " #   varrbdd[i,7] = 'https://www.ncbi.nlm.nih.gov/nuccore/NC_0000'+varrbdd[i,3]+'?report=fasta&log$=seqview&format=text&from='+varrbdd[i,4]+'&to='+varrbdd[i,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recuperer_sequence(url):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    time.sleep(4)\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    browser.quit()\n",
    "    soup = BeautifulSoup(innerHTML,\"lxml\")\n",
    "    seq = soup.find_all(\"span\",{'class':\"ff_line\"})\n",
    "    rep = seq[0].text+seq[1].text+seq[2].text+seq[3].text\n",
    "    rep = ''.join(rep.split())\n",
    "    return rep\n",
    "\n",
    "def recuperer_sequence2(url):\n",
    "    browser.get(url)\n",
    "    time.sleep(1.3)\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,\"lxml\")\n",
    "    seq = soup.find_all(\"div\",{'class':\"seq gbff\"})\n",
    "    rep = seq[0].text   \n",
    "    return rep\n",
    "\n",
    "def transformer_database(bddrow):\n",
    "    sequence=recuperer_sequence(bddrow[6])\n",
    "    sequence=transformer_sequence(sequence,len(sequence))\n",
    "    sequencebis=recuperer_sequence(bddrow[7])\n",
    "    sequencebis=transformer_sequence(sequencebis,len(sequencebis))    \n",
    "    bddrow = np.array([bddrow[18],bddrow[19],bddrow[20],bddrow[21],sequence,bddrow[0],bddrow[1],bddrow[2],bddrow[8],bddrow[10],bddrow[11],bddrow[14],bddrow[16],sequencebis,bddrow[3],bddrow[4],bddrow[5],bddrow[9],bddrow[12],bddrow[13],bddrow[15],bddrow[17]], dtype=object)\n",
    "    return bddrow\n",
    "\n",
    "def transformer_database2(bddrow):\n",
    "    sequence=recuperer_sequence2(bddrow[6])\n",
    "    sequencebis=recuperer_sequence2(bddrow[7])\n",
    "    bddrow = np.array([sequence,sequencebis,bddrow[8],bddrow[9]], dtype=object)\n",
    "    return bddrow\n",
    "\n",
    "def construire_seq_et_structure_bdd(bdd):\n",
    "    for i,el in enumerate(bdd):\n",
    "        bdd[i,2] = np.array(RNA.pfl_fold_up(bdd[i,0],1,201,201))[1:,1]\n",
    "        bdd[i,3] = np.array(RNA.pfl_fold_up(bdd[i,1],1,201,201))[1:,1]\n",
    "        #bdd[i,0] = transformer_sequence(bdd[i,0],201)\n",
    "        #bdd[i,1] = transformer_sequence(bdd[i,1],201)\n",
    "    return bdd\n",
    "\n",
    "def nettoyer(bdd):\n",
    "    for i,el in enumerate(bdd):\n",
    "        bdd[i,0]=bdd[i,0].split('\\n')[1]+bdd[i,0].split('\\n')[2]+bdd[i,0].split('\\n')[3]\n",
    "        bdd[i,1]=bdd[i,1].split('\\n')[1]+bdd[i,1].split('\\n')[2]+bdd[i,1].split('\\n')[3]\n",
    "    return bdd\n",
    "\n",
    "def traduire_séquences(bdd):\n",
    "    for i,el in enumerate(bdd):\n",
    "        bdd[i,0]=traduire(bdd[i,0],bdd[i,2])\n",
    "        bdd[i,1]=traduire(bdd[i,1],bdd[i,3])\n",
    "    return bdd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
