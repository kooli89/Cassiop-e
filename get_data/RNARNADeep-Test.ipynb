{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import RNA\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99515374, 0.98200108, 0.97327828, 0.83831717, 0.71386684,\n",
       "       0.70952593, 0.79190253, 0.74114161, 0.24841253, 0.20895404,\n",
       "       0.37044396, 0.07146792, 0.09764458, 0.13250682, 0.1708176 ,\n",
       "       0.30276394, 0.90875754, 0.84469873, 0.79897224, 0.34805775,\n",
       "       0.62012246, 0.22103493, 0.05596614, 0.39760585, 0.50417549,\n",
       "       0.45270863, 0.77728394, 0.78237052, 0.35248909, 0.27293962,\n",
       "       0.40199972, 0.82078131, 0.26921011, 0.79933257, 0.63717472,\n",
       "       0.42815308, 0.4700229 , 0.01334941, 0.25455379, 0.22365579,\n",
       "       0.20206252, 0.21610497, 0.92423724, 0.77800337, 0.34187502,\n",
       "       0.1734228 , 0.17031566, 0.79341276])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqtest = 'augugacugaugcuagugaugucgucgucgaggggaucuagcugaucg'\n",
    "z = len(seqtest)\n",
    "np.array(RNA.pfl_fold_up(seqtest,1,z,z))[1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduire(seq):\n",
    "    nucleotides = ''\n",
    "    for lettre in seq:\n",
    "            if lettre=='A' or lettre=='a':\n",
    "                nucleotides=nucleotides+'a'\n",
    "            elif lettre=='C' or lettre=='c':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            elif lettre=='G' or lettre=='g':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'u'\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sequence(sequence):\n",
    "    matrice = np.zeros((36,4))\n",
    "    for i in range(len(sequence)):\n",
    "            if sequence[i]=='a':\n",
    "                matrice[i,0] = 1\n",
    "            elif sequence[i]=='u':\n",
    "                matrice[i,1] = 1\n",
    "            elif sequence[i]=='g':\n",
    "                matrice[i,2] = 1\n",
    "            elif sequence[i]=='c':\n",
    "                matrice[i,3] = 1\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_matrice3D(seq1,seq2):\n",
    "    matrice = np.zeros((36,36,1))\n",
    "    for j in range(len(seq1)):\n",
    "        for k in range(len(seq2)):\n",
    "            if (seq1[j][0]==1 and seq2[k][1]==1) or (seq1[j][1]==1 and seq2[k][0]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "            elif (seq1[j][2]==1 and seq2[k][3]==1) or (seq1[j][3]==1 and seq2[k][2]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "            elif (seq1[j][2]==1 and seq2[k][1]==1) or (seq1[j][1]==1 and seq2[k][2]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_proba(proba1,proba2):\n",
    "    product = np.zeros((36,36,1))\n",
    "    for j in range(len(proba1)):\n",
    "            for k in range(len(proba2)):\n",
    "                product[j][k] = proba1[j] * proba2[k]\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_proba(proba1,proba2):\n",
    "    somme = np.zeros((36,36,1))\n",
    "    for j in range(len(proba1)):\n",
    "            for k in range(len(proba2)):\n",
    "                somme[j][k] = proba1[j] + proba2[k]\n",
    "    return somme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datatest():\n",
    "    datatest = pd.read_csv(\"genomes/nar-03123-met-g-2015-File010.csv\", sep = \",\", header=None)\n",
    "    datatest = np.array(datatest)\n",
    "    l = len(datatest)\n",
    "    seq1 = []\n",
    "    seq2 = []\n",
    "    matrice3D = []\n",
    "    product = []\n",
    "    somme = []\n",
    "    index = 0\n",
    "    for i in range(1,l):\n",
    "        \n",
    "        begin1 = int(datatest[i,9])\n",
    "        end1 = int(datatest[i,10])\n",
    "        size1 = end1-begin1\n",
    "        if(size1 <= 36):\n",
    "    \n",
    "            begin2 = int(datatest[i,11])\n",
    "            end2 = int(datatest[i,12])\n",
    "            size2 = end2-begin2\n",
    "    \n",
    "            if(size2 <= 36):\n",
    "                proba1 = []\n",
    "                proba2 = []\n",
    "            \n",
    "                sequence = traduire(datatest[i,23][begin1:end1])\n",
    "                z = len(sequence)\n",
    "                proba1 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                seq1.append(convert_sequence(sequence))\n",
    "                \n",
    "                sequence = traduire(datatest[i,24][begin2:end2])\n",
    "                z = len(sequence)\n",
    "                proba2 = np.array(RNA.pfl_fold_up(sequence, 1, z, z))[1:,1]\n",
    "                seq2.append(convert_sequence(sequence))\n",
    "                \n",
    "                matrice3D.append(convert_matrice3D(seq1[index],seq2[index]))\n",
    "                product.append(get_product_proba(proba1,proba2))\n",
    "                somme.append(get_sum_proba(proba1,proba2))\n",
    "                index+=1\n",
    "    \n",
    "    l = len(seq1)\n",
    "    revseq1 = np.zeros((l,36,4))\n",
    "    revseq2 = np.zeros((l,36,4))\n",
    "    \n",
    "    for i in range(l):\n",
    "        for j in range(36):\n",
    "            revseq1[i][36-j-1] = seq1[i][j]\n",
    "            revseq2[i][36-j-1] = seq2[i][j]\n",
    "    \n",
    "    test = []\n",
    "    test.append(matrice3D)\n",
    "    test.append(product)\n",
    "    test.append(seq1)\n",
    "    test.append(seq2)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(seq):\n",
    "    seq = seq.split('\\n')\n",
    "    seq2 = ''\n",
    "    for j in seq:\n",
    "        seq2 = seq2 + j\n",
    "    seq2 = seq2[2:len(seq2)-1]\n",
    "    seq2 = seq2.split(' ')\n",
    "    #print(seq2)\n",
    "    seq3=[]\n",
    "    for j in seq2:\n",
    "        #print(j)\n",
    "        if j=='':\n",
    "            a=0\n",
    "        else:\n",
    "            seq3.append(float(j))\n",
    "    return seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # load your data using this function\n",
    "    neg1 = []\n",
    "\n",
    "    neg1 = pd.read_csv(\"genomes/negatifs_m-m-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg1 = np.array(neg1)\n",
    "    print(len(neg1))\n",
    "    for i in range(len(neg1)):\n",
    "        for j in range(2):\n",
    "            if len(neg1[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "    neg2 = pd.read_csv(\"genomes/negatifs_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg2 = np.array(neg2)\n",
    "    print(len(neg2))\n",
    "    for i in range(len(neg2)):\n",
    "        for j in range(2):\n",
    "            if len(neg2[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    \n",
    "    neg3 = pd.read_csv(\"genomes/negatifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg3 = np.array(neg3)\n",
    "    print(len(neg3))\n",
    "    for i in range(len(neg3)):\n",
    "        for j in range(2):\n",
    "            if len(neg3[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "\n",
    "    pos1 = pd.read_csv(\"genomes/positifs_m-m-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos1 = np.array(pos1)\n",
    "    print(len(pos1))\n",
    "    for i in range(len(pos1)):\n",
    "        for j in range(2):\n",
    "            if len(pos1[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l = len(pos1)\n",
    "    \n",
    "    pos2 = pd.read_csv(\"genomes/positifs_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos2 = np.array(pos2)\n",
    "    print(len(pos2))\n",
    "    for i in range(len(pos2)):\n",
    "        for j in range(2):\n",
    "            if len(pos2[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l=l+len(pos2)\n",
    "\n",
    "    pos3 = pd.read_csv(\"genomes/positifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos3 = np.array(pos3)\n",
    "    print(len(pos3))\n",
    "    for i in range(len(pos3)):\n",
    "        for j in range(2):\n",
    "            if len(pos3[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l=l+len(pos3)\n",
    "\n",
    "    bdd = np.concatenate((pos1,pos2,pos3,neg1,neg2,neg3))\n",
    "\n",
    "    pos1 = []\n",
    "    neg1 = []\n",
    "    pos2 = []\n",
    "    neg2 = []\n",
    "    pos3 = []\n",
    "    neg3 = []\n",
    "\n",
    "    labels = np.zeros((len(bdd),1))\n",
    "\n",
    "    bdd = np.concatenate((bdd,labels),axis=1)\n",
    "    for i in range(l):\n",
    "        bdd[i,4]=1\n",
    "    labels=[]\n",
    "    \n",
    "    # shuffle total\n",
    "\n",
    "    indices = np.arange(len(bdd))\n",
    "    shuffle(indices)\n",
    "    bdd = bdd[indices]\n",
    "    indices=[]\n",
    "\n",
    "    l = len(bdd)\n",
    "    matrice = np.zeros((l,36,36,1))\n",
    "    matrice1 = np.zeros((l,36,36,1))\n",
    "    matrice2 = np.zeros((l,36,4))\n",
    "    matrice3 = np.zeros((l,36,4))\n",
    "\n",
    "    for i in range(l):\n",
    "        seq1 = bdd[i,0]\n",
    "        seq2 = bdd[i,1]\n",
    "        prob1 = np.array(clean(bdd[i,2]))\n",
    "        prob2 = np.array(clean(bdd[i,3]))\n",
    "        for j in range(len(seq1)):\n",
    "            for k in range(len(seq2)):\n",
    "                if (seq1[j]=='a' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='a'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                elif (seq1[j]=='g' and seq2[k]=='c') or (seq1[j]=='c' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                elif (seq1[j]=='g' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                matrice1[i,j,k,0] = prob1[j]+prob2[k]\n",
    "        for j in range(len(seq1)):\n",
    "            if seq1[j]=='a':\n",
    "                matrice2[i,j,0] = 1\n",
    "            elif seq1[j]=='u':\n",
    "                matrice2[i,j,1] = 1\n",
    "            elif seq1[j]=='g':\n",
    "                matrice2[i,j,2] = 1\n",
    "            elif seq1[j]=='c':\n",
    "                matrice2[i,j,3] = 1\n",
    "        for j in range(len(seq2)):\n",
    "            if seq2[j]=='a':\n",
    "                matrice3[i,j,0] = 1\n",
    "            elif seq2[j]=='u':\n",
    "                matrice3[i,j,1] = 1\n",
    "            elif seq2[j]=='g':\n",
    "                matrice3[i,j,2] = 1\n",
    "            elif seq2[j]=='c':\n",
    "                matrice3[i,j,3] = 1  \n",
    "    return matrice, matrice1, matrice2, matrice3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(nb_train = 165000,nb_val = 6000,nb_test = 5000):\n",
    "    data = load_dataset()\n",
    "    matrice = data[0]\n",
    "    matrice1 = data[1]\n",
    "    matrice2 = data[2]\n",
    "    matrice3 = data[3]\n",
    "    \n",
    "    training = []\n",
    "    training.append(matrice[:nb_train])\n",
    "    training.append(matrice1[:nb_train])\n",
    "    training.append(matrice2[:nb_train])\n",
    "    training.append(matrice3[:nb_train])\n",
    "\n",
    "    validation = []\n",
    "    validation.append(matrice[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice1[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice2[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice3[nb_train:nb_train+nb_val])\n",
    "\n",
    "    testing = []\n",
    "    testing.append(matrice[-nb_test:])\n",
    "    testing.append(matrice1[-nb_test:])\n",
    "    testing.append(matrice2[-nb_test:])\n",
    "    testing.append(matrice3[-nb_test:])\n",
    "\n",
    "    labels = bdd[:,4]\n",
    "    bdd = []\n",
    "    y = labels[:nb_train]\n",
    "    y = keras.utils.np_utils.to_categorical(y,2)\n",
    "    val_y = labels[nb_train:nb_train+nb_val]\n",
    "    val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "    true_y = labels[-nb_test:]\n",
    "    argtest=[]\n",
    "    np.sum(y[:,1])\n",
    "    return training, y, validation, val_y, testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(test_num, pred_y,  labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+ fp)\n",
    "    sensitivity = float(tp)/ (tp+fn)\n",
    "    specificity = float(tn)/(tn + fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network(matrixsize = 6, nbfilter = 24, matrixsize2 = 7, nbfilter2 = 6):    \n",
    "    k = matrixsize\n",
    "    # init_weights\n",
    "    I = np.eye(k)\n",
    "    M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "    I2 = np.zeros((k,k))\n",
    "    M2 = np.zeros((k,k))\n",
    "    for j in range(k):\n",
    "        I2[:,j] = I[:,k-j-1]\n",
    "        M2[:,j] = M[:,k-j-1]        \n",
    "    W = np.zeros((k,k,1,nbfilter))\n",
    "    W[:,:,0,0] = I\n",
    "    W[:,:,0,1] = I2\n",
    "    W[:,:,0,2] = M\n",
    "    W[:,:,0,3] = M2\n",
    "    for j in range(4,nbfilter):\n",
    "        W[:,:,0,j] = np.random.randn(k,k)*0.2\n",
    "    print('configure cnn network')\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = nbfilter, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter)]))    \n",
    "    model.add(AveragePooling2D(pool_size=(3,3)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    k2 = matrixsize2\n",
    "    I = np.eye(k2)\n",
    "    M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "    I2=np.zeros((k2,k2))\n",
    "    M2=np.zeros((k2,k2))\n",
    "    for j in range(k2):\n",
    "        I2[:,j] = I[:,k2-j-1]\n",
    "        M2[:,j] = M[:,k2-j-1]   \n",
    "    \n",
    "    Z = np.zeros((k2,k2,nbfilter,nbfilter2))\n",
    "    \n",
    "    for u in range(nbfilter):\n",
    "        Z[:,:,u,0] = I\n",
    "        Z[:,:,u,1] = I2\n",
    "        Z[:,:,u,2] = M\n",
    "        Z[:,:,u,3] = M2\n",
    "        for p in range(4,nbfilter2):\n",
    "            Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            \n",
    "\n",
    "    model.add(Conv2D(filters = nbfilter2, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter2)]))\n",
    "    print(model.output_shape)    \n",
    "    model.add(Flatten())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network_seq(nbfilter = 64, kernelsize = 7):        \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = nbfilter, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(MaxPooling1D(pool_size=6))\n",
    "    print(model.output_shape)\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(matrixsize11 = 6, nbfilter11 = 24, matrixsize12 = 7, nbfilter12 = 6, \n",
    "                 matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7, nbfilter22 = 6,\n",
    "                 nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                 Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    training_net=[]\n",
    "    training_net1 = []\n",
    "    training_net2 = []\n",
    "    training_net1.append(get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12))\n",
    "    training_net1.append(get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22))\n",
    "    training_net2.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net2.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    \n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(Merge(training_net2, mode ='concat'))\n",
    "    model2.add(Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Activation('relu'))\n",
    "    #model2.add(Dropout(0.1))\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Merge(training_net1, mode ='concat'))\n",
    "    model1.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model1.add(Dropout(0.1))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Activation('relu'))\n",
    "    \n",
    "    training_net.append(model1)\n",
    "    training_net.append(model2)\n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    "    \n",
    "    model.add(Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy','roc_curve','auc'])\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(matrixsize11 = 6, nbfilter11 = 24, matrixsize12 = 7, nbfilter12 = 6, \n",
    "                 matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7, nbfilter22 = 6,\n",
    "                 nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                 Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    training_net=[]\n",
    "    training_net1 = []\n",
    "    training_net2 = []\n",
    "    training_net21 = []\n",
    "    training_net22 = []\n",
    "    training_net1.append(get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12))\n",
    "    training_net1.append(get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22))\n",
    "    training_net21.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net21.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net22.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    training_net22.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    \n",
    "    \n",
    "    model21 = Sequential()\n",
    "    model21.add(Merge(training_net21, mode ='concat'))\n",
    "    model21.add(Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model21.add(BatchNormalization())\n",
    "    model21.add(Activation('relu'))\n",
    "    model21.add(Dropout(0.2))\n",
    "    model22 = Sequential()\n",
    "    model22.add(Merge(training_net22, mode ='concat'))\n",
    "    model22.add(Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model22.add(BatchNormalization())\n",
    "    model22.add(Activation('relu'))\n",
    "    model22.add(Dropout(0.2))\n",
    "    training_net2.append(model21)\n",
    "    training_net2.append(model22)\n",
    "    model2 = Sequential()\n",
    "    model2.add(Merge(training_net2, mode ='concat'))\n",
    "    model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Activation('relu'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    \n",
    "    #model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model2.add(BatchNormalization())\n",
    "    #model2.add(Activation('relu'))\n",
    "    #model2.add(Dropout(0.2))\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Merge(training_net1, mode ='concat'))\n",
    "    model1.add(Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model1.add(Dropout(0.1))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Activation('relu'))\n",
    "    \n",
    "    training_net.append(model1)\n",
    "    training_net.append(model2)\n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    "    \n",
    "    model.add(Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    if Dense5>0:\n",
    "        model.add(Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data = load_data(165000,6000,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training = [], y = [], batch_size=16, epochs=6, verbose1 = 1, verbose2 = 1,\n",
    "                              validation = [], val_y = [], testing = [], matrixsize11 = 6, nbfilter11 = 24, \n",
    "                              matrixsize12 = 7, nbfilter12 = 6, matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7,\n",
    "                              nbfilter22 = 6, nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                              Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    model = create_model(matrixsize11, nbfilter11, matrixsize12, nbfilter12, matrixsize21, nbfilter21, \n",
    "                         matrixsize22, nbfilter22, nbfilter1, kernel_size1, nbfilters2,kernel_size2,\n",
    "                         Dense1, Dense2, Dense3, Dense4, Dense5)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose = verbose1)\n",
    "    print('model training')\n",
    "    model.fit(training, y, batch_size, epochs, verbose = verbose2, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "    \n",
    "    # test   \n",
    "    \n",
    "    print('predicting')\n",
    "    \n",
    "        \n",
    "    predictions = model.predict_proba(testing)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"batch_size : \", batch_size)\n",
    "    print(\"epochs : \",epochs)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(true_y = [],training = [], y = [], batch_size=4, epochs=12, verbose1 = 1, verbose2 = 1,\n",
    "                              validation = [], val_y = [], testing = [], matrixsize11 = 6, nbfilter11 = 24, \n",
    "                              matrixsize12 = 7, nbfilter12 = 4, matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7,\n",
    "                              nbfilter22 = 4, nbfilter1 = 96, kernel_size1 = 7, nbfilters2 = 96,kernel_size2 = 7,\n",
    "                              Dense1 = 32, Dense2 = 192, Dense3 = 512, Dense4 = 64, Dense5 = 0):\n",
    "    \n",
    "    model = create_model2(matrixsize11, nbfilter11, matrixsize12, nbfilter12, matrixsize21, nbfilter21, \n",
    "                         matrixsize22, nbfilter22, nbfilter1, kernel_size1, nbfilters2,kernel_size2,\n",
    "                         Dense1, Dense2, Dense3, Dense4, Dense5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename = \"model\"):\n",
    "    # serialize model to JSON\n",
    "    print(\"Model is saved in : \", filename + \".json\")\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    print(\"Weights are saved in : \", filename + \".h5\")\n",
    "    model.save_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename = \"model\"):\n",
    "    # load json and create model\n",
    "    print(\"Model loaded from : \", filename + \".json\")\n",
    "    json_file = open(filename + \".json\", 'rb')\n",
    "    loaded_model_json = json_file.read()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    json_file.close()\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(filename + \".h5\")\n",
    "    print(\"Weight of the model loaded from : \", filename + \".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf(seq1, seq2):\n",
    "    z = len(seq1)\n",
    "    proba1 = np.array(RNA.pfl_fold_up(seq1,1,z,z))[1:,1]\n",
    "    sequ1 = convert_sequence(seq1)\n",
    "    \n",
    "    z = len(seq2)\n",
    "    proba2 = np.array(RNA.pfl_fold_up(seq1,1,z,z))[1:,1]\n",
    "    sequ2 = convert_sequence(seq2)\n",
    "    \n",
    "    matrice3D = convert_matrice3D(sequ1,sequ2)\n",
    "    product = get_product_proba(proba1,proba2)\n",
    "       \n",
    "    test = []\n",
    "    test.append(matrice3D)\n",
    "    test.append(product)\n",
    "    test.append(seq1)\n",
    "    test.append(seq2)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(seq1,seq2,window = 36, filename = 'model'):\n",
    "    model = load_model(filename)\n",
    "    l1 = len(seq1)\n",
    "    l2 = len(seq2)\n",
    "    output = np.zeros((max(1,l1-window),max(1,l2-window)))\n",
    "    for i in range(max(1,l1-window)):\n",
    "        for j in range(max(1,l2-window)):\n",
    "            test = transf(seq1[i:i+35],seq2[j:j+35])\n",
    "            output[i,j] = (model.predict_proba(test)[:,1])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_benchmark(filename = 'model'):\n",
    "    model = test_model([],[],[],32,16,1,1,[],[],[],Dense1=128,Dense2=512,Dense3=512,Dense4=128,Dense5=64)\n",
    "    model.load_weights(filename)\n",
    "    test = load_datatest()\n",
    "    model.predict_proba(test)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"batch_size : \", batch_size)\n",
    "    print(\"epochs : \",epochs)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configure cnn network\n",
      "(None, 10, 10, 24)\n",
      "(None, 4, 4, 4)\n",
      "configure cnn network\n",
      "(None, 10, 10, 24)\n",
      "(None, 4, 4, 4)\n",
      "(None, 5, 96)\n",
      "(None, 5, 96)\n",
      "(None, 5, 96)\n",
      "(None, 5, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-ca1f50acab75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_on_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_double77-6.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-288-79aee0c646ea>\u001b[0m in \u001b[0;36mtest_on_benchmark\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_datatest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnulll\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \"\"\"\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             warnings.warn('Network returning invalid probability values. '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1823\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1825\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "test_on_benchmark(\"model_double77-6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_model(Data[0],Data[1],16,6,1,1,Data[2],Data[3],Data[4],6,24,7,6,6,24,7,6,64,7,64,7,512,128,512,128,64)\n",
    "#save_model(model, \"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
