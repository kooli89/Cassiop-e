{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import RNA\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "#from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99515374, 0.98200108, 0.97327828, 0.83831717, 0.71386684,\n",
       "       0.70952593, 0.79190253, 0.74114161, 0.24841253, 0.20895404,\n",
       "       0.37044396, 0.07146792, 0.09764458, 0.13250682, 0.1708176 ,\n",
       "       0.30276394, 0.90875754, 0.84469873, 0.79897224, 0.34805775,\n",
       "       0.62012246, 0.22103493, 0.05596614, 0.39760585, 0.50417549,\n",
       "       0.45270863, 0.77728394, 0.78237052, 0.35248909, 0.27293962,\n",
       "       0.40199972, 0.82078131, 0.26921011, 0.79933257, 0.63717472,\n",
       "       0.42815308, 0.4700229 , 0.01334941, 0.25455379, 0.22365579,\n",
       "       0.20206252, 0.21610497, 0.92423724, 0.77800337, 0.34187502,\n",
       "       0.1734228 , 0.17031566, 0.79341276])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqtest = 'augugacugaugcuagugaugucgucgucgaggggaucuagcugaucg'\n",
    "z = len(seqtest)\n",
    "np.array(RNA.pfl_fold_up(seqtest,1,z,z))[1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduire(seq):\n",
    "    nucleotides = ''\n",
    "    for lettre in seq:\n",
    "            if lettre=='A' or lettre=='a':\n",
    "                nucleotides=nucleotides+'a'\n",
    "            elif lettre=='C' or lettre=='c':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            elif lettre=='G' or lettre=='g':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'u'\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sequence(sequence):\n",
    "    matrice = np.zeros((36,4))\n",
    "    for i in range(len(sequence)):\n",
    "            if sequence[i]=='a':\n",
    "                matrice[i,0] = 1\n",
    "            elif sequence[i]=='u':\n",
    "                matrice[i,1] = 1\n",
    "            elif sequence[i]=='g':\n",
    "                matrice[i,2] = 1\n",
    "            elif sequence[i]=='c':\n",
    "                matrice[i,3] = 1\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_matrice3D(seq1,seq2):\n",
    "    matrice = np.zeros((36,36,1))\n",
    "    for j in range(len(seq1)):\n",
    "        for k in range(len(seq2)):\n",
    "            if (seq1[j][0]==1 and seq2[k][1]==1) or (seq1[j][1]==1 and seq2[k][0]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "            elif (seq1[j][2]==1 and seq2[k][3]==1) or (seq1[j][3]==1 and seq2[k][2]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "            elif (seq1[j][2]==1 and seq2[k][1]==1) or (seq1[j][1]==1 and seq2[k][2]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_proba(proba1,proba2):\n",
    "    product = np.zeros((36,36,1))\n",
    "    for j in range(len(proba1)):\n",
    "            for k in range(len(proba2)):\n",
    "                product[j][k] = proba1[j] * proba2[k]\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_proba(proba1,proba2):\n",
    "    somme = np.zeros((36,36,1))\n",
    "    for j in range(len(proba1)):\n",
    "            for k in range(len(proba2)):\n",
    "                somme[j][k] = proba1[j] + proba2[k]\n",
    "    return somme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datatest():\n",
    "    datatest = pd.read_csv(\"genomes/nar-03123-met-g-2015-File010.csv\", sep = \",\", header=None)\n",
    "    datatest = np.array(datatest)\n",
    "    l = len(datatest)\n",
    "    seq1 = np.zeros((200,36,4))\n",
    "    seq2 = np.zeros((200,36,4))\n",
    "    matrice3D = np.zeros((200,36,36,1))\n",
    "    product = np.zeros((200,36,36,1))\n",
    "    index = 0\n",
    "    for i in range(1,l):\n",
    "        print(\"i : \",i)\n",
    "        begin1 = int(datatest[i,9])\n",
    "        end1 = int(datatest[i,10])\n",
    "        size1 = end1-begin1\n",
    "        if(size1 <= 36):\n",
    "    \n",
    "            begin2 = int(datatest[i,11])\n",
    "            end2 = int(datatest[i,12])\n",
    "            size2 = end2-begin2\n",
    "    \n",
    "            if(size2 <= 36):\n",
    "                proba1 = []\n",
    "                proba2 = []\n",
    "                print(\"taille 1 : \", size1)\n",
    "                print(\"taille 2 : \", size2)\n",
    "                print(\"index : \",index)\n",
    "                sequence = traduire(datatest[i,23][begin1:end1])\n",
    "                z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", z)\n",
    "                proba1 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                seq1[index] = convert_sequence(sequence)\n",
    "                \n",
    "                sequence = traduire(datatest[i,24][begin2:end2])\n",
    "                z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", z)\n",
    "                proba2 = np.array(RNA.pfl_fold_up(sequence, 1, z, z))[1:,1]\n",
    "                seq2[index] = convert_sequence(sequence)\n",
    "                \n",
    "                matrice3D[index] = convert_matrice3D(seq1[index],seq2[index])\n",
    "                product[index] = get_product_proba(proba1,proba2)\n",
    "                \n",
    "                index+=1\n",
    "                print(\"index : \",index)\n",
    "                longueur1 = int(datatest[i,7])\n",
    "                if((longueur1 - end1) < 36):\n",
    "                    sequence = traduire(datatest[i,23][begin1-37:begin1-1])                \n",
    "                else :\n",
    "                    sequence = traduire(datatest[i,23][end1+1:end1+37])\n",
    "                \n",
    "                z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", z)\n",
    "                proba1 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                seq1[index] = convert_sequence(sequence)\n",
    "            \n",
    "                sequence = traduire(datatest[i,24][end2+1:end2+37])\n",
    "                z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", z)\n",
    "                proba2 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                seq2[index] = convert_sequence(sequence)\n",
    "                \n",
    "                matrice3D[index] = convert_matrice3D(seq1[index],seq2[index])\n",
    "                product[index] = get_product_proba(proba1,proba2)\n",
    "                \n",
    "                index+=1\n",
    "    \n",
    "    l = len(seq1)\n",
    "    revseq1 = np.zeros((l,36,4))\n",
    "    revseq2 = np.zeros((l,36,4))\n",
    "    \n",
    "    for i in range(l):\n",
    "        for j in range(36):\n",
    "            revseq1[i][36-j-1] = seq1[i][j]\n",
    "            revseq2[i][36-j-1] = seq2[i][j]\n",
    "    \n",
    "    test = []\n",
    "    test.append(matrice3D)\n",
    "    test.append(product)\n",
    "    test.append(seq1)\n",
    "    test.append(revseq1)\n",
    "    test.append(seq2)\n",
    "    test.append(revseq2)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(seq):\n",
    "    seq = seq.split('\\n')\n",
    "    seq2 = ''\n",
    "    for j in seq:\n",
    "        seq2 = seq2 + j\n",
    "    seq2 = seq2[2:len(seq2)-1]\n",
    "    seq2 = seq2.split(' ')\n",
    "    #print(seq2)\n",
    "    seq3=[]\n",
    "    for j in seq2:\n",
    "        #print(j)\n",
    "        if j=='':\n",
    "            a=0\n",
    "        else:\n",
    "            seq3.append(float(j))\n",
    "    return seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # load your data using this function\n",
    "    neg1 = []\n",
    "\n",
    "    neg1 = pd.read_csv(\"genomes/negatifs_m-m-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg1 = np.array(neg1)\n",
    "    print(len(neg1))\n",
    "    for i in range(len(neg1)):\n",
    "        for j in range(2):\n",
    "            if len(neg1[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "    neg2 = pd.read_csv(\"genomes/negatifs_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg2 = np.array(neg2)\n",
    "    print(len(neg2))\n",
    "    for i in range(len(neg2)):\n",
    "        for j in range(2):\n",
    "            if len(neg2[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    \n",
    "    neg3 = pd.read_csv(\"genomes/negatifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg3 = np.array(neg3)\n",
    "    print(len(neg3))\n",
    "    for i in range(len(neg3)):\n",
    "        for j in range(2):\n",
    "            if len(neg3[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "\n",
    "    pos1 = pd.read_csv(\"genomes/positifs_m-m-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos1 = np.array(pos1)\n",
    "    print(len(pos1))\n",
    "    for i in range(len(pos1)):\n",
    "        for j in range(2):\n",
    "            if len(pos1[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l = len(pos1)\n",
    "    \n",
    "    pos2 = pd.read_csv(\"genomes/positifs_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos2 = np.array(pos2)\n",
    "    print(len(pos2))\n",
    "    for i in range(len(pos2)):\n",
    "        for j in range(2):\n",
    "            if len(pos2[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l=l+len(pos2)\n",
    "\n",
    "    pos3 = pd.read_csv(\"genomes/positifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos3 = np.array(pos3)\n",
    "    print(len(pos3))\n",
    "    for i in range(len(pos3)):\n",
    "        for j in range(2):\n",
    "            if len(pos3[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l=l+len(pos3)\n",
    "\n",
    "    bdd = np.concatenate((pos1,pos2,pos3,neg1,neg2,neg3))\n",
    "\n",
    "    pos1 = []\n",
    "    neg1 = []\n",
    "    pos2 = []\n",
    "    neg2 = []\n",
    "    pos3 = []\n",
    "    neg3 = []\n",
    "\n",
    "    labels = np.zeros((len(bdd),1))\n",
    "\n",
    "    bdd = np.concatenate((bdd,labels),axis=1)\n",
    "    for i in range(l):\n",
    "        bdd[i,4]=1\n",
    "    labels=[]\n",
    "    \n",
    "    # shuffle total\n",
    "\n",
    "    indices = np.arange(len(bdd))\n",
    "    shuffle(indices)\n",
    "    bdd = bdd[indices]\n",
    "    indices=[]\n",
    "\n",
    "    l = len(bdd)\n",
    "    matrice = np.zeros((l,36,36,1))\n",
    "    matrice1 = np.zeros((l,36,36,1))\n",
    "    matrice2 = np.zeros((l,36,4))\n",
    "    matrice3 = np.zeros((l,36,4))\n",
    "\n",
    "    for i in range(l):\n",
    "        seq1 = bdd[i,0]\n",
    "        seq2 = bdd[i,1]\n",
    "        prob1 = np.array(clean(bdd[i,2]))\n",
    "        prob2 = np.array(clean(bdd[i,3]))\n",
    "        for j in range(len(seq1)):\n",
    "            for k in range(len(seq2)):\n",
    "                if (seq1[j]=='a' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='a'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                elif (seq1[j]=='g' and seq2[k]=='c') or (seq1[j]=='c' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                elif (seq1[j]=='g' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                matrice1[i,j,k,0] = prob1[j]+prob2[k]\n",
    "        for j in range(len(seq1)):\n",
    "            if seq1[j]=='a':\n",
    "                matrice2[i,j,0] = 1\n",
    "            elif seq1[j]=='u':\n",
    "                matrice2[i,j,1] = 1\n",
    "            elif seq1[j]=='g':\n",
    "                matrice2[i,j,2] = 1\n",
    "            elif seq1[j]=='c':\n",
    "                matrice2[i,j,3] = 1\n",
    "        for j in range(len(seq2)):\n",
    "            if seq2[j]=='a':\n",
    "                matrice3[i,j,0] = 1\n",
    "            elif seq2[j]=='u':\n",
    "                matrice3[i,j,1] = 1\n",
    "            elif seq2[j]=='g':\n",
    "                matrice3[i,j,2] = 1\n",
    "            elif seq2[j]=='c':\n",
    "                matrice3[i,j,3] = 1\n",
    "                \n",
    "    labels = bdd[:,4]\n",
    "    bdd = []\n",
    "    \n",
    "    return matrice, matrice1, matrice2, matrice3, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(nb_train = 165000,nb_val = 6000,nb_test = 5000):\n",
    "    data = load_dataset()\n",
    "    matrice = data[0]\n",
    "    matrice1 = data[1]\n",
    "    matrice2 = data[2]\n",
    "    matrice3 = data[3]\n",
    "    labels = data[4]\n",
    "    \n",
    "    training = []\n",
    "    training.append(matrice[:nb_train])\n",
    "    training.append(matrice1[:nb_train])\n",
    "    training.append(matrice2[:nb_train])\n",
    "    training.append(matrice3[:nb_train])\n",
    "\n",
    "    validation = []\n",
    "    validation.append(matrice[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice1[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice2[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice3[nb_train:nb_train+nb_val])\n",
    "\n",
    "    testing = []\n",
    "    testing.append(matrice[-nb_test:])\n",
    "    testing.append(matrice1[-nb_test:])\n",
    "    testing.append(matrice2[-nb_test:])\n",
    "    testing.append(matrice3[-nb_test:])\n",
    "    \n",
    "    y = labels[:nb_train]\n",
    "    y = keras.utils.np_utils.to_categorical(y,2)\n",
    "    val_y = labels[nb_train:nb_train+nb_val]\n",
    "    val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "    true_y = labels[-nb_test:]\n",
    "    argtest=[]\n",
    "    np.sum(y[:,1])\n",
    "    return training, y, validation, val_y, testing, true_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(test_num, pred_y,  labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    print(tp+tn)\n",
    "    if (tp+fp) == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = float(tp)/(tp+ fp)\n",
    "    if (tp+fn) == 0:\n",
    "        sensitivity = 0\n",
    "    else:\n",
    "        sensitivity = float(tp)/ (tp+fn)\n",
    "    if (tn+fp) == 0:\n",
    "        specificity = 0  \n",
    "    else:\n",
    "        specificity = float(tn)/(tn + fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfs_test(model,test,true):\n",
    "    print('predicting')\n",
    "            \n",
    "    predictions = model.predict(test)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true)\n",
    "    \n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network(matrixsize = 6, nbfilter = 24, matrixsize2 = 7, nbfilter2 = 6):    \n",
    "    k = matrixsize\n",
    "    # init_weights\n",
    "    I = np.eye(k)\n",
    "    M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "    I2 = np.zeros((k,k))\n",
    "    M2 = np.zeros((k,k))\n",
    "    for j in range(k):\n",
    "        I2[:,j] = I[:,k-j-1]\n",
    "        M2[:,j] = M[:,k-j-1]        \n",
    "    W = np.zeros((k,k,1,nbfilter))\n",
    "    W[:,:,0,0] = I\n",
    "    W[:,:,0,1] = I2\n",
    "    W[:,:,0,2] = M\n",
    "    W[:,:,0,3] = M2\n",
    "    for j in range(4,nbfilter):\n",
    "        W[:,:,0,j] = np.random.randn(k,k)*0.2\n",
    "    print('configure cnn network')\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = nbfilter, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter)]))    \n",
    "    model.add(AveragePooling2D(pool_size=(3,3)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    k2 = matrixsize2\n",
    "    I = np.eye(k2)\n",
    "    M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "    I2=np.zeros((k2,k2))\n",
    "    M2=np.zeros((k2,k2))\n",
    "    for j in range(k2):\n",
    "        I2[:,j] = I[:,k2-j-1]\n",
    "        M2[:,j] = M[:,k2-j-1]   \n",
    "    \n",
    "    Z = np.zeros((k2,k2,nbfilter,nbfilter2))\n",
    "    \n",
    "    for u in range(nbfilter):\n",
    "        Z[:,:,u,0] = I\n",
    "        Z[:,:,u,1] = I2\n",
    "        Z[:,:,u,2] = M\n",
    "        Z[:,:,u,3] = M2\n",
    "        for p in range(4,nbfilter2):\n",
    "            Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            \n",
    "\n",
    "    model.add(Conv2D(filters = nbfilter2, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter2)]))\n",
    "    print(model.output_shape)    \n",
    "    model.add(Flatten())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network_seq(nbfilter = 64, kernelsize = 7):        \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = nbfilter, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(MaxPooling1D(pool_size=6))\n",
    "    print(model.output_shape)\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(matrixsize11 = 6, nbfilter11 = 24, matrixsize12 = 7, nbfilter12 = 6, \n",
    "                 matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7, nbfilter22 = 6,\n",
    "                 nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                 Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    training_net=[]\n",
    "    training_net1 = []\n",
    "    training_net2 = []\n",
    "    training_net1.append(get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12))\n",
    "    training_net1.append(get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22))\n",
    "    training_net2.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net2.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    \n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(Merge(training_net2, mode ='concat'))\n",
    "    model2.add(Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Activation('relu'))\n",
    "    #model2.add(Dropout(0.1))\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Merge(training_net1, mode ='concat'))\n",
    "    model1.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model1.add(Dropout(0.1))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Activation('relu'))\n",
    "    \n",
    "    training_net.append(model1)\n",
    "    training_net.append(model2)\n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    "    \n",
    "    model.add(Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy','roc_curve','auc'])\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(matrixsize11 = 6, nbfilter11 = 24, matrixsize12 = 7, nbfilter12 = 6, \n",
    "                 matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7, nbfilter22 = 6,\n",
    "                 nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                 Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    training_net=[]\n",
    "    training_net1 = []\n",
    "    training_net2 = []\n",
    "    training_net21 = []\n",
    "    training_net22 = []\n",
    "    training_net1.append(get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12))\n",
    "    training_net1.append(get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22))\n",
    "    training_net21.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net21.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net22.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    training_net22.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    \n",
    "    \n",
    "    model21 = Sequential()\n",
    "    model21.add(Merge(training_net21, mode ='concat'))\n",
    "    model21.add(Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model21.add(BatchNormalization())\n",
    "    model21.add(Activation('relu'))\n",
    "    model21.add(Dropout(0.2))\n",
    "    model22 = Sequential()\n",
    "    model22.add(Merge(training_net22, mode ='concat'))\n",
    "    model22.add(Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model22.add(BatchNormalization())\n",
    "    model22.add(Activation('relu'))\n",
    "    model22.add(Dropout(0.2))\n",
    "    training_net2.append(model21)\n",
    "    training_net2.append(model22)\n",
    "    model2 = Sequential()\n",
    "    model2.add(Merge(training_net2, mode ='concat'))\n",
    "    model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Activation('relu'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    \n",
    "    #model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model2.add(BatchNormalization())\n",
    "    #model2.add(Activation('relu'))\n",
    "    #model2.add(Dropout(0.2))\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Merge(training_net1, mode ='concat'))\n",
    "    model1.add(Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model1.add(Dropout(0.1))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Activation('relu'))\n",
    "    \n",
    "    training_net.append(model1)\n",
    "    training_net.append(model2)\n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    "    \n",
    "    model.add(Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    if Dense5>0:\n",
    "        model.add(Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data = load_data(165000,6000,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training = [], y = [], batch_size=16, epochs=6, verbose1 = 1, verbose2 = 1,\n",
    "                              validation = [], val_y = [], testing = [], matrixsize11 = 6, nbfilter11 = 24, \n",
    "                              matrixsize12 = 7, nbfilter12 = 6, matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7,\n",
    "                              nbfilter22 = 6, nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                              Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    model = create_model(matrixsize11, nbfilter11, matrixsize12, nbfilter12, matrixsize21, nbfilter21, \n",
    "                         matrixsize22, nbfilter22, nbfilter1, kernel_size1, nbfilters2,kernel_size2,\n",
    "                         Dense1, Dense2, Dense3, Dense4, Dense5)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose = verbose1)\n",
    "    print('model training')\n",
    "    model.fit(training, y, batch_size, epochs, verbose = verbose2, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "    \n",
    "    # test   \n",
    "    \n",
    "    print('predicting')\n",
    "    \n",
    "        \n",
    "    predictions = model.predict_proba(testing)[:,1]\n",
    "    print(predictions)\n",
    "    for i,null in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    true_y = np.ones((len(predictions)))\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"batch_size : \", batch_size)\n",
    "    print(\"epochs : \",epochs)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres \n",
    "batch_size=64\n",
    "epochs=25\n",
    "verbose1 = 1\n",
    "verbose2 = 1\n",
    "matrixsize11 = 6\n",
    "nbfilter11 = 24\n",
    "matrixsize12 = 7\n",
    "nbfilter12 = 4\n",
    "matrixsize21 = 6\n",
    "nbfilter21 = 24\n",
    "matrixsize22 = 7\n",
    "nbfilter22 = 4\n",
    "nbfilter1 = 96\n",
    "kernelsize = 7\n",
    "nbfilters2 = 96\n",
    "kernel_size2 = 7\n",
    "Dense1 = 128\n",
    "Dense2 = 512\n",
    "Dense3 = 512\n",
    "Dense4 = 128\n",
    "Dense5 = 64\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras import Model\n",
    "k = matrixsize11\n",
    "# init_weights\n",
    "I = np.eye(k)\n",
    "M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "I2 = np.zeros((k,k))\n",
    "M2 = np.zeros((k,k))\n",
    "for j in range(k):\n",
    "    I2[:,j] = I[:,k-j-1]\n",
    "    M2[:,j] = M[:,k-j-1]        \n",
    "W = np.zeros((k,k,1,nbfilter11))\n",
    "W[:,:,0,0] = I\n",
    "W[:,:,0,1] = I2\n",
    "W[:,:,0,2] = M\n",
    "W[:,:,0,3] = M2\n",
    "for j in range(4,nbfilter11):\n",
    "    W[:,:,0,j] = np.random.randn(k,k)*0.2\n",
    "\n",
    "\n",
    "k2 = matrixsize12\n",
    "I = np.eye(k2)\n",
    "M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "I2=np.zeros((k2,k2))\n",
    "M2=np.zeros((k2,k2))\n",
    "for j in range(k2):\n",
    "    I2[:,j] = I[:,k2-j-1]\n",
    "    M2[:,j] = M[:,k2-j-1]   \n",
    "\n",
    "Z = np.zeros((k2,k2,nbfilter11,nbfilter12))\n",
    "\n",
    "for u in range(nbfilter12):\n",
    "    Z[:,:,u,0] = I\n",
    "    Z[:,:,u,1] = I2\n",
    "    Z[:,:,u,2] = M\n",
    "    Z[:,:,u,3] = M2\n",
    "    for p in range(4,nbfilter12):\n",
    "        Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle similaire à celui enregistré\n",
    "c2d1_input = keras.Input(shape=(36,36,1))\n",
    "cnn2d1 = Conv2D(filters = nbfilter11, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter11)])(c2d1_input)\n",
    "cnn2d1 = AveragePooling2D(pool_size=(3,3))(cnn2d1)\n",
    "cnn2d1 = Conv2D(filters = nbfilter12, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter12)])(cnn2d1)\n",
    "cnn2d1 = Dropout(0.1)(cnn2d1)\n",
    "cnn2d1 = Flatten()(cnn2d1)\n",
    "#cnn2d1 = get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12)(c2d1_input)\n",
    "#training_net1.append(cnn2d1)\n",
    "c2d2_input = keras.Input(shape=(36,36,1))\n",
    "cnn2d2 = Conv2D(filters = nbfilter11, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter11)])(c2d1_input)\n",
    "cnn2d2 = AveragePooling2D(pool_size=(3,3))(cnn2d2)\n",
    "cnn2d2 = Conv2D(filters = nbfilter12, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter12)])(cnn2d2)\n",
    "cnn2d2 = Dropout(0.1)(cnn2d2)\n",
    "cnn2d2 = Flatten()(cnn2d2)\n",
    "#cnn2d2 = get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22)(c2d2_input)\n",
    "#training_net1.append(cnn2d2)\n",
    "c1d1_input = keras.Input(shape=(36,4))\n",
    "cnn1d1 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d1_input)\n",
    "cnn1d1 = MaxPooling1D(pool_size=8)(cnn1d1)\n",
    "cnn1d1 = Dropout(0.2)(cnn1d1)\n",
    "cnn1d1 = Flatten()(cnn1d1)\n",
    "#cnn1d1 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d1_input)\n",
    "c1d2_input = keras.Input(shape=(36,4))\n",
    "cnn1d2 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d2_input)\n",
    "cnn1d2 = MaxPooling1D(pool_size=8)(cnn1d2)\n",
    "cnn1d2 = Dropout(0.2)(cnn1d2)\n",
    "cnn1d2 = Flatten()(cnn1d2)\n",
    "#cnn1d2 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d2_input)\n",
    "c1d3_input = keras.Input(shape=(36,4))\n",
    "cnn1d3 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d3_input)\n",
    "cnn1d3 = MaxPooling1D(pool_size=8)(cnn1d3)\n",
    "cnn1d3 = Dropout(0.2)(cnn1d3)\n",
    "cnn1d3 = Flatten()(cnn1d3)\n",
    "#cnn1d3 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d3_input)\n",
    "c1d4_input = keras.Input(shape=(36,4))\n",
    "cnn1d4 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d4_input)\n",
    "cnn1d4 = MaxPooling1D(pool_size=8)(cnn1d4)\n",
    "cnn1d4 = Dropout(0.2)(cnn1d4)\n",
    "cnn1d4 = Flatten()(cnn1d4)\n",
    "#cnn1d4 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d4_input)\n",
    "#training_net21.append(cnn1d1)\n",
    "#training_net21.append(cnn1d2)\n",
    "#training_net22.append(cnn1d3)\n",
    "#training_net22.append(cnn1d4)\n",
    "\n",
    "#model21 = Sequential()\n",
    "model21 = keras.layers.concatenate([cnn1d1,cnn1d2])\n",
    "model21 = Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model21)\n",
    "model21 = BatchNormalization()(model21)\n",
    "model21 = Activation('relu')(model21)\n",
    "model21 = Dropout(0.2)(model21)\n",
    "#model22 = Sequential()\n",
    "model22 = keras.layers.concatenate([cnn1d3,cnn1d4])\n",
    "model22 = Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model22)\n",
    "model22 = BatchNormalization()(model22)\n",
    "model22 = Activation('relu')(model22)\n",
    "model22 = Dropout(0.2)(model22)\n",
    "#training_net2.append(model21)\n",
    "#training_net2.append(model22)\n",
    "#model2 = Sequential()\n",
    "model2 = keras.layers.concatenate([model21,model22])\n",
    "model2 = Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Activation('relu')(model2)\n",
    "model2 = Dropout(0.2)(model2)\n",
    "#model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "#model2.add(BatchNormalization())\n",
    "#model2.add(Activation('relu'))\n",
    "#model2.add(Dropout(0.2))\n",
    "\n",
    "#model1 = Sequential()\n",
    "model1 = keras.layers.concatenate([cnn2d1,cnn2d2])\n",
    "model1 = Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model1)\n",
    "model1 = Dropout(0.1)(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "model1 = Activation('relu')(model1)\n",
    "\n",
    "#training_net.append(model1)\n",
    "#training_net.append(model2)\n",
    "#model = Sequential()\n",
    "model = keras.layers.concatenate([model1,model2])\n",
    "model = Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "#model.add(Dropout(0.1))\n",
    "if Dense5>0:\n",
    "    model = Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = keras.layers.Activation('relu')(model)\n",
    "model = Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "#interaction_output = Activation('softmax', axis=1)(model)\n",
    "#model.add(Activation('softmax'))\n",
    "interaction = Model(inputs=[c2d1_input,c2d2_input,c1d1_input,c1d2_input,c1d3_input,c1d4_input],outputs=[keras.layers.Activation('softmax')(model)])\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "interaction.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose = verbose1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(true_y = [],training = [], y = [], batch_size=4, epochs=12, verbose1 = 1, verbose2 = 1,\n",
    "                              validation = [], val_y = [], testing = [], matrixsize11 = 6, nbfilter11 = 24, \n",
    "                              matrixsize12 = 7, nbfilter12 = 4, matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7,\n",
    "                              nbfilter22 = 4, nbfilter1 = 96, kernel_size1 = 7, nbfilters2 = 96,kernel_size2 = 7,\n",
    "                              Dense1 = 32, Dense2 = 192, Dense3 = 512, Dense4 = 64, Dense5 = 0):\n",
    "    \n",
    "    model = create_model2(matrixsize11, nbfilter11, matrixsize12, nbfilter12, matrixsize21, nbfilter21, \n",
    "                         matrixsize22, nbfilter22, nbfilter1, kernel_size1, nbfilters2,kernel_size2,\n",
    "                         Dense1, Dense2, Dense3, Dense4, Dense5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename = 'model', delete = True):\n",
    "    model.save(filename+'.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "    if delete :\n",
    "        del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_json(model, filename = \"model\"):\n",
    "    # serialize model to JSON\n",
    "    print(\"Model is saved in : \", filename + \".json\")\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    print(\"Weights are saved in : \", filename + \".h5\")\n",
    "    model.save_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_yaml(model, filename = \"model\"):\n",
    "    # serialize model to JSON\n",
    "    print(\"Model is saved in : \", filename + \".yaml\")\n",
    "    open(filename + \".yaml\", \"w\").write(model.to_yaml())\n",
    "    # serialize weights to HDF5\n",
    "    print(\"Weights are saved in : \", filename + \".h5\")\n",
    "    model.save_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_json(filename = \"model\"):\n",
    "    # load json and create model\n",
    "    print(\"Model loaded from : \", filename + \".json\")\n",
    "    model = model_from_json(open(filename + \".json\").read())\n",
    "    print(\"Weight of the model loaded from : \", filename + \".h5\")\n",
    "    model.load_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    # returns a compiled model\n",
    "    # identical to the previous one\n",
    "    model = load_model(filename+'.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_json(filename = \"model\"):\n",
    "    # load json and create model\n",
    "    print(\"Model loaded from : \", filename + \".yaml\")\n",
    "    model = model_from_yaml(open(filename + \".yaml\").read())\n",
    "    print(\"Weight of the model loaded from : \", filename + \".h5\")\n",
    "    model.load_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf(seq1, seq2):\n",
    "    seq1 = traduire(seq1)\n",
    "    seq2 = traduire(seq2)\n",
    "    \n",
    "    print(seq1)\n",
    "    print(seq2)\n",
    "    \n",
    "    sequence1 = np.zeros((1,36,4))\n",
    "    sequence2 = np.zeros((1,36,4))\n",
    "    revseq1 = np.zeros((1,36,4))\n",
    "    revseq2 = np.zeros((1,36,4))\n",
    "    product = np.zeros((1,36,36,1))\n",
    "    matrice3D = np.zeros((1,36,36,1))\n",
    "    \n",
    "    z = len(seq1)\n",
    "    z = len(seq2)\n",
    "    proba1 = np.array(RNA.pfl_fold_up(seq1,1,z,z))[1:,1]\n",
    "    proba2 = np.array(RNA.pfl_fold_up(seq2,1,z,z))[1:,1]\n",
    "    product[0] = get_product_proba(proba1,proba2)\n",
    "    \n",
    "    sequence1[0] = convert_sequence(seq1)\n",
    "    sequence2[0] = convert_sequence(seq2)\n",
    "    \n",
    "    matrice3D[0] = convert_matrice3D(sequence1[0],sequence2[0])\n",
    "    \n",
    "    for j in range(36):\n",
    "        revseq1[0][36-j-1] = sequence1[0][j]\n",
    "        revseq2[0][36-j-1] = sequence2[0][j]\n",
    "    \n",
    "    test = []\n",
    "    test.append(matrice3D)\n",
    "    test.append(product)\n",
    "    test.append(sequence1)\n",
    "    test.append(sequence2)\n",
    "    test.append(revseq1)\n",
    "    test.append(revseq2)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(seq1,seq2,window = 36, filename = 'model'):\n",
    "    model = test_model([],[],[],32,16,1,1,[],[],[],Dense1=128,Dense2=512,Dense3=512,Dense4=128,Dense5=64)\n",
    "    model.load_weights(filename)\n",
    "    l1 = len(seq1)\n",
    "    l2 = len(seq2)\n",
    "    output = np.zeros((max(1,l1-window),max(1,l2-window)))\n",
    "    for i in range(max(1,l1-window)):\n",
    "        for j in range(max(1,l2-window)):\n",
    "            test = transf(seq1[i:i+35],seq2[j:j+35])\n",
    "            output[i,j] = (model.predict_proba(test)[:,1])\n",
    "            print(output[i,j])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_benchmark(model, filename = \"model\"):\n",
    "    print(\"Test on benchmark dataset\")\n",
    "    #model = load_model(filename)\n",
    "    test = load_datatest()\n",
    "    print(\"Data loaded\")\n",
    "    predictions = model.predict(test)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    true_y = np.ones((len(predictions)))\n",
    "    for i in range(0,len(predictions),2):\n",
    "        print(i)\n",
    "        true_y[i] = 0\n",
    "    \n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = get_output('GUGCGGCCUGAAAAACAGUGCUGUGCCCUUGUAACUCAUCAUAAUAAUUUACGGCGCAGCCAAGAUUUCCCUGGUGUUGGCGCAGUAUUCGCGCACCCCGGUCUAGCCGGGGUCAUUUUUU','UUAUGCUGGUAACGCGCUGCGCGGCUACGGUAAUCUGAUUAUCAUCAAACAUAAUGAUGAUUACCUGAGUGCCUACGCCCAUAACGACACAAUGCUGGUCCGGGAACAACAAGAAGUUAAGGCGGGGCAAAAAAUAGCGACCAUGGGUAGCACCGGAACCAGUUCAACACGCUUGCAUUUUGAAAUUCGUUACAAGGGGAAAUCCGUAAACCCGCUGCGUUAUUUGCCGCAGCGAUAAAUCGGCGGAACCAGGCUUUUGCUUGAAUGUUCCGUCAAGGGAUCACGGGUAGGAGCCACCUUAUGAGUCAGAAUACGCUGAAAGUUCAUGAUUUAAAUGAAGAUGCGGAAUUUGAUGAGAACGGAGUUGAGGUUUUUGACGAAAAGGCCUUAGUAGAACAGGAACCCAGUGAUAACGAUUUGGCCGAAGAGGAACUGUUAUCGCAGGGAGCCACACAGCGUGUGUUGGACGCGACUCAGCUUUACCUUGGUGAGAUUGGUUAUUCACCACUGUUAACGGCCGAAGAAGAAGUUUAUUUUGCGCGUCGCGCACUGCGUGGAGAUGUCGCCUCUCGCCGCCGGAUGAUCGAGAGUAACUUGCGUCUGGUGGUAAAAAUUGCCCGCCGUUAUGGCAAUCGUGGUCUGGCGUUGCUGGACCUUAUCGAAGAGGGCAACCUGGGGCUGAUCCGCGCGGUAGAGAAGUUUGACCCGGAACGUGGUUUCCGCUUCUCAACAUACGCAACCUGGUGGAUUCGCCAGACGAUUGAACGGGCGAUUAUGAACCAAACCCGUACUAUUCGUUUGCCGAUUCACAUCGUAAAGGAGCUGAACGUUUACCUGCGAACCGCACGUGAGUUGUCCCAUAAGCUGGACCAUGAACCAAGUGCGGAAGAGAUCGCAGAGCAACUGGAUAAGCCAGUUGAUGACGUCAGCCGUAUGCUUCGUCUUAACGAGCGCAUUACCUCGGUAGACACCCCGCUGGGUGGUGAUUCCGAAAAAGCGUUGCUGGACAUCCUGGCCGAUGAAAAAGAGAACGGUCCGGAAGAUACCACGCAAGAUGACGAUAUGAAGCAGAGCAUCGUCAAAUGGCUGUUCGAGCUGAACGCCAAACAGCGUGAAGUGCUGGCACGUCGAUUCGGUUUGCUGGGGUACGAAGCGGCAACACUGGAAGAUGUAGGUCGUGAAAUUGGCCUCACCCGUGAACGUGUUCGCCAGAUUCAGGUUGAAGGCCUGCGCCGUUUGCGCGAAAUCCUGCAAACGCAGGGGCUGAAUAUCGAAGCGCUGUUCCGCGAGUAA',36, \"model_double77-6\")\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on benchmark dataset\n",
      "i :  1\n",
      "taille 1 :  25\n",
      "taille 2 :  21\n",
      "index :  0\n",
      "sequence :  uucccugguguuggcgcaguauucg  de longueur :  25\n",
      "sequence :  gaaauucguuacaaggggaaa  de longueur :  21\n",
      "index :  1\n",
      "sequence :  uguaacucaucauaauaauuuacggcgcagccaaga  de longueur :  36\n",
      "sequence :  ccguaaacccgcugcguuauuugccgcagcgauaaa  de longueur :  36\n",
      "i :  2\n",
      "taille 1 :  9\n",
      "taille 2 :  10\n",
      "index :  2\n",
      "sequence :  uuucccugg  de longueur :  9\n",
      "sequence :  caggagaaau  de longueur :  10\n",
      "index :  3\n",
      "sequence :  guuggcgcaguauucgcgcaccccggucaaaccggg  de longueur :  36\n",
      "sequence :  gauggaaaccacucagaccagcacuauugcuucgau  de longueur :  36\n",
      "i :  3\n",
      "taille 1 :  24\n",
      "taille 2 :  20\n",
      "index :  4\n",
      "sequence :  uucccugguguuggcgcaguauuc  de longueur :  24\n",
      "sequence :  gauaugcaaucccagggaga  de longueur :  20\n",
      "index :  5\n",
      "sequence :  uuugacaucaucauaauaagcacggcgcagccacga  de longueur :  36\n",
      "sequence :  aauauguuuuugcauaacauuaaaauacguucaaaa  de longueur :  36\n",
      "i :  4\n",
      "taille 1 :  17\n",
      "taille 2 :  16\n",
      "index :  6\n",
      "sequence :  ccugguguuggcgcagu  de longueur :  17\n",
      "sequence :  cuguacauuuccaggg  de longueur :  16\n",
      "index :  7\n",
      "sequence :  gacaucaucauaauaagcacggcgcagccacgauuu  de longueur :  36\n",
      "sequence :  aacccggucaccguugccaacguuauuccgcaggcu  de longueur :  36\n",
      "i :  5\n",
      "taille 1 :  20\n",
      "taille 2 :  20\n",
      "index :  8\n",
      "sequence :  ugaaauuccucuuugacggg  de longueur :  20\n",
      "sequence :  ccgucaaagaguuauuucau  de longueur :  20\n",
      "index :  9\n",
      "sequence :  caccgucgcuuaaagugacggcauaauaauaaaaaa  de longueur :  36\n",
      "sequence :  aaucaauaccgcaauauuuaaauugcgguuuuuaag  de longueur :  36\n",
      "i :  6\n",
      "taille 1 :  11\n",
      "taille 2 :  11\n",
      "index :  10\n",
      "sequence :  ccucuuugacg  de longueur :  11\n",
      "sequence :  gucaaagagga  de longueur :  11\n",
      "index :  11\n",
      "sequence :  gcuuaaagugacggcauaauaauaaaaaaaugaaau  de longueur :  36\n",
      "sequence :  uaacccaugcguacguuuaguggcaaacguaguacg  de longueur :  36\n"
     ]
    }
   ],
   "source": [
    "# Récupération des poids\n",
    "interaction.load_weights('model_double_final.h5')\n",
    "\n",
    "test_on_benchmark(interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_model(Data[0],Data[1],16,6,1,1,Data[2],Data[3],Data[4],6,24,7,6,6,24,7,6,64,7,64,7,512,128,512,128,64)\n",
    "#save_model(model, \"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
