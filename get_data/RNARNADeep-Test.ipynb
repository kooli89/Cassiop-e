{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/conspiracy/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import RNA\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "#from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99515374, 0.98200108, 0.97327828, 0.83831717, 0.71386684,\n",
       "       0.70952593, 0.79190253, 0.74114161, 0.24841253, 0.20895404,\n",
       "       0.37044396, 0.07146792, 0.09764458, 0.13250682, 0.1708176 ,\n",
       "       0.30276394, 0.90875754, 0.84469873, 0.79897224, 0.34805775,\n",
       "       0.62012246, 0.22103493, 0.05596614, 0.39760585, 0.50417549,\n",
       "       0.45270863, 0.77728394, 0.78237052, 0.35248909, 0.27293962,\n",
       "       0.40199972, 0.82078131, 0.26921011, 0.79933257, 0.63717472,\n",
       "       0.42815308, 0.4700229 , 0.01334941, 0.25455379, 0.22365579,\n",
       "       0.20206252, 0.21610497, 0.92423724, 0.77800337, 0.34187502,\n",
       "       0.1734228 , 0.17031566, 0.79341276])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqtest = 'augugacugaugcuagugaugucgucgucgaggggaucuagcugaucg'\n",
    "z = len(seqtest)\n",
    "np.array(RNA.pfl_fold_up(seqtest,1,z,z))[1:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduire(seq):\n",
    "    nucleotides = ''\n",
    "    for lettre in seq:\n",
    "            if lettre=='A' or lettre=='a':\n",
    "                nucleotides=nucleotides+'a'\n",
    "            elif lettre=='C' or lettre=='c':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            elif lettre=='G' or lettre=='g':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'u'\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sequence(sequence):\n",
    "    matrice = np.zeros((36,4))\n",
    "    for i in range(len(sequence)):\n",
    "            if sequence[i]=='a':\n",
    "                matrice[i,0] = 1\n",
    "            elif sequence[i]=='u':\n",
    "                matrice[i,1] = 1\n",
    "            elif sequence[i]=='g':\n",
    "                matrice[i,2] = 1\n",
    "            elif sequence[i]=='c':\n",
    "                matrice[i,3] = 1\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_matrice3D(seq1,seq2):\n",
    "    matrice = np.zeros((36,36,1))\n",
    "    for j in range(len(seq1)):\n",
    "        for k in range(len(seq2)):\n",
    "            if (seq1[j][0]==1 and seq2[k][1]==1) or (seq1[j][1]==1 and seq2[k][0]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "            elif (seq1[j][2]==1 and seq2[k][3]==1) or (seq1[j][3]==1 and seq2[k][2]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "            elif (seq1[j][2]==1 and seq2[k][1]==1) or (seq1[j][1]==1 and seq2[k][2]==1):\n",
    "                matrice[j,k,0] = 1\n",
    "    return matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_proba(proba1,proba2):\n",
    "    product = np.zeros((36,36,1))\n",
    "    for j in range(len(proba1)):\n",
    "            for k in range(len(proba2)):\n",
    "                product[j][k] = proba1[j] * proba2[k]\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_proba(proba1,proba2):\n",
    "    somme = np.zeros((36,36,1))\n",
    "    for j in range(len(proba1)):\n",
    "            for k in range(len(proba2)):\n",
    "                somme[j][k] = proba1[j] + proba2[k]\n",
    "    return somme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datatest_positif():\n",
    "    datatest = pd.read_csv(\"genomes/nar-03123-met-g-2015-File010.csv\", sep = \",\", header=None)\n",
    "    datatest = np.array(datatest)\n",
    "    l = len(datatest)\n",
    "    seq1 = np.zeros((100,36,4))\n",
    "    seq2 = np.zeros((100,36,4))\n",
    "    matrice3D = np.zeros((100,36,36,1))\n",
    "    product = np.zeros((100,36,36,1))\n",
    "    index = 0\n",
    "    for i in range(1,l):\n",
    "        print(\"i : \",i)\n",
    "        begin1 = int(datatest[i,9])\n",
    "        end1 = int(datatest[i,10])\n",
    "        size1 = end1-begin1\n",
    "        if(size1 <= 36):\n",
    "    \n",
    "            begin2 = int(datatest[i,11])\n",
    "            end2 = int(datatest[i,12])\n",
    "            size2 = end2-begin2\n",
    "    \n",
    "            if(size2 <= 36):\n",
    "                proba1 = []\n",
    "                proba2 = []\n",
    "                print(\"taille 1 : \", size1)\n",
    "                print(\"taille 2 : \", size2)\n",
    "                print(\"index : \",index)\n",
    "                sequence = traduire(datatest[i,23][begin1:end1])\n",
    "                #z = len(sequence)\n",
    "                print(sequence)\n",
    "                #proba1 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                proba1 = np.array(RNA.pfl_fold_up(sequence,1,len(sequence),len(sequence)))[1:,1]\n",
    "                seq1[index] = convert_sequence(sequence)\n",
    "                \n",
    "                sequence = traduire(datatest[i,24][begin2:end2])\n",
    "                bdd[index][1] = sequence\n",
    "                #z = len(sequence)\n",
    "                print(sequence)\n",
    "                print(proba1)\n",
    "                #proba2 = np.array(RNA.pfl_fold_up(sequence, 1, z, z))[1:,1]\n",
    "                proba2 = np.array(RNA.pfl_fold_up(sequence,1,len(sequence),len(sequence)))[1:,1]\n",
    "                print(\"proba : \", proba2)\n",
    "                seq2[index] = convert_sequence(sequence)\n",
    "                \n",
    "                bdd[index][2] = proba1\n",
    "                bdd[index][3] = proba2\n",
    "                \n",
    "                matrice3D[index] = convert_matrice3D(seq1[index],seq2[index])\n",
    "                product[index] = get_product_proba(proba1,proba2)\n",
    "                \n",
    "                '''\n",
    "                index+=1\n",
    "                print(\"index : \",index)\n",
    "                longueur = int(datatest[i,7])\n",
    "                print(\"longueur : \", longueur)\n",
    "                if((longueur - end1) < 36):\n",
    "                    begin = begin1-37\n",
    "                    end = begin1-1                \n",
    "                else :\n",
    "                    begin = end1 + 1\n",
    "                    end = end1 + 37\n",
    "                \n",
    "                print(\"begin : \", begin, \", end : \", end)\n",
    "                sequence = traduire(datatest[i,23][begin:end])\n",
    "                #z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", len(sequence))\n",
    "                #proba1 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                proba1 = np.array(RNA.pfl_fold_up(sequence,1,len(sequence),len(sequence)))[1:,1]\n",
    "                print(\"proba : \", proba1)\n",
    "                seq1[index] = convert_sequence(sequence)\n",
    "            \n",
    "                print(\"begin : \", begin, \", end : \", end)\n",
    "                begin = end2+1\n",
    "                end = end2+37\n",
    "                sequence = traduire(datatest[i,24][begin:end])\n",
    "                #z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", len(sequence))\n",
    "                #proba2 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                proba2 = np.array(RNA.pfl_fold_up(sequence,1,len(sequence),len(sequence)))[1:,1]\n",
    "                print(\"proba : \", proba1)\n",
    "                seq2[index] = convert_sequence(sequence)\n",
    "                \n",
    "                matrice3D[index] = convert_matrice3D(seq1[index],seq2[index])\n",
    "                product[index] = get_product_proba(proba1,proba2)\n",
    "                \n",
    "                index+=1\n",
    "                '''\n",
    "    \n",
    "    df = pd.DataFrame(bdd)\n",
    "    df.to_csv(\"/genomes/positif_benchmark.csvs\")\n",
    "    \n",
    "    l = len(seq1)\n",
    "    revseq1 = np.zeros((l,36,4))\n",
    "    revseq2 = np.zeros((l,36,4))\n",
    "    \n",
    "    for i in range(l):\n",
    "        for j in range(36):\n",
    "            revseq1[i][36-j-1] = seq1[i][j]\n",
    "            revseq2[i][36-j-1] = seq2[i][j]\n",
    "    \n",
    "    test = []\n",
    "    test.append(matrice3D)\n",
    "    test.append(product)\n",
    "    test.append(seq1)\n",
    "    test.append(revseq1)\n",
    "    test.append(seq2)\n",
    "    test.append(revseq2)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datatest_negatif():\n",
    "    datatest = pd.read_csv(\"genomes/nar-03123-met-g-2015-File010.csv\", sep = \",\", header=None)\n",
    "    datatest = np.array(datatest)\n",
    "    l = len(datatest)\n",
    "    seq1 = np.zeros((100,36,4))\n",
    "    seq2 = np.zeros((100,36,4))\n",
    "    matrice3D = np.zeros((100,36,36,1))\n",
    "    product = np.zeros((100,36,36,1))\n",
    "    index = 0\n",
    "    for i in range(1,l):\n",
    "        print(\"i : \",i)\n",
    "        begin1 = int(datatest[i,9])\n",
    "        end1 = int(datatest[i,10])\n",
    "        size1 = end1-begin1\n",
    "        if(size1 <= 36):\n",
    "    \n",
    "            begin2 = int(datatest[i,11])\n",
    "            end2 = int(datatest[i,12])\n",
    "            size2 = end2-begin2\n",
    "    \n",
    "            if(size2 <= 36):\n",
    "                proba1 = []\n",
    "                proba2 = []\n",
    "                print(\"taille 1 : \", size1)\n",
    "                print(\"taille 2 : \", size2)\n",
    "                print(\"index : \",index)\n",
    "                longueur = int(datatest[i,7])\n",
    "                print(\"longueur : \", longueur)\n",
    "                if((longueur - end1) < 36):\n",
    "                    begin = begin1-37\n",
    "                    end = begin1-1                \n",
    "                else :\n",
    "                    begin = end1 + 1\n",
    "                    end = end1 + 37\n",
    "                \n",
    "                print(\"begin : \", begin, \", end : \", end)\n",
    "                sequence = traduire(datatest[i,23][begin:end])\n",
    "                #z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", len(sequence))\n",
    "                #proba1 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                proba1 = np.array(RNA.pfl_fold_up(sequence,1,len(sequence),len(sequence)))[1:,1]\n",
    "                print(\"proba : \", proba1)\n",
    "                seq1[index] = convert_sequence(sequence)\n",
    "            \n",
    "                print(\"begin : \", begin, \", end : \", end)\n",
    "                begin = end2+1\n",
    "                end = end2+37\n",
    "                sequence = traduire(datatest[i,24][begin:end])\n",
    "                #z = len(sequence)\n",
    "                print(\"sequence : \",sequence, \" de longueur : \", len(sequence))\n",
    "                #proba2 = np.array(RNA.pfl_fold_up(sequence,1,z,z))[1:,1]\n",
    "                proba2 = np.array(RNA.pfl_fold_up(sequence,1,len(sequence),len(sequence)))[1:,1]\n",
    "                print(\"proba : \", proba1)\n",
    "                seq2[index] = convert_sequence(sequence)\n",
    "                \n",
    "                matrice3D[index] = convert_matrice3D(seq1[index],seq2[index])\n",
    "                product[index] = get_product_proba(proba1,proba2)\n",
    "                \n",
    "                index+=1\n",
    "    \n",
    "    l = len(seq1)\n",
    "    revseq1 = np.zeros((l,36,4))\n",
    "    revseq2 = np.zeros((l,36,4))\n",
    "    \n",
    "    for i in range(l):\n",
    "        for j in range(36):\n",
    "            revseq1[i][36-j-1] = seq1[i][j]\n",
    "            revseq2[i][36-j-1] = seq2[i][j]\n",
    "    \n",
    "    test = []\n",
    "    test.append(matrice3D)\n",
    "    test.append(product)\n",
    "    test.append(seq1)\n",
    "    test.append(revseq1)\n",
    "    test.append(seq2)\n",
    "    test.append(revseq2)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(seq):\n",
    "    seq = seq.split('\\n')\n",
    "    seq2 = ''\n",
    "    for j in seq:\n",
    "        seq2 = seq2 + j\n",
    "    seq2 = seq2[2:len(seq2)-1]\n",
    "    seq2 = seq2.split(' ')\n",
    "    #print(seq2)\n",
    "    seq3=[]\n",
    "    for j in seq2:\n",
    "        #print(j)\n",
    "        if j=='':\n",
    "            a=0\n",
    "        else:\n",
    "            seq3.append(float(j))\n",
    "    return seq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # load your data using this function\n",
    "    neg1 = []\n",
    "\n",
    "    neg1 = pd.read_csv(\"genomes/negatifs_m-m-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg1 = np.array(neg1)\n",
    "    print(len(neg1))\n",
    "    for i in range(len(neg1)):\n",
    "        for j in range(2):\n",
    "            if len(neg1[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "    neg2 = pd.read_csv(\"genomes/negatifs_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg2 = np.array(neg2)\n",
    "    print(len(neg2))\n",
    "    for i in range(len(neg2)):\n",
    "        for j in range(2):\n",
    "            if len(neg2[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    \n",
    "    neg3 = pd.read_csv(\"genomes/negatifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)\n",
    "    neg3 = np.array(neg3)\n",
    "    print(len(neg3))\n",
    "    for i in range(len(neg3)):\n",
    "        for j in range(2):\n",
    "            if len(neg3[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "\n",
    "    pos1 = pd.read_csv(\"genomes/positifs_m-m-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos1 = np.array(pos1)\n",
    "    print(len(pos1))\n",
    "    for i in range(len(pos1)):\n",
    "        for j in range(2):\n",
    "            if len(pos1[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l = len(pos1)\n",
    "    \n",
    "    pos2 = pd.read_csv(\"genomes/positifs_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos2 = np.array(pos2)\n",
    "    print(len(pos2))\n",
    "    for i in range(len(pos2)):\n",
    "        for j in range(2):\n",
    "            if len(pos2[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l=l+len(pos2)\n",
    "\n",
    "    pos3 = pd.read_csv(\"genomes/positifs_mouse_divers-str.csv\", sep = \"\\t\",header=None)  \n",
    "    pos3 = np.array(pos3)\n",
    "    print(len(pos3))\n",
    "    for i in range(len(pos3)):\n",
    "        for j in range(2):\n",
    "            if len(pos3[i,j]) <= 15:\n",
    "                print(i,\"erreur\")\n",
    "    l=l+len(pos3)\n",
    "\n",
    "    bdd = np.concatenate((pos1,pos2,pos3,neg1,neg2,neg3))\n",
    "\n",
    "    pos1 = []\n",
    "    neg1 = []\n",
    "    pos2 = []\n",
    "    neg2 = []\n",
    "    pos3 = []\n",
    "    neg3 = []\n",
    "\n",
    "    labels = np.zeros((len(bdd),1))\n",
    "\n",
    "    bdd = np.concatenate((bdd,labels),axis=1)\n",
    "    for i in range(l):\n",
    "        bdd[i,4]=1\n",
    "    labels=[]\n",
    "    \n",
    "    # shuffle total\n",
    "\n",
    "    indices = np.arange(len(bdd))\n",
    "    shuffle(indices)\n",
    "    bdd = bdd[indices]\n",
    "    indices=[]\n",
    "\n",
    "    l = len(bdd)\n",
    "    matrice = np.zeros((l,36,36,1))\n",
    "    matrice1 = np.zeros((l,36,36,1))\n",
    "    matrice2 = np.zeros((l,36,4))\n",
    "    matrice3 = np.zeros((l,36,4))\n",
    "\n",
    "    for i in range(l):\n",
    "        seq1 = bdd[i,0]\n",
    "        seq2 = bdd[i,1]\n",
    "        prob1 = np.array(clean(bdd[i,2]))\n",
    "        prob2 = np.array(clean(bdd[i,3]))\n",
    "        for j in range(len(seq1)):\n",
    "            for k in range(len(seq2)):\n",
    "                if (seq1[j]=='a' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='a'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                elif (seq1[j]=='g' and seq2[k]=='c') or (seq1[j]=='c' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                elif (seq1[j]=='g' and seq2[k]=='u') or (seq1[j]=='u' and seq2[k]=='g'):\n",
    "                    matrice[i,j,k,0] = 1\n",
    "                matrice1[i,j,k,0] = prob1[j]+prob2[k]\n",
    "        for j in range(len(seq1)):\n",
    "            if seq1[j]=='a':\n",
    "                matrice2[i,j,0] = 1\n",
    "            elif seq1[j]=='u':\n",
    "                matrice2[i,j,1] = 1\n",
    "            elif seq1[j]=='g':\n",
    "                matrice2[i,j,2] = 1\n",
    "            elif seq1[j]=='c':\n",
    "                matrice2[i,j,3] = 1\n",
    "        for j in range(len(seq2)):\n",
    "            if seq2[j]=='a':\n",
    "                matrice3[i,j,0] = 1\n",
    "            elif seq2[j]=='u':\n",
    "                matrice3[i,j,1] = 1\n",
    "            elif seq2[j]=='g':\n",
    "                matrice3[i,j,2] = 1\n",
    "            elif seq2[j]=='c':\n",
    "                matrice3[i,j,3] = 1\n",
    "                \n",
    "    labels = bdd[:,4]\n",
    "    bdd = []\n",
    "    \n",
    "    return matrice, matrice1, matrice2, matrice3, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(nb_train = 165000,nb_val = 6000,nb_test = 5000):\n",
    "    data = load_dataset()\n",
    "    matrice = data[0]\n",
    "    matrice1 = data[1]\n",
    "    matrice2 = data[2]\n",
    "    matrice3 = data[3]\n",
    "    labels = data[4]\n",
    "    \n",
    "    training = []\n",
    "    training.append(matrice[:nb_train])\n",
    "    training.append(matrice1[:nb_train])\n",
    "    training.append(matrice2[:nb_train])\n",
    "    training.append(matrice3[:nb_train])\n",
    "\n",
    "    validation = []\n",
    "    validation.append(matrice[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice1[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice2[nb_train:nb_train+nb_val])\n",
    "    validation.append(matrice3[nb_train:nb_train+nb_val])\n",
    "\n",
    "    testing = []\n",
    "    testing.append(matrice[-nb_test:])\n",
    "    testing.append(matrice1[-nb_test:])\n",
    "    testing.append(matrice2[-nb_test:])\n",
    "    testing.append(matrice3[-nb_test:])\n",
    "    \n",
    "    y = labels[:nb_train]\n",
    "    y = keras.utils.np_utils.to_categorical(y,2)\n",
    "    val_y = labels[nb_train:nb_train+nb_val]\n",
    "    val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "    true_y = labels[-nb_test:]\n",
    "    argtest=[]\n",
    "    np.sum(y[:,1])\n",
    "    return training, y, validation, val_y, testing, true_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(test_num, pred_y,  labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    print(tp+tn)\n",
    "    if (tp+fp) == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = float(tp)/(tp+ fp)\n",
    "    if (tp+fn) == 0:\n",
    "        sensitivity = 0\n",
    "    else:\n",
    "        sensitivity = float(tp)/ (tp+fn)\n",
    "    if (tn+fp) == 0:\n",
    "        specificity = 0  \n",
    "    else:\n",
    "        specificity = float(tn)/(tn + fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfs_test(model,test,true):\n",
    "    print('predicting')\n",
    "            \n",
    "    predictions = model.predict(test)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true)\n",
    "    \n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network(matrixsize = 6, nbfilter = 24, matrixsize2 = 7, nbfilter2 = 6):    \n",
    "    k = matrixsize\n",
    "    # init_weights\n",
    "    I = np.eye(k)\n",
    "    M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "    I2 = np.zeros((k,k))\n",
    "    M2 = np.zeros((k,k))\n",
    "    for j in range(k):\n",
    "        I2[:,j] = I[:,k-j-1]\n",
    "        M2[:,j] = M[:,k-j-1]        \n",
    "    W = np.zeros((k,k,1,nbfilter))\n",
    "    W[:,:,0,0] = I\n",
    "    W[:,:,0,1] = I2\n",
    "    W[:,:,0,2] = M\n",
    "    W[:,:,0,3] = M2\n",
    "    for j in range(4,nbfilter):\n",
    "        W[:,:,0,j] = np.random.randn(k,k)*0.2\n",
    "    print('configure cnn network')\n",
    " \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = nbfilter, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter)]))    \n",
    "    model.add(AveragePooling2D(pool_size=(3,3)))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    k2 = matrixsize2\n",
    "    I = np.eye(k2)\n",
    "    M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "    I2=np.zeros((k2,k2))\n",
    "    M2=np.zeros((k2,k2))\n",
    "    for j in range(k2):\n",
    "        I2[:,j] = I[:,k2-j-1]\n",
    "        M2[:,j] = M[:,k2-j-1]   \n",
    "    \n",
    "    Z = np.zeros((k2,k2,nbfilter,nbfilter2))\n",
    "    \n",
    "    for u in range(nbfilter):\n",
    "        Z[:,:,u,0] = I\n",
    "        Z[:,:,u,1] = I2\n",
    "        Z[:,:,u,2] = M\n",
    "        Z[:,:,u,3] = M2\n",
    "        for p in range(4,nbfilter2):\n",
    "            Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            \n",
    "\n",
    "    model.add(Conv2D(filters = nbfilter2, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter2)]))\n",
    "    print(model.output_shape)    \n",
    "    model.add(Flatten())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network_seq(nbfilter = 64, kernelsize = 7):        \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters = nbfilter, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(MaxPooling1D(pool_size=6))\n",
    "    print(model.output_shape)\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(matrixsize11 = 6, nbfilter11 = 24, matrixsize12 = 7, nbfilter12 = 6, \n",
    "                 matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7, nbfilter22 = 6,\n",
    "                 nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                 Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    training_net=[]\n",
    "    training_net1 = []\n",
    "    training_net2 = []\n",
    "    training_net1.append(get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12))\n",
    "    training_net1.append(get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22))\n",
    "    training_net2.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net2.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    \n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(Merge(training_net2, mode ='concat'))\n",
    "    model2.add(Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Activation('relu'))\n",
    "    #model2.add(Dropout(0.1))\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Merge(training_net1, mode ='concat'))\n",
    "    model1.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model1.add(Dropout(0.1))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Activation('relu'))\n",
    "    \n",
    "    training_net.append(model1)\n",
    "    training_net.append(model2)\n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    "    \n",
    "    model.add(Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy','roc_curve','auc'])\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(matrixsize11 = 6, nbfilter11 = 24, matrixsize12 = 7, nbfilter12 = 6, \n",
    "                 matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7, nbfilter22 = 6,\n",
    "                 nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                 Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    training_net=[]\n",
    "    training_net1 = []\n",
    "    training_net2 = []\n",
    "    training_net21 = []\n",
    "    training_net22 = []\n",
    "    training_net1.append(get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12))\n",
    "    training_net1.append(get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22))\n",
    "    training_net21.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net21.append(get_cnn_network_seq(nbfilter1,kernel_size1))\n",
    "    training_net22.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    training_net22.append(get_cnn_network_seq(nbfilters2,kernel_size2))\n",
    "    \n",
    "    \n",
    "    model21 = Sequential()\n",
    "    model21.add(Merge(training_net21, mode ='concat'))\n",
    "    model21.add(Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model21.add(BatchNormalization())\n",
    "    model21.add(Activation('relu'))\n",
    "    model21.add(Dropout(0.2))\n",
    "    model22 = Sequential()\n",
    "    model22.add(Merge(training_net22, mode ='concat'))\n",
    "    model22.add(Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model22.add(BatchNormalization())\n",
    "    model22.add(Activation('relu'))\n",
    "    model22.add(Dropout(0.2))\n",
    "    training_net2.append(model21)\n",
    "    training_net2.append(model22)\n",
    "    model2 = Sequential()\n",
    "    model2.add(Merge(training_net2, mode ='concat'))\n",
    "    model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model2.add(BatchNormalization())\n",
    "    model2.add(Activation('relu'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    \n",
    "    #model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    #model2.add(BatchNormalization())\n",
    "    #model2.add(Activation('relu'))\n",
    "    #model2.add(Dropout(0.2))\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Merge(training_net1, mode ='concat'))\n",
    "    model1.add(Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model1.add(Dropout(0.1))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(Activation('relu'))\n",
    "    \n",
    "    training_net.append(model1)\n",
    "    training_net.append(model2)\n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    "    \n",
    "    model.add(Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    if Dense5>0:\n",
    "        model.add(Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data = load_data(165000,6000,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training = [], y = [], batch_size=16, epochs=6, verbose1 = 1, verbose2 = 1,\n",
    "                              validation = [], val_y = [], testing = [], matrixsize11 = 6, nbfilter11 = 24, \n",
    "                              matrixsize12 = 7, nbfilter12 = 6, matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7,\n",
    "                              nbfilter22 = 6, nbfilter1 = 64, kernel_size1 = 7, nbfilters2 = 64,kernel_size2 = 7,\n",
    "                              Dense1 = 512, Dense2 = 128, Dense3 = 512, Dense4 = 128, Dense5 = 64):\n",
    "    model = create_model(matrixsize11, nbfilter11, matrixsize12, nbfilter12, matrixsize21, nbfilter21, \n",
    "                         matrixsize22, nbfilter22, nbfilter1, kernel_size1, nbfilters2,kernel_size2,\n",
    "                         Dense1, Dense2, Dense3, Dense4, Dense5)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose = verbose1)\n",
    "    print('model training')\n",
    "    model.fit(training, y, batch_size, epochs, verbose = verbose2, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "    \n",
    "    # test   \n",
    "    \n",
    "    print('predicting')\n",
    "    \n",
    "        \n",
    "    predictions = model.predict_proba(testing)[:,1]\n",
    "    print(predictions)\n",
    "    for i,null in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    true_y = np.ones((len(predictions)))\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"batch_size : \", batch_size)\n",
    "    print(\"epochs : \",epochs)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres \n",
    "batch_size=64\n",
    "epochs=25\n",
    "verbose1 = 1\n",
    "verbose2 = 1\n",
    "matrixsize11 = 6\n",
    "nbfilter11 = 24\n",
    "matrixsize12 = 7\n",
    "nbfilter12 = 4\n",
    "matrixsize21 = 6\n",
    "nbfilter21 = 24\n",
    "matrixsize22 = 7\n",
    "nbfilter22 = 4\n",
    "nbfilter1 = 96\n",
    "kernelsize = 7\n",
    "nbfilters2 = 96\n",
    "kernel_size2 = 7\n",
    "Dense1 = 128\n",
    "Dense2 = 512\n",
    "Dense3 = 512\n",
    "Dense4 = 128\n",
    "Dense5 = 64\n",
    "\n",
    "from keras.layers import merge\n",
    "from keras import Model\n",
    "k = matrixsize11\n",
    "# init_weights\n",
    "I = np.eye(k)\n",
    "M = np.diag(np.ones(k-1),1) + np.diag(np.ones(k-1),-1) + np.eye(k)\n",
    "I2 = np.zeros((k,k))\n",
    "M2 = np.zeros((k,k))\n",
    "for j in range(k):\n",
    "    I2[:,j] = I[:,k-j-1]\n",
    "    M2[:,j] = M[:,k-j-1]        \n",
    "W = np.zeros((k,k,1,nbfilter11))\n",
    "W[:,:,0,0] = I\n",
    "W[:,:,0,1] = I2\n",
    "W[:,:,0,2] = M\n",
    "W[:,:,0,3] = M2\n",
    "for j in range(4,nbfilter11):\n",
    "    W[:,:,0,j] = np.random.randn(k,k)*0.2\n",
    "\n",
    "\n",
    "k2 = matrixsize12\n",
    "I = np.eye(k2)\n",
    "M = np.diag(np.ones(k2-1),1) + np.diag(np.ones(k2-1),-1) + np.eye(k2)\n",
    "I2=np.zeros((k2,k2))\n",
    "M2=np.zeros((k2,k2))\n",
    "for j in range(k2):\n",
    "    I2[:,j] = I[:,k2-j-1]\n",
    "    M2[:,j] = M[:,k2-j-1]   \n",
    "\n",
    "Z = np.zeros((k2,k2,nbfilter11,nbfilter12))\n",
    "\n",
    "for u in range(nbfilter12):\n",
    "    Z[:,:,u,0] = I\n",
    "    Z[:,:,u,1] = I2\n",
    "    Z[:,:,u,2] = M\n",
    "    Z[:,:,u,3] = M2\n",
    "    for p in range(4,nbfilter12):\n",
    "        Z[:,:,u,p]=np.random.randn(k2,k2)*0.3            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un modèle similaire à celui enregistré\n",
    "c2d1_input = keras.Input(shape=(36,36,1))\n",
    "cnn2d1 = Conv2D(filters = nbfilter11, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter11)])(c2d1_input)\n",
    "cnn2d1 = AveragePooling2D(pool_size=(3,3))(cnn2d1)\n",
    "cnn2d1 = Conv2D(filters = nbfilter12, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter12)])(cnn2d1)\n",
    "cnn2d1 = Dropout(0.1)(cnn2d1)\n",
    "cnn2d1 = Flatten()(cnn2d1)\n",
    "#cnn2d1 = get_cnn_network(matrixsize11, nbfilter11, matrixsize12, nbfilter12)(c2d1_input)\n",
    "#training_net1.append(cnn2d1)\n",
    "c2d2_input = keras.Input(shape=(36,36,1))\n",
    "cnn2d2 = Conv2D(filters = nbfilter11, kernel_size=(k,k), padding='valid', input_shape=(36,36,1),strides=(1,1),weights=[W,np.zeros(nbfilter11)])(c2d1_input)\n",
    "cnn2d2 = AveragePooling2D(pool_size=(3,3))(cnn2d2)\n",
    "cnn2d2 = Conv2D(filters = nbfilter12, kernel_size = (k2, k2),strides=(1,1),padding='valid',weights=[Z,np.zeros(nbfilter12)])(cnn2d2)\n",
    "cnn2d2 = Dropout(0.1)(cnn2d2)\n",
    "cnn2d2 = Flatten()(cnn2d2)\n",
    "#cnn2d2 = get_cnn_network(matrixsize21, nbfilter21, matrixsize22, nbfilter22)(c2d2_input)\n",
    "#training_net1.append(cnn2d2)\n",
    "c1d1_input = keras.Input(shape=(36,4))\n",
    "cnn1d1 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d1_input)\n",
    "cnn1d1 = MaxPooling1D(pool_size=8)(cnn1d1)\n",
    "cnn1d1 = Dropout(0.2)(cnn1d1)\n",
    "cnn1d1 = Flatten()(cnn1d1)\n",
    "#cnn1d1 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d1_input)\n",
    "c1d2_input = keras.Input(shape=(36,4))\n",
    "cnn1d2 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d2_input)\n",
    "cnn1d2 = MaxPooling1D(pool_size=8)(cnn1d2)\n",
    "cnn1d2 = Dropout(0.2)(cnn1d2)\n",
    "cnn1d2 = Flatten()(cnn1d2)\n",
    "#cnn1d2 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d2_input)\n",
    "c1d3_input = keras.Input(shape=(36,4))\n",
    "cnn1d3 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d3_input)\n",
    "cnn1d3 = MaxPooling1D(pool_size=8)(cnn1d3)\n",
    "cnn1d3 = Dropout(0.2)(cnn1d3)\n",
    "cnn1d3 = Flatten()(cnn1d3)\n",
    "#cnn1d3 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d3_input)\n",
    "c1d4_input = keras.Input(shape=(36,4))\n",
    "cnn1d4 = Conv1D(filters = nbfilter1, kernel_size = kernelsize, strides = 1, padding = 'valid', input_shape=(36,4), kernel_initializer = keras.initializers.lecun_uniform(seed=None))(c1d4_input)\n",
    "cnn1d4 = MaxPooling1D(pool_size=8)(cnn1d4)\n",
    "cnn1d4 = Dropout(0.2)(cnn1d4)\n",
    "cnn1d4 = Flatten()(cnn1d4)\n",
    "#cnn1d4 = get_cnn_network_seq(nbfilter1,kernel_size1)(c1d4_input)\n",
    "#training_net21.append(cnn1d1)\n",
    "#training_net21.append(cnn1d2)\n",
    "#training_net22.append(cnn1d3)\n",
    "#training_net22.append(cnn1d4)\n",
    "\n",
    "#model21 = Sequential()\n",
    "model21 = keras.layers.concatenate([cnn1d1,cnn1d2])\n",
    "model21 = Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model21)\n",
    "model21 = BatchNormalization()(model21)\n",
    "model21 = Activation('relu')(model21)\n",
    "model21 = Dropout(0.2)(model21)\n",
    "#model22 = Sequential()\n",
    "model22 = keras.layers.concatenate([cnn1d3,cnn1d4])\n",
    "model22 = Dense(int(Dense2/2),kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model22)\n",
    "model22 = BatchNormalization()(model22)\n",
    "model22 = Activation('relu')(model22)\n",
    "model22 = Dropout(0.2)(model22)\n",
    "#training_net2.append(model21)\n",
    "#training_net2.append(model22)\n",
    "#model2 = Sequential()\n",
    "model2 = keras.layers.concatenate([model21,model22])\n",
    "model2 = Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model2)\n",
    "model2 = BatchNormalization()(model2)\n",
    "model2 = Activation('relu')(model2)\n",
    "model2 = Dropout(0.2)(model2)\n",
    "#model2.add(Dense(Dense2,kernel_initializer=keras.initializers.lecun_uniform(seed=None)))\n",
    "#model2.add(BatchNormalization())\n",
    "#model2.add(Activation('relu'))\n",
    "#model2.add(Dropout(0.2))\n",
    "\n",
    "#model1 = Sequential()\n",
    "model1 = keras.layers.concatenate([cnn2d1,cnn2d2])\n",
    "model1 = Dense(Dense1,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model1)\n",
    "model1 = Dropout(0.1)(model1)\n",
    "model1 = BatchNormalization()(model1)\n",
    "model1 = Activation('relu')(model1)\n",
    "\n",
    "#training_net.append(model1)\n",
    "#training_net.append(model2)\n",
    "#model = Sequential()\n",
    "model = keras.layers.concatenate([model1,model2])\n",
    "model = Dense(Dense3,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Dense(Dense4,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "model = Activation('relu')(model)\n",
    "#model.add(Dropout(0.1))\n",
    "if Dense5>0:\n",
    "    model = Dense(Dense5,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = keras.layers.Activation('relu')(model)\n",
    "model = Dense(2,kernel_initializer=keras.initializers.lecun_uniform(seed=None))(model)\n",
    "model = BatchNormalization()(model)\n",
    "#interaction_output = Activation('softmax', axis=1)(model)\n",
    "#model.add(Activation('softmax'))\n",
    "interaction = Model(inputs=[c2d1_input,c2d2_input,c1d1_input,c1d2_input,c1d3_input,c1d4_input],outputs=[keras.layers.Activation('softmax')(model)])\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "interaction.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose = verbose1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(true_y = [],training = [], y = [], batch_size=4, epochs=12, verbose1 = 1, verbose2 = 1,\n",
    "                              validation = [], val_y = [], testing = [], matrixsize11 = 6, nbfilter11 = 24, \n",
    "                              matrixsize12 = 7, nbfilter12 = 4, matrixsize21 = 6, nbfilter21 = 24, matrixsize22 = 7,\n",
    "                              nbfilter22 = 4, nbfilter1 = 96, kernel_size1 = 7, nbfilters2 = 96,kernel_size2 = 7,\n",
    "                              Dense1 = 32, Dense2 = 192, Dense3 = 512, Dense4 = 64, Dense5 = 0):\n",
    "    \n",
    "    model = create_model2(matrixsize11, nbfilter11, matrixsize12, nbfilter12, matrixsize21, nbfilter21, \n",
    "                         matrixsize22, nbfilter22, nbfilter1, kernel_size1, nbfilters2,kernel_size2,\n",
    "                         Dense1, Dense2, Dense3, Dense4, Dense5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, filename = 'model', delete = True):\n",
    "    model.save(filename+'.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "    if delete :\n",
    "        del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_json(model, filename = \"model\"):\n",
    "    # serialize model to JSON\n",
    "    print(\"Model is saved in : \", filename + \".json\")\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    print(\"Weights are saved in : \", filename + \".h5\")\n",
    "    model.save_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_yaml(model, filename = \"model\"):\n",
    "    # serialize model to JSON\n",
    "    print(\"Model is saved in : \", filename + \".yaml\")\n",
    "    open(filename + \".yaml\", \"w\").write(model.to_yaml())\n",
    "    # serialize weights to HDF5\n",
    "    print(\"Weights are saved in : \", filename + \".h5\")\n",
    "    model.save_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_json(filename = \"model\"):\n",
    "    # load json and create model\n",
    "    print(\"Model loaded from : \", filename + \".json\")\n",
    "    model = model_from_json(open(filename + \".json\").read())\n",
    "    print(\"Weight of the model loaded from : \", filename + \".h5\")\n",
    "    model.load_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    # returns a compiled model\n",
    "    # identical to the previous one\n",
    "    model = load_model(filename+'.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_json(filename = \"model\"):\n",
    "    # load json and create model\n",
    "    print(\"Model loaded from : \", filename + \".yaml\")\n",
    "    model = model_from_yaml(open(filename + \".yaml\").read())\n",
    "    print(\"Weight of the model loaded from : \", filename + \".h5\")\n",
    "    model.load_weights(filename + \".h5\")\n",
    "    json_file.close()\n",
    "    print(\"Loaded model from disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf(seq1, seq2):\n",
    "    seq1 = traduire(seq1)\n",
    "    seq2 = traduire(seq2)\n",
    "    \n",
    "    print(seq1)\n",
    "    print(seq2)\n",
    "    \n",
    "    sequence1 = np.zeros((1,36,4))\n",
    "    sequence2 = np.zeros((1,36,4))\n",
    "    revseq1 = np.zeros((1,36,4))\n",
    "    revseq2 = np.zeros((1,36,4))\n",
    "    product = np.zeros((1,36,36,1))\n",
    "    matrice3D = np.zeros((1,36,36,1))\n",
    "    \n",
    "    z = len(seq1)\n",
    "    z = len(seq2)\n",
    "    proba1 = np.array(RNA.pfl_fold_up(seq1,1,z,z))[1:,1]\n",
    "    proba2 = np.array(RNA.pfl_fold_up(seq2,1,z,z))[1:,1]\n",
    "    product[0] = get_product_proba(proba1,proba2)\n",
    "    \n",
    "    sequence1[0] = convert_sequence(seq1)\n",
    "    sequence2[0] = convert_sequence(seq2)\n",
    "    \n",
    "    matrice3D[0] = convert_matrice3D(sequence1[0],sequence2[0])\n",
    "    \n",
    "    for j in range(36):\n",
    "        revseq1[0][36-j-1] = sequence1[0][j]\n",
    "        revseq2[0][36-j-1] = sequence2[0][j]\n",
    "    \n",
    "    test = []\n",
    "    test.append(matrice3D)\n",
    "    test.append(product)\n",
    "    test.append(sequence1)\n",
    "    test.append(sequence2)\n",
    "    test.append(revseq1)\n",
    "    test.append(revseq2)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(seq1,seq2,window = 36, filename = 'model'):\n",
    "    model = test_model([],[],[],32,16,1,1,[],[],[],Dense1=128,Dense2=512,Dense3=512,Dense4=128,Dense5=64)\n",
    "    model.load_weights(filename)\n",
    "    l1 = len(seq1)\n",
    "    l2 = len(seq2)\n",
    "    output = np.zeros((max(1,l1-window),max(1,l2-window)))\n",
    "    for i in range(max(1,l1-window)):\n",
    "        for j in range(max(1,l2-window)):\n",
    "            test = transf(seq1[i:i+35],seq2[j:j+35])\n",
    "            output[i,j] = (model.predict_proba(test)[:,1])\n",
    "            print(output[i,j])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_benchmark(model, test, filename = \"model\"):\n",
    "    print(\"Test on benchmark dataset\")\n",
    "    #model = load_model(filename)\n",
    "    print(\"Data loaded\")\n",
    "    predictions = model.predict(test)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    true_y = np.zeros((len(predictions)))\n",
    "    '''\n",
    "    for i in range(0,len(predictions),2):\n",
    "        print(i)\n",
    "        true_y[i] = 0\n",
    "    '''\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = get_output('GUGCGGCCUGAAAAACAGUGCUGUGCCCUUGUAACUCAUCAUAAUAAUUUACGGCGCAGCCAAGAUUUCCCUGGUGUUGGCGCAGUAUUCGCGCACCCCGGUCUAGCCGGGGUCAUUUUUU','UUAUGCUGGUAACGCGCUGCGCGGCUACGGUAAUCUGAUUAUCAUCAAACAUAAUGAUGAUUACCUGAGUGCCUACGCCCAUAACGACACAAUGCUGGUCCGGGAACAACAAGAAGUUAAGGCGGGGCAAAAAAUAGCGACCAUGGGUAGCACCGGAACCAGUUCAACACGCUUGCAUUUUGAAAUUCGUUACAAGGGGAAAUCCGUAAACCCGCUGCGUUAUUUGCCGCAGCGAUAAAUCGGCGGAACCAGGCUUUUGCUUGAAUGUUCCGUCAAGGGAUCACGGGUAGGAGCCACCUUAUGAGUCAGAAUACGCUGAAAGUUCAUGAUUUAAAUGAAGAUGCGGAAUUUGAUGAGAACGGAGUUGAGGUUUUUGACGAAAAGGCCUUAGUAGAACAGGAACCCAGUGAUAACGAUUUGGCCGAAGAGGAACUGUUAUCGCAGGGAGCCACACAGCGUGUGUUGGACGCGACUCAGCUUUACCUUGGUGAGAUUGGUUAUUCACCACUGUUAACGGCCGAAGAAGAAGUUUAUUUUGCGCGUCGCGCACUGCGUGGAGAUGUCGCCUCUCGCCGCCGGAUGAUCGAGAGUAACUUGCGUCUGGUGGUAAAAAUUGCCCGCCGUUAUGGCAAUCGUGGUCUGGCGUUGCUGGACCUUAUCGAAGAGGGCAACCUGGGGCUGAUCCGCGCGGUAGAGAAGUUUGACCCGGAACGUGGUUUCCGCUUCUCAACAUACGCAACCUGGUGGAUUCGCCAGACGAUUGAACGGGCGAUUAUGAACCAAACCCGUACUAUUCGUUUGCCGAUUCACAUCGUAAAGGAGCUGAACGUUUACCUGCGAACCGCACGUGAGUUGUCCCAUAAGCUGGACCAUGAACCAAGUGCGGAAGAGAUCGCAGAGCAACUGGAUAAGCCAGUUGAUGACGUCAGCCGUAUGCUUCGUCUUAACGAGCGCAUUACCUCGGUAGACACCCCGCUGGGUGGUGAUUCCGAAAAAGCGUUGCUGGACAUCCUGGCCGAUGAAAAAGAGAACGGUCCGGAAGAUACCACGCAAGAUGACGAUAUGAAGCAGAGCAUCGUCAAAUGGCUGUUCGAGCUGAACGCCAAACAGCGUGAAGUGCUGGCACGUCGAUUCGGUUUGCUGGGGUACGAAGCGGCAACACUGGAAGAUGUAGGUCGUGAAAUUGGCCUCACCCGUGAACGUGUUCGCCAGAUUCAGGUUGAAGGCCUGCGCCGUUUGCGCGAAAUCCUGCAAACGCAGGGGCUGAAUAUCGAAGCGCUGUUCCGCGAGUAA',36, \"model_double77-6\")\n",
    "#print(output)\n",
    "test = load_datatest_negatif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des poids\n",
    "interaction.load_weights('model_double_final.h5')\n",
    "test_on_benchmark(interaction,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_model(Data[0],Data[1],16,6,1,1,Data[2],Data[3],Data[4],6,24,7,6,6,24,7,6,64,7,64,7,512,128,512,128,64)\n",
    "#save_model(model, \"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
