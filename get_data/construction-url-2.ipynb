{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction de la base d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pour construire la base d'apprentissage seulement\n",
    "#from Bio import SeqIO\n",
    "#import RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformer_sequence(sequence,long):\n",
    "    nucleotides = np.zeros((4,int(max(long,25))))\n",
    "    cnt = 0\n",
    "    for lettre in sequence:\n",
    "        if lettre=='a':\n",
    "            nucleotides[0,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        elif lettre=='c':\n",
    "            nucleotides[1,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        elif lettre=='g':\n",
    "            nucleotides[2,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        else:\n",
    "            nucleotides[3,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformer_structure(structure):\n",
    "    long = len(structure)\n",
    "    a = np.zeros(25)\n",
    "    a[:long] = structure\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traduire(seq,strand):\n",
    "    nucleotides = ''\n",
    "    if strand:\n",
    "        for lettre in seq:\n",
    "            if lettre=='A':\n",
    "                nucleotides=nucleotides+'a'\n",
    "            elif lettre=='C':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            elif lettre=='G':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'u'    \n",
    "    else:\n",
    "        string = ''\n",
    "        for i in seq:\n",
    "            string = i + string\n",
    "        seq = string\n",
    "        for lettre in seq:\n",
    "            if lettre=='A':\n",
    "                nucleotides=nucleotides+'u'\n",
    "            elif lettre=='C':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            elif lettre=='G':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'a'\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des bdd (ne pas exécuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = SeqIO.parse(open(\"genomes/GRCh38.genome.fa\"),\"fasta\")\n",
    "dict_fasta = dict([(seq.id, seq) for seq in parser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11473\n"
     ]
    }
   ],
   "source": [
    "# base de données rise_human_targeted\n",
    "bdd = pd.read_csv(\"rise_human_targeted.csv\", sep = \"\\t\",header=None)\n",
    "arrbdd = np.array(bdd)\n",
    "\n",
    "# Ne garder que les interactions miRNA - mRNA\n",
    "arg_mi = np.argwhere(arrbdd[:,14]=='miRNA')[:,0]\n",
    "arg_m = np.argwhere(arrbdd[:,15]=='protein_coding')[:,0]\n",
    "args = [k for k in arg_mi if k in arg_m]\n",
    "args = np.array(args)\n",
    "arrbdd = arrbdd[args]\n",
    "\n",
    "#enlever les chromosomes M\n",
    "arg_sansM = np.argwhere((arrbdd[:,0] != 'chrM') & (arrbdd[:,3] != 'chrM'))\n",
    "arrbdd = arrbdd[arg_sansM[:,0]]\n",
    "#enlever les champs vides\n",
    "arg_sansM = np.argwhere((arrbdd[:,1] != '.') & (arrbdd[:,2] != '.') & (arrbdd[:,4] != '.') & (arrbdd[:,5] != '.') & (arrbdd[:,0] != '.') & (arrbdd[:,3] != '.') & (arrbdd[:,8] != '.') & (arrbdd[:,9] != '.'))\n",
    "print(len(arg_sansM))\n",
    "varrbdd = arrbdd[arg_sansM[:,0]]\n",
    "\n",
    "\n",
    "#changement strand\n",
    "for i,el in enumerate(varrbdd[:,8]):\n",
    "    if varrbdd[i,8] == '+':\n",
    "        varrbdd[i,8] = True\n",
    "    else:\n",
    "        varrbdd[i,8] = False\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,9]):\n",
    "    if varrbdd[i,9] == '+':\n",
    "        varrbdd[i,9] = True\n",
    "    else:\n",
    "        varrbdd[i,9] = False\n",
    "\n",
    "#génération position début et fin de séquence (nucléotides)\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,1]):\n",
    "    varrbdd[i,1] = int(varrbdd[i,1])-1\n",
    "    varrbdd[i,2] = int(varrbdd[i,2])\n",
    "    milieu = int((int(varrbdd[i,4])+int(varrbdd[i,5]))/2)\n",
    "    varrbdd[i,4] = milieu-50-1\n",
    "    varrbdd[i,5] = milieu+50\n",
    "bdd = varrbdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = csv.writer(open(\"genomes/positifs.csv\", 'w',newline=''),delimiter=\"\\t\")\n",
    "    \n",
    "for i in range(len(bdd)):\n",
    "    pos = bdd[i,:4]\n",
    "    ide, begin, end = [bdd[i,0],bdd[i,1],bdd[i,2]]\n",
    "    pos[0] = traduire(dict_fasta[ide][begin:end].format(\"fasta\").split('\\n')[1],bdd[i,8])\n",
    "    ide, begin, end = [bdd[i,3],bdd[i,4],bdd[i,5]]\n",
    "    seq = dict_fasta[ide][begin:end].format(\"fasta\").split('\\n')\n",
    "    pos[1] = traduire(seq[1]+seq[2],bdd[i,9])\n",
    "    w = len(pos[0])\n",
    "    pos[2] = np.array(RNA.pfl_fold_up(pos[0],1,w,w))[1:,1]\n",
    "    pos[3] = np.array(RNA.pfl_fold_up(pos[1],1,101,101))[1:,1]\n",
    "    writer.writerow(pos)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10024\n"
     ]
    }
   ],
   "source": [
    "# base de données rise_human_targeted\n",
    "bdd = pd.read_csv(\"rise_human_targeted.csv\", sep = \"\\t\",header=None)\n",
    "arrbdd = np.array(bdd)[:16650]\n",
    "\n",
    "# Ne garder que les interactions miRNA - mRNA\n",
    "arg_mi = np.argwhere(arrbdd[:,14]=='miRNA')[:,0]\n",
    "arg_m = np.argwhere(arrbdd[:,15]=='protein_coding')[:,0]\n",
    "args = [k for k in arg_mi if k in arg_m]\n",
    "args = np.array(args)\n",
    "arrbdd = arrbdd[args]\n",
    "\n",
    "#enlever les chromosomes M\n",
    "arg_sansM = np.argwhere((arrbdd[:,0] != 'chrM') & (arrbdd[:,3] != 'chrM'))\n",
    "arrbdd = arrbdd[arg_sansM[:,0]]\n",
    "#enlever les champs vides\n",
    "arg_sansM = np.argwhere((arrbdd[:,1] != '.') & (arrbdd[:,2] != '.') & (arrbdd[:,4] != '.') & (arrbdd[:,5] != '.') & (arrbdd[:,0] != '.') & (arrbdd[:,3] != '.') & (arrbdd[:,8] != '.') & (arrbdd[:,9] != '.'))\n",
    "print(len(arg_sansM))\n",
    "varrbdd = arrbdd[arg_sansM[:,0]]\n",
    "\n",
    "\n",
    "#changement strand\n",
    "for i,el in enumerate(varrbdd[:,8]):\n",
    "    if varrbdd[i,8] == '+':\n",
    "        varrbdd[i,8] = True\n",
    "    else:\n",
    "        varrbdd[i,8] = False\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,9]):\n",
    "    if varrbdd[i,9] == '+':\n",
    "        varrbdd[i,9] = True\n",
    "    else:\n",
    "        varrbdd[i,9] = False\n",
    "\n",
    "#génération position début et fin de séquence (nucléotides)\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,1]):\n",
    "    varrbdd[i,1] = int(varrbdd[i,1])-1\n",
    "    varrbdd[i,2] = int(varrbdd[i,2])\n",
    "    milieu = int((int(varrbdd[i,4])+int(varrbdd[i,5]))/2)\n",
    "    varrbdd[i,4] = milieu-50-1\n",
    "    varrbdd[i,5] = milieu+50\n",
    "bdd = varrbdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = csv.writer(open(\"genomes/negatifs.csv\", 'w',newline=''),delimiter=\"\\t\")\n",
    "\n",
    "for i in range(len(bdd)): \n",
    "    neg = bdd[i,:4]\n",
    "    ide, begin, end = [bdd[i,0],bdd[i,1],bdd[i,2]]\n",
    "    neg[0] = traduire(dict_fasta[ide][begin:end].format(\"fasta\").split('\\n')[1],bdd[i,8])\n",
    "    ide, begin, end = bdd[i,3], bdd[i,4], bdd[i,5]\n",
    "    begin = begin+80\n",
    "    end = end+80\n",
    "    sequ = dict_fasta[ide][begin:end].format(\"fasta\").split('\\n')\n",
    "    neg[1] = traduire(sequ[1]+sequ[2],bdd[i,9])\n",
    "    w = len(neg[0])\n",
    "    neg[2] = np.array(RNA.pfl_fold_up(neg[0],1,w,w))[1:,1]\n",
    "    neg[3] = np.array(RNA.pfl_fold_up(neg[1],1,101,101))[1:,1]\n",
    "    writer.writerow(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération et vérification des bases de données (exécuter à partir d'ici pour l'apprentissage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau de neurones basé sur iDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/DATA/anaconda3/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import load_model\n",
    "#from keras.optimizers import kl_divergence\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import gzip\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse\n",
    "\n",
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_performance(test_num, pred_y,  labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+ fp)\n",
    "    sensitivity = float(tp)/ (tp+fn)\n",
    "    specificity = float(tn)/(tn + fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(seq):\n",
    "    seq = seq.split('\\n')\n",
    "    seq2 = ''\n",
    "    for j in seq:\n",
    "        seq2 = seq2 + j\n",
    "    seq2 = seq2[2:len(seq2)-1]\n",
    "    seq2 = seq2.split(' ')\n",
    "    #print(seq2)\n",
    "    seq3=[]\n",
    "    for j in seq2:\n",
    "        #print(j)\n",
    "        if j=='':\n",
    "            a=0\n",
    "        else:\n",
    "            seq3.append(float(j))\n",
    "    return seq3\n",
    "\n",
    "verif = pd.read_csv(\"genomes/negatifs.csv\", sep = \"\\t\",header=None)\n",
    "verif = np.array(verif)\n",
    "for i in range(len(verif)):\n",
    "    for j in range(4):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "for i in range(len(verif)):\n",
    "    verif[i,2] = np.array(clean(verif[i,2]))\n",
    "    verif[i,3] = np.array(clean(verif[i,3]))    \n",
    "\n",
    "neg = verif\n",
    "\n",
    "verif = pd.read_csv(\"genomes/positifs.csv\", sep = \"\\t\",header=None)  \n",
    "verif = np.array(verif)\n",
    "for i in range(len(verif)):\n",
    "    for j in range(4):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "for i in range(len(verif)):\n",
    "    verif[i,2] = np.array(clean(verif[i,2]))\n",
    "    verif[i,3] = np.array(clean(verif[i,3]))\n",
    "\n",
    "pos = verif\n",
    "verif = []\n",
    "bdd = np.concatenate((pos,neg))\n",
    "pos = []\n",
    "neg = []\n",
    "labels = np.zeros((len(bdd),1))\n",
    "\n",
    "bdd = np.concatenate((bdd,labels),axis=1)\n",
    "for i in range(int(len(bdd)/2)):\n",
    "    bdd[i,4]=1    \n",
    "labels=[]\n",
    "for i in range(len(bdd)):\n",
    "    bdd[i,0] = transformer_sequence(bdd[i,0],len(bdd[i,0]))\n",
    "    bdd[i,1] = transformer_sequence(bdd[i,1],101)\n",
    "    bdd[i,2] = transformer_structure(bdd[i,2])\n",
    "\n",
    "# shuffle pour mélanger positifs et négatifs\n",
    "\n",
    "indices = np.zeros(len(bdd),dtype=int)\n",
    "for i in range(int(len(bdd)/2)):\n",
    "    indices[2*i] = int(i)\n",
    "    indices[2*i+1] = int(i + int(len(bdd)/2))\n",
    "bdd = bdd[indices]\n",
    "indices = []\n",
    "\n",
    "# shuffle total\n",
    "\n",
    "indices = np.arange(len(bdd))\n",
    "shuffle(indices)\n",
    "bdd = bdd[indices]\n",
    "\n",
    "for i in range(len(bdd)):\n",
    "    bdd[i,0] = bdd[i,0].transpose()\n",
    "    bdd[i,1] = bdd[i,1].transpose()\n",
    "\n",
    "resh0 = np.zeros((20048,25,4))\n",
    "\n",
    "for i in range(len(bdd)):\n",
    "    resh0[i,:,:] = bdd[i,0]\n",
    "    \n",
    "resh1 = np.zeros((20048,101,4))\n",
    "\n",
    "for i in range(len(bdd)):\n",
    "    resh1[i,:,:] = bdd[i,1]\n",
    "\n",
    "resh2 = np.zeros((20048,25))\n",
    "\n",
    "for i in range(len(bdd)):\n",
    "    resh2[i,:] = bdd[i,2]\n",
    "    \n",
    "resh3 = np.zeros((20048,101))\n",
    "\n",
    "for i in range(len(bdd)):\n",
    "    resh3[i,:] = bdd[i,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train = 17000\n",
    "nb_val = 500\n",
    "nb_test = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resh0_train = resh0[:nb_train]\n",
    "resh0_val = resh0[nb_train:nb_train+nb_val]\n",
    "resh1_train = resh1[:nb_train]\n",
    "resh1_val = resh1[nb_train:nb_train+nb_val]\n",
    "resh2_train = resh2[:nb_train]\n",
    "resh2_val = resh2[nb_train:nb_train+nb_val]\n",
    "resh3_train = resh3[:nb_train]\n",
    "resh3_val = resh3[nb_train:nb_train+nb_val]\n",
    "resh0_test = resh0[-nb_test:]\n",
    "resh1_test = resh1[-nb_test:]\n",
    "resh2_test = resh2[-nb_test:]\n",
    "resh3_test = resh3[-nb_test:]\n",
    "resh0=[]\n",
    "resh1=[]\n",
    "resh2=[]\n",
    "resh3=[]\n",
    "train = bdd[:nb_train]\n",
    "valid = bdd[nb_train:nb_train+nb_val]\n",
    "y = train[:,4]\n",
    "y = keras.utils.np_utils.to_categorical(y,2)\n",
    "val_y = valid[:,4]\n",
    "val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "train=[]\n",
    "valid=[]\n",
    "test = bdd[-nb_test:]\n",
    "bdd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_networks_train_predict():\n",
    "    \n",
    "    print('training', nb_train)\n",
    "    \n",
    "    micro_structure_hid = 64\n",
    "    messenger_structure_hid = 128\n",
    "    micro_seq_hid = 22\n",
    "    messenger_seq_hid = 100\n",
    "    \n",
    "    micro_seq_train = resh0_train\n",
    "    micro_seq_validation = resh0_val\n",
    "    micro_seq_net =  get_cnn_network_microRNA()\n",
    "    \n",
    "    messenger_seq_train = resh1_train\n",
    "    messenger_seq_validation = resh1_val\n",
    "    messenger_seq_net = get_cnn_network_messengerRNA()\n",
    "    \n",
    "    micro_structure_train = resh2_train\n",
    "    micro_structure_validation = resh2_val\n",
    "    micro_structure_net = get_mlp_microRNA()\n",
    "    \n",
    "    messenger_structure_train = resh3_train\n",
    "    messenger_structure_validation = resh3_val\n",
    "    messenger_structure_net = get_mlp_messengerRNA()        \n",
    "    \n",
    "    #y, encoder = preprocess_labels(training_label)\n",
    "    #val_y, encoder = preprocess_labels(validation_label, encoder = encoder)\n",
    "       \n",
    "    \n",
    "    #model = Sequential()\n",
    "    training_net=[]\n",
    "    training = []\n",
    "    validation = []\n",
    "    total_hid = 0\n",
    "    \n",
    "    training_net.append(micro_seq_net)\n",
    "    training.append(micro_seq_train)\n",
    "    validation.append(micro_seq_validation)\n",
    "    total_hid = total_hid + micro_seq_hid\n",
    "    micro_seq_train = []\n",
    "    micro_seq_validation = [] \n",
    "    \n",
    "    training_net.append(messenger_seq_net)\n",
    "    training.append(messenger_seq_train)\n",
    "    validation.append(messenger_seq_validation)\n",
    "    total_hid = total_hid + messenger_seq_hid\n",
    "    messenger_seq_train = []\n",
    "    messenger_seq_validation = []\n",
    "    \n",
    "    training_net.append(micro_structure_net)\n",
    "    training.append(micro_structure_train)\n",
    "    validation.append(micro_structure_validation)\n",
    "    total_hid = total_hid + micro_structure_hid\n",
    "    micro_structure_train = []\n",
    "    micro_structure_validation = []\n",
    "    \n",
    "    training_net.append(messenger_structure_net)\n",
    "    training.append(messenger_structure_train)\n",
    "    validation.append(messenger_structure_validation)\n",
    "    total_hid = total_hid + messenger_structure_hid\n",
    "    messenger_structure_train = []\n",
    "    messenger_structure_validation = []\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    " \n",
    "    #model.add(Dense(total_hid, input_shape=(total_hid,)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    #model.add(BatchNormalization(mode=2))\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    print(total_hid)\n",
    "    model.add(Dense(2, input_shape=(total_hid,)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "    #checkpointer = ModelCheckpoint(filepath=\"models/bestmodel.hdf5\", verbose=0, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "    #validation_data=(np.transpose(validmat['validxdata'],axes=(0,2,1)), validmat['validdata']), callbacks=[checkpointer,earlystopper]\n",
    "    print('model training')\n",
    "    model.fit(training, y, batch_size=10000, epochs=1000, verbose=0, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "    \n",
    "    training = []\n",
    "    validation = []\n",
    "    \n",
    "    # test\n",
    "    true_y = test[:,4]\n",
    "    \n",
    "    print('predicting')\n",
    "    testing = []\n",
    "    testing.append(resh0_test)\n",
    "    testing.append(resh1_test)\n",
    "    testing.append(resh2_test)\n",
    "    testing.append(resh3_test)\n",
    "        \n",
    "    predictions = model.predict_proba(testing)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 17000\n",
      "configure cnn network\n",
      "configure cnn network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/DATA/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314\n",
      "model training\n",
      "predicting\n",
      "[0.44264212 0.5621759  0.5483066  ... 0.6002486  0.54963225 0.56503737]\n",
      "[0. 1. 1. ... 1. 1. 1.] [0.0 0.0 1 ... 1 1 1]\n",
      "acc :  0.5564\n",
      "precision :  0.5346628679962013\n",
      "sensitivity :  0.8972111553784861\n",
      "specificity :  0.21285140562248997\n",
      "MCC :  0.15103195995226454\n"
     ]
    }
   ],
   "source": [
    "m = merge_networks_train_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 100 trains : \n",
    "acc :  0.5\n",
    "precision :  0.5333333333333333\n",
    "sensitivity :  0.7272727272727273\n",
    "specificity :  0.2222222222222222\n",
    "MCC :  -0.058025885318565944\n",
    "\n",
    "##### 17000 trains\n",
    "acc :  0.5964\n",
    "precision :  0.6069017254313578\n",
    "sensitivity :  0.625193199381762\n",
    "specificity :  0.5655058043117744\n",
    "MCC :  0.19100235122777048\n",
    "##### \n",
    "acc :  0.5564\n",
    "precision :  0.5346628679962013\n",
    "sensitivity :  0.8972111553784861\n",
    "specificity :  0.21285140562248997\n",
    "MCC :  0.15103195995226454\n",
    "(10000 batch et 1000 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_network_microRNA():    \n",
    "    print('configure cnn network')\n",
    "    nbfilter = 22\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(activation=\"relu\", input_shape=(25, 4), filters=22, kernel_size=7, strides=1, padding=\"valid\"))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(nbfilter, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    #model.add(BatchNormalization(mode=2))\n",
    "    #model.add(Dense(64))\n",
    " \n",
    "    #model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_network_messengerRNA():    \n",
    "    print('configure cnn network')\n",
    "    nbfilter = 100\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(activation=\"relu\", input_shape=(101, 4), filters=100, kernel_size=7, strides=1, padding=\"valid\"))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    #model.add(Dense(nbfilter, activation='relu'))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    #model.add(BatchNormalization(mode=2))\n",
    "    #model.add(Dense(64))    \n",
    "    \n",
    "    #model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn2D():\n",
    "    nb_conv = 4\n",
    "    nb_pool = 2\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (nb_conv, nb_conv), padding='valid', input_shape=(1, 101,4),strides=(1,1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mlp_microRNA(num_hidden = 64):\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(num_hidden, input_dim=train.shape[1], activation='relu'))\n",
    "    model.add(Dense(num_hidden, input_shape=(25,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_hidden, input_dim=num_hidden, activation='relu'))\n",
    "    #model.add(Dense(num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    model.add(Dense(sec_num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mlp_messengerRNA(num_hidden = 128):\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(num_hidden, input_dim=train.shape[1], activation='relu'))\n",
    "    model.add(Dense(num_hidden, input_shape=(101,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_hidden, input_dim=num_hidden, activation='relu'))\n",
    "    #model.add(Dense(num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    model.add(Dense(sec_num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération rapide des séquences à partir de chr_nb start end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download genome at ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_human/release_24/GRCh38.primary_assembly.genome.fa.gz\n",
    "\n",
    "parser = SeqIO.parse(open(\"genomes/GRCh38.genome.fa\"),\"fasta\")\n",
    "\n",
    "dict_fasta = dict([(seq.id, seq) for seq in parser])\n",
    "\n",
    "ide, begin, end = ['chr17',8222876,8222895]\n",
    "\n",
    "dict_fasta[ide][begin:end].format(\"fasta\").split('\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir de là c'est l'ancienne méthode (à conserver au cas où, archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On  sauvegarde la base de données dans un fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "import csv\n",
    "\n",
    "writer = csv.writer(open(\"genomes/database.csv\", 'w',newline=''),delimiter=\"\\t\")\n",
    "for i,el in enumerate(varrbdd):\n",
    "    row = transformer_database2(varrbdd[i])    \n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche les trous dans les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_5000_trouee = pd.read_csv(\"genomes/database.csv\", sep = \"\\t\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4983"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdd_5000_trouee = np.array(bdd_5000_trouee)\n",
    "l = []\n",
    "for i,el in enumerate(bdd_5000_trouee):\n",
    "    if type(bdd_5000_trouee[i,0])==float or type(bdd_5000_trouee[i,1])==float:\n",
    "        a=0\n",
    "    elif len(bdd_5000_trouee[i,0])==0 or len(bdd_5000_trouee[i,0])==0:\n",
    "        a=0\n",
    "    else:\n",
    "        l.append(i)\n",
    "l = np.array(l)\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_miRNA_mRNA = bdd_5000_trouee[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_miRNA_mRNA = nettoyer(bdd_miRNA_mRNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_miRNA_mRNA = traduire_séquences(bdd_miRNA_mRNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdd_apprentissage = construire_seq_et_structure_bdd(bdd_miRNA_mRNA[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir de mainenant, on essaye d'obtenir les probabilités d'accesibilité de chaque nucléotide sur une séquence quelconque ARN par exemple 'ACGUUGUCACACGAU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traduire_test(seq,strand):\n",
    "    nucleotides = ''\n",
    "    if strand:\n",
    "        string = ''\n",
    "        for i in seq:\n",
    "            string = i + string\n",
    "        seq = string\n",
    "        for lettre in seq:\n",
    "            if lettre=='a':\n",
    "                nucleotides=nucleotides+'u'\n",
    "            elif lettre=='c':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            elif lettre=='g':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'a'\n",
    "    else:\n",
    "        for lettre in seq:\n",
    "            if lettre=='a':\n",
    "                nucleotides=nucleotides+'a'\n",
    "            elif lettre=='c':\n",
    "                nucleotides=nucleotides+'c'\n",
    "            elif lettre=='g':\n",
    "                nucleotides=nucleotides+'g'\n",
    "            else:\n",
    "                nucleotides=nucleotides+'u'    \n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cacuacagcaagccuggcacgaccucuaaggcgguucgcggcaacguccgcacgucggcucguuggucuagggguaugauucucgcuucgggugcgagaggucccggguucaaaucccggacgagcccuccuuuaccuuuuacugagacaagagugucuuc'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traduire_test('gaagacactcttgtctcagtaaaaggtaaaggagggctcgtccgggatttgaacccgggacctctcgcacccgaagcgagaatcatacccctagaccaacgagccgacgtgcggacgttgccgcgaaccgccttagaggtcgtgccaggcttgctgtagtg',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'gaagacactcttgtctcagtaaaaggtaaaggagggctcgtccgggatttgaacccgggacctctcgcacccgaagcgagaatcatacccctagaccaacgagccgacgtgcggacgttgccgcgaaccgccttagaggtcgtgccaggcttgctgtagtg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/DATA/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (1,2,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "bdd = pd.read_csv(\"rise_human_transcriptome (copie).csv\", sep = \"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bdd convertie panda -> numpy array et seuls les 70 000 premiers RRIs retenus\n",
    "arrbdd = np.array(bdd)[:70000]\n",
    "\n",
    "#enlever 'chr' et formater les numéros de chromosomes\n",
    "for i,el in enumerate(arrbdd[:,0]):\n",
    "    arrbdd[i,0] = arrbdd[i,0][3:]\n",
    "\n",
    "for i,el in enumerate(arrbdd[:,0]):\n",
    "    if arrbdd[i,0] == 'X':\n",
    "        arrbdd[i,0] = '23'\n",
    "    if arrbdd[i,0] == 'Y':\n",
    "        arrbdd[i,0] = '24'\n",
    "    if arrbdd[i,0] == '1':\n",
    "        arrbdd[i,0] = '01'\n",
    "    if arrbdd[i,0] == '2':\n",
    "        arrbdd[i,0] = '02'\n",
    "    if arrbdd[i,0] == '3':\n",
    "        arrbdd[i,0] = '03'\n",
    "    if arrbdd[i,0] == '4':\n",
    "        arrbdd[i,0] = '04'\n",
    "    if arrbdd[i,0] == '5':\n",
    "        arrbdd[i,0] = '05'\n",
    "    if arrbdd[i,0] == '6':\n",
    "        arrbdd[i,0] = '06'\n",
    "    if arrbdd[i,0] == '7':\n",
    "        arrbdd[i,0] = '07'\n",
    "    if arrbdd[i,0] == '8':\n",
    "        arrbdd[i,0] = '08'\n",
    "    if arrbdd[i,0] == '9':\n",
    "        arrbdd[i,0] = '09'\n",
    "\n",
    "for i,el in enumerate(arrbdd[:,3]):\n",
    "    arrbdd[i,3] = arrbdd[i,3][3:]\n",
    "\n",
    "for i,el in enumerate(arrbdd[:,3]):\n",
    "    if arrbdd[i,3] == 'X':\n",
    "        arrbdd[i,3] = '23'\n",
    "    elif arrbdd[i,3] == 'Y':\n",
    "        arrbdd[i,3] = '24'  \n",
    "    elif arrbdd[i,3] == '1':\n",
    "        arrbdd[i,3] = '01'\n",
    "    elif arrbdd[i,3] == '2':\n",
    "        arrbdd[i,3] = '02'\n",
    "    elif arrbdd[i,3] == '3':\n",
    "        arrbdd[i,3] = '03'\n",
    "    elif arrbdd[i,3] == '4':\n",
    "        arrbdd[i,3] = '04'\n",
    "    elif arrbdd[i,3] == '5':\n",
    "        arrbdd[i,3] = '05'\n",
    "    elif arrbdd[i,3] == '6':\n",
    "        arrbdd[i,3] = '06'\n",
    "    elif arrbdd[i,3] == '7':\n",
    "        arrbdd[i,3] = '07'\n",
    "    elif arrbdd[i,3] == '8':\n",
    "        arrbdd[i,3] = '08'\n",
    "    elif arrbdd[i,3] == '9':\n",
    "        arrbdd[i,3] = '09'\n",
    "\n",
    "\n",
    "#enlever les chromosomes M\n",
    "arg_sansM = np.argwhere((arrbdd[:,0] != 'M') & (arrbdd[:,3] != 'M'))\n",
    "varrbdd = arrbdd[arg_sansM[:,0]]\n",
    "\n",
    "#changement strand\n",
    "for i,el in enumerate(varrbdd[:,8]):\n",
    "    if varrbdd[i,8] == '+':\n",
    "        varrbdd[i,8] = 'true'\n",
    "    else:\n",
    "        varrbdd[i,8] = 'false'\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,9]):\n",
    "    if varrbdd[i,9] == '+':\n",
    "        varrbdd[i,9] = 'true'\n",
    "    else:\n",
    "        varrbdd[i,9] = 'false'\n",
    "\n",
    "\n",
    "#int str\n",
    "for i,el in enumerate(varrbdd[:,1]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,1] = str(el)\n",
    "for i,el in enumerate(varrbdd[:,2]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,2] = str(el)\n",
    "for i,el in enumerate(varrbdd[:,4]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,4] = str(el)\n",
    "for i,el in enumerate(varrbdd[:,5]):\n",
    "    if type(el) == int:\n",
    "        varrbdd[i,5] = str(el)\n",
    "\n",
    "#génération position début et fin de séquence (nucléotides)\n",
    "\n",
    "for i,el in enumerate(varrbdd[:,1]):\n",
    "    milieu = int((int(varrbdd[i,1])+int(varrbdd[i,2]))/2)\n",
    "    varrbdd[i,1] = str(milieu-50)\n",
    "    varrbdd[i,2] = str(milieu+50)\n",
    "    milieu = int((int(varrbdd[i,4])+int(varrbdd[i,5]))/2)\n",
    "    varrbdd[i,4] = str(milieu-50)\n",
    "    varrbdd[i,5] = str(milieu+50)\n",
    "\n",
    "# Génération URLs\n",
    "\n",
    "#for i,el in enumerate(varrbdd[:,6]):\n",
    " #   varrbdd[i,6] = \"https://www.ncbi.nlm.nih.gov/nuccore/NC_0000\"+varrbdd[i,0]+\"?report=genbank&from=\"+varrbdd[i,1]+\"&to=\"+varrbdd[i,2]+\"&strand=\"+varrbdd[i,8]+\".html\"\n",
    "#for i,el in enumerate(varrbdd[:,6]):\n",
    " #   varrbdd[i,6] = 'https://www.ncbi.nlm.nih.gov/nuccore/NC_0000'+varrbdd[i,0]+'?report=fasta&log$=seqview&format=text&from='+varrbdd[i,1]+'&to='+varrbdd[i,2]\n",
    "\n",
    "#for i,el in enumerate(varrbdd[:,7]):\n",
    " #   varrbdd[i,7] = \"https://www.ncbi.nlm.nih.gov/nuccore/NC_0000\"+varrbdd[i,3]+\"?report=genbank&from=\"+varrbdd[i,4]+\"&to=\"+varrbdd[i,5]+\"&strand=\"+varrbdd[i,9]+\".html\"\n",
    "#for i,el in enumerate(varrbdd[:,7]):\n",
    " #   varrbdd[i,7] = 'https://www.ncbi.nlm.nih.gov/nuccore/NC_0000'+varrbdd[i,3]+'?report=fasta&log$=seqview&format=text&from='+varrbdd[i,4]+'&to='+varrbdd[i,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recuperer_sequence(url):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    time.sleep(4)\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    browser.quit()\n",
    "    soup = BeautifulSoup(innerHTML,\"lxml\")\n",
    "    seq = soup.find_all(\"span\",{'class':\"ff_line\"})\n",
    "    rep = seq[0].text+seq[1].text+seq[2].text+seq[3].text\n",
    "    rep = ''.join(rep.split())\n",
    "    return rep\n",
    "\n",
    "def recuperer_sequence2(url):\n",
    "    browser.get(url)\n",
    "    time.sleep(1.3)\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,\"lxml\")\n",
    "    seq = soup.find_all(\"div\",{'class':\"seq gbff\"})\n",
    "    rep = seq[0].text   \n",
    "    return rep\n",
    "\n",
    "def transformer_database(bddrow):\n",
    "    sequence=recuperer_sequence(bddrow[6])\n",
    "    sequence=transformer_sequence(sequence,len(sequence))\n",
    "    sequencebis=recuperer_sequence(bddrow[7])\n",
    "    sequencebis=transformer_sequence(sequencebis,len(sequencebis))    \n",
    "    bddrow = np.array([bddrow[18],bddrow[19],bddrow[20],bddrow[21],sequence,bddrow[0],bddrow[1],bddrow[2],bddrow[8],bddrow[10],bddrow[11],bddrow[14],bddrow[16],sequencebis,bddrow[3],bddrow[4],bddrow[5],bddrow[9],bddrow[12],bddrow[13],bddrow[15],bddrow[17]], dtype=object)\n",
    "    return bddrow\n",
    "\n",
    "def transformer_database2(bddrow):\n",
    "    sequence=recuperer_sequence2(bddrow[6])\n",
    "    sequencebis=recuperer_sequence2(bddrow[7])\n",
    "    bddrow = np.array([sequence,sequencebis,bddrow[8],bddrow[9]], dtype=object)\n",
    "    return bddrow\n",
    "\n",
    "def construire_seq_et_structure_bdd(bdd):\n",
    "    for i,el in enumerate(bdd):\n",
    "        bdd[i,2] = np.array(RNA.pfl_fold_up(bdd[i,0],1,201,201))[1:,1]\n",
    "        bdd[i,3] = np.array(RNA.pfl_fold_up(bdd[i,1],1,201,201))[1:,1]\n",
    "        #bdd[i,0] = transformer_sequence(bdd[i,0],201)\n",
    "        #bdd[i,1] = transformer_sequence(bdd[i,1],201)\n",
    "    return bdd\n",
    "\n",
    "def nettoyer(bdd):\n",
    "    for i,el in enumerate(bdd):\n",
    "        bdd[i,0]=bdd[i,0].split('\\n')[1]+bdd[i,0].split('\\n')[2]+bdd[i,0].split('\\n')[3]\n",
    "        bdd[i,1]=bdd[i,1].split('\\n')[1]+bdd[i,1].split('\\n')[2]+bdd[i,1].split('\\n')[3]\n",
    "    return bdd\n",
    "\n",
    "def traduire_séquences(bdd):\n",
    "    for i,el in enumerate(bdd):\n",
    "        bdd[i,0]=traduire(bdd[i,0],bdd[i,2])\n",
    "        bdd[i,1]=traduire(bdd[i,1],bdd[i,3])\n",
    "    return bdd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
