{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau de neurones basé sur iDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D,Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_config\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Merge\n",
    "#from keras.optimizers import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, grid_search\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_curve\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib \n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gzip\n",
    "import pdb\n",
    "from math import  sqrt\n",
    "#import theano\n",
    "import subprocess as sp\n",
    "import scipy.stats as stats\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_structure(structure):\n",
    "    long = len(structure)\n",
    "    a = np.zeros(25)\n",
    "    a[:long] = structure\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_sequence(sequence,long):\n",
    "    nucleotides = np.zeros((4,int(max(long,25))))\n",
    "    cnt = 0\n",
    "    for lettre in sequence:\n",
    "        if lettre=='a':\n",
    "            nucleotides[0,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        elif lettre=='c':\n",
    "            nucleotides[1,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        elif lettre=='g':\n",
    "            nucleotides[2,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "        else:\n",
    "            nucleotides[3,cnt] = 1\n",
    "            cnt = cnt+1\n",
    "    return nucleotides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network_microRNA(nbfilter = 22):    \n",
    "    print('configure cnn network')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(activation=\"relu\", input_shape=(25, 4), filters=nbfilter, kernel_size=7, strides=1, padding=\"valid\"))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(nbfilter, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    #model.add(BatchNormalization(mode=2))\n",
    "    #model.add(Dense(64))\n",
    " \n",
    "    #model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_network_messengerRNA(nbfilter = 100):    \n",
    "    print('configure cnn network')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(activation=\"relu\", input_shape=(101, 4), filters=nbfilter, kernel_size=7, strides=1, padding=\"valid\"))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    #model.add(Dense(nbfilter, activation='relu'))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    #model.add(BatchNormalization(mode=2))\n",
    "    #model.add(Dense(64))    \n",
    "    \n",
    "    #model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn2D():\n",
    "    nb_conv = 4\n",
    "    nb_pool = 2\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (nb_conv, nb_conv), padding='valid', input_shape=(1, 101,4),strides=(1,1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_microRNA(num_hidden = 64):\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(num_hidden, input_dim=train.shape[1], activation='relu'))\n",
    "    model.add(Dense(num_hidden, input_shape=(25,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_hidden, input_dim=num_hidden, activation='relu'))\n",
    "    #model.add(Dense(num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    model.add(Dense(sec_num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_messengerRNA(num_hidden = 128):\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Dense(num_hidden, input_dim=train.shape[1], activation='relu'))\n",
    "    model.add(Dense(num_hidden, input_shape=(101,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_hidden, input_dim=num_hidden, activation='relu'))\n",
    "    #model.add(Dense(num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    model.add(Dense(sec_num_hidden, input_shape=(num_hidden,), activation='relu'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    '''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(test_num, pred_y,  labels):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for index in range(test_num):\n",
    "        if labels[index] ==1:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tp = tp +1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            if labels[index] == pred_y[index]:\n",
    "                tn = tn +1\n",
    "            else:\n",
    "                fp = fp + 1               \n",
    "            \n",
    "    acc = float(tp + tn)/test_num\n",
    "    precision = float(tp)/(tp+ fp)\n",
    "    sensitivity = float(tp)/ (tp+fn)\n",
    "    specificity = float(tn)/(tn + fp)\n",
    "    MCC = float(tp*tn-fp*fn)/(np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)))\n",
    "    return acc, precision, sensitivity, specificity, MCC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(seq):\n",
    "    seq = seq.split('\\n')\n",
    "    seq2 = ''\n",
    "    for j in seq:\n",
    "        seq2 = seq2 + j\n",
    "    seq2 = seq2[2:len(seq2)-1]\n",
    "    seq2 = seq2.split(' ')\n",
    "    #print(seq2)\n",
    "    seq3=[]\n",
    "    for j in seq2:\n",
    "        #print(j)\n",
    "        if j=='':\n",
    "            a=0\n",
    "        else:\n",
    "            seq3.append(float(j))\n",
    "    return seq3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif = pd.read_csv(\"negatifs.csv\", sep = \"\\t\",header=None)\n",
    "verif = np.array(verif)\n",
    "for i in range(len(verif)):\n",
    "    for j in range(4):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "for i in range(len(verif)):\n",
    "    verif[i,2] = np.array(clean(verif[i,2]))\n",
    "    verif[i,3] = np.array(clean(verif[i,3]))    \n",
    "\n",
    "neg = verif\n",
    "\n",
    "verif = pd.read_csv(\"positifs.csv\", sep = \"\\t\",header=None)  \n",
    "verif = np.array(verif)\n",
    "for i in range(len(verif)):\n",
    "    for j in range(4):\n",
    "        if len(verif[i,j]) <= 15:\n",
    "            print(i,\"erreur\")\n",
    "            \n",
    "\n",
    "for i in range(len(verif)):\n",
    "    verif[i,2] = np.array(clean(verif[i,2]))\n",
    "    verif[i,3] = np.array(clean(verif[i,3]))\n",
    "\n",
    "pos = verif\n",
    "verif = []\n",
    "bdd = np.concatenate((pos,neg))\n",
    "pos = []\n",
    "neg = []\n",
    "labels = np.zeros((len(bdd),1))\n",
    "\n",
    "bdd = np.concatenate((bdd,labels),axis=1)\n",
    "for i in range(int(len(bdd)/2)):\n",
    "    bdd[i,4]=1    \n",
    "labels=[]\n",
    "for i in range(len(bdd)):\n",
    "    bdd[i,0] = transformer_sequence(bdd[i,0],len(bdd[i,0]))\n",
    "    bdd[i,1] = transformer_sequence(bdd[i,1],101)\n",
    "    bdd[i,2] = transformer_structure(bdd[i,2])\n",
    "\n",
    "# shuffle pour mélanger positifs et négatifs\n",
    "\n",
    "indices = np.zeros(len(bdd),dtype=int)\n",
    "for i in range(int(len(bdd)/2)):\n",
    "    indices[2*i] = int(i)\n",
    "    indices[2*i+1] = int(i + int(len(bdd)/2))\n",
    "bdd = bdd[indices]\n",
    "indices = []\n",
    "\n",
    "# shuffle total\n",
    "\n",
    "indices = np.arange(len(bdd))\n",
    "shuffle(indices)\n",
    "bdd = bdd[indices]\n",
    "\n",
    "for i in range(len(bdd)):\n",
    "    bdd[i,0] = bdd[i,0].transpose()\n",
    "    bdd[i,1] = bdd[i,1].transpose()\n",
    "\n",
    "resh0 = np.zeros((20048,25,4))\n",
    "resh1 = np.zeros((20048,101,4))\n",
    "resh2 = np.zeros((20048,25))\n",
    "resh3 = np.zeros((20048,101))\n",
    "\n",
    "for i in range(len(bdd)):\n",
    "    resh0[i,:,:] = bdd[i,0]\n",
    "    resh1[i,:,:] = bdd[i,1]\n",
    "    resh2[i,:] = bdd[i,2]\n",
    "    resh3[i,:] = bdd[i,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train = 17000\n",
    "nb_val = 500\n",
    "nb_test = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resh0_train = resh0[:nb_train]\n",
    "resh0_val = resh0[nb_train:nb_train+nb_val]\n",
    "resh1_train = resh1[:nb_train]\n",
    "resh1_val = resh1[nb_train:nb_train+nb_val]\n",
    "resh2_train = resh2[:nb_train]\n",
    "resh2_val = resh2[nb_train:nb_train+nb_val]\n",
    "resh3_train = resh3[:nb_train]\n",
    "resh3_val = resh3[nb_train:nb_train+nb_val]\n",
    "resh0_test = resh0[-nb_test:]\n",
    "resh1_test = resh1[-nb_test:]\n",
    "resh2_test = resh2[-nb_test:]\n",
    "resh3_test = resh3[-nb_test:]\n",
    "resh0=[]\n",
    "resh1=[]\n",
    "resh2=[]\n",
    "resh3=[]\n",
    "train = bdd[:nb_train]\n",
    "valid = bdd[nb_train:nb_train+nb_val]\n",
    "y = train[:,4]\n",
    "y = keras.utils.np_utils.to_categorical(y,2)\n",
    "val_y = valid[:,4]\n",
    "val_y = keras.utils.np_utils.to_categorical(val_y,2)\n",
    "train=[]\n",
    "valid=[]\n",
    "test = bdd[-nb_test:]\n",
    "bdd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_networks_train_predict(micro_seq_hid = 22, messenger_seq_hid = 100, batch=10000, epoch=1000):\n",
    "    \n",
    "    print('training', nb_train)\n",
    "    \n",
    "    micro_structure_hid = 64\n",
    "    messenger_structure_hid = 128\n",
    "    \n",
    "    micro_seq_train = resh0_train\n",
    "    micro_seq_validation = resh0_val\n",
    "    micro_seq_net =  get_cnn_network_microRNA(micro_seq_hid)\n",
    "    \n",
    "    messenger_seq_train = resh1_train\n",
    "    messenger_seq_validation = resh1_val\n",
    "    messenger_seq_net = get_cnn_network_messengerRNA(messenger_seq_hid)\n",
    "    \n",
    "    micro_structure_train = resh2_train\n",
    "    micro_structure_validation = resh2_val\n",
    "    micro_structure_net = get_mlp_microRNA()\n",
    "    \n",
    "    messenger_structure_train = resh3_train\n",
    "    messenger_structure_validation = resh3_val\n",
    "    messenger_structure_net = get_mlp_messengerRNA()        \n",
    "    \n",
    "    #y, encoder = preprocess_labels(training_label)\n",
    "    #val_y, encoder = preprocess_labels(validation_label, encoder = encoder)\n",
    "       \n",
    "    training = []\n",
    "    validation = []\n",
    "    total_hid = 0\n",
    "\n",
    "    #print(\"Création des réseaux pour les deux séquences\")\n",
    "\n",
    "    training_net=[]\n",
    "    training_net.append(micro_seq_net)\n",
    "    training.append(micro_seq_train)\n",
    "    validation.append(micro_seq_validation)\n",
    "    total_hid = total_hid + micro_seq_hid\n",
    "    micro_seq_train = []\n",
    "    micro_seq_validation = [] \n",
    "    \n",
    "    training_net.append(messenger_seq_net)\n",
    "    training.append(messenger_seq_train)\n",
    "    validation.append(messenger_seq_validation)\n",
    "    total_hid = total_hid + messenger_seq_hid\n",
    "    messenger_seq_train = []\n",
    "    messenger_seq_validation = []\n",
    "    \n",
    "    '''\n",
    "    print(\"Concaténation des deux séquences\")\n",
    "    \n",
    "    left = Sequential()\n",
    "    left.add(Merge(training_net, mode='concat'))\n",
    "    left.add(Dropout(0.5))\n",
    "    print(total_hid)\n",
    "    left.add(Dense((micro_seq_hid+messenger_seq_hid), input_shape=((micro_seq_hid+messenger_seq_hid),)))\n",
    "    left.add(Activation('softmax'))\n",
    "    \n",
    "    print(\"Création des réseaux pour les deux structures\")\n",
    "    \n",
    "    training_net=[]\n",
    "    '''\n",
    "    \n",
    "    training_net.append(micro_structure_net)\n",
    "    training.append(micro_structure_train)\n",
    "    validation.append(micro_structure_validation)\n",
    "    total_hid = total_hid + micro_structure_hid\n",
    "    micro_structure_train = []\n",
    "    micro_structure_validation = []\n",
    "    \n",
    "    training_net.append(messenger_structure_net)\n",
    "    training.append(messenger_structure_train)\n",
    "    validation.append(messenger_structure_validation)\n",
    "    total_hid = total_hid + messenger_structure_hid\n",
    "    messenger_structure_train = []\n",
    "    messenger_structure_validation = []\n",
    "    \n",
    "    '''\n",
    "    print(\"Concaténation des deux structures\")\n",
    "    \n",
    "    right = Sequential()\n",
    "    right.add(Merge(training_net, mode='concat'))\n",
    "    right.add(Dropout(0.5))\n",
    "    print(total_hid)\n",
    "    right.add(Dense(192, input_shape=(192,)))\n",
    "    right.add(Activation('softmax'))\n",
    "    \n",
    "    print(\"Concaténation des deux modèles\")\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(Merge([left,right], mode='concat'))\n",
    "    model.add(Merge(training_net, mode='concat'))\n",
    " \n",
    "    #model.add(Dense(total_hid, input_shape=(total_hid,)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(PReLU())\n",
    "    #model.add(BatchNormalization(mode=2))\n",
    "    #model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    print(total_hid)\n",
    "    model.add(Dense(2, input_shape=(total_hid,)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    \n",
    "    #checkpointer = ModelCheckpoint(filepath=\"models/bestmodel.hdf5\", verbose=0, save_best_only=True)\n",
    "    earlystopper = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "    #validation_data=(np.transpose(validmat['validxdata'],axes=(0,2,1)), validmat['validdata']), callbacks=[checkpointer,earlystopper]\n",
    "    print('model training')\n",
    "    model.fit(training, y, batch_size=batch, epochs=epoch, verbose=0, validation_data=(validation, val_y), callbacks=[earlystopper])\n",
    "    print('training finished')\n",
    "    training = []\n",
    "    validation = []\n",
    "    \n",
    "    # test\n",
    "    true_y = test[:,4]\n",
    "    \n",
    "    print('predicting')\n",
    "    testing = []\n",
    "    testing.append(resh0_test)\n",
    "    testing.append(resh1_test)\n",
    "    testing.append(resh2_test)\n",
    "    testing.append(resh3_test)\n",
    "        \n",
    "    predictions = model.predict_proba(testing)[:,1]\n",
    "    print(predictions)\n",
    "    for i,nulll in enumerate(predictions):\n",
    "        predictions[i] = round(predictions[i])\n",
    "    print(predictions,true_y)\n",
    "    perfs = calculate_performance(len(predictions), predictions, true_y)\n",
    "    print(\"acc : \", perfs[0])\n",
    "    print(\"precision : \", perfs[1])\n",
    "    print(\"sensitivity : \", perfs[2])\n",
    "    print(\"specificity : \", perfs[3])\n",
    "    print(\"MCC : \", perfs[4])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = merge_networks_train_predict(23,101,700,70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc :  0.6096\n",
    "precision :  0.6086956521739131\n",
    "sensitivity :  0.5979049153908138\n",
    "specificity :  0.6211278792692613\n",
    "MCC :  0.21909450356110444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 100 trains : \n",
    "acc :  0.5\n",
    "precision :  0.5333333333333333\n",
    "sensitivity :  0.7272727272727273\n",
    "specificity :  0.2222222222222222\n",
    "MCC :  -0.058025885318565944\n",
    "\n",
    "##### 17000 trains\n",
    "acc :  0.5964\n",
    "precision :  0.6069017254313578\n",
    "sensitivity :  0.625193199381762\n",
    "specificity :  0.5655058043117744\n",
    "MCC :  0.19100235122777048\n",
    "##### \n",
    "acc :  0.5564\n",
    "precision :  0.5346628679962013\n",
    "sensitivity :  0.8972111553784861\n",
    "specificity :  0.21285140562248997\n",
    "MCC :  0.15103195995226454\n",
    "(10000 batch et 1000 epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
